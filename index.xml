<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hshsh&#39;s little site</title>
    <link>http://hshsh.me/</link>
    <description>Recent content on hshsh&#39;s little site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sat, 21 Oct 2017 23:00:00 +0800</lastBuildDate>
    
	<atom:link href="http://hshsh.me/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>一致性哈希算法学习笔记</title>
      <link>http://hshsh.me/post/2017-10-21-consistent-hashing/</link>
      <pubDate>Sat, 21 Oct 2017 23:00:00 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2017-10-21-consistent-hashing/</guid>
      <description>一致性哈希算法（Consistent Hashing）学习笔记。
主要内容从参考资料中摘抄，版权归原作者所有。
参考资料：
 一致性哈希算法及其在分布式系统中的应用 深入云存储系统Swift核心组件：Ring实现原理剖析      &amp;#20998;&amp;#24067;&amp;#24335;&amp;#32531;&amp;#23384;&amp;#38382;&amp;#39064;&amp;#182;假设我们有一个网站，最近发现随着流量增加，服务器压力越来越大，之前直接读写数据库的方式不太给力了，于是我们想引入 Memcached 作为缓存机制。现在我们一共有三台机器可以作为 Memcached 服务器，如下图所示。
很显然，最简单的策略是将每一次 Memcached 请求随机发送到一台 Memcached 服务器，但是这种策略可能会带来两个问题：
 同一份数据可能被存在不同的机器上而造成数据冗余； 有可能某数据已经被缓存但是访问却没有命中，因为无法保证对相同 key 的访问都被发送到相同的服务器。  因此，随机策略无论是时间效率还是空间效率都非常不好。
要解决上述问题需要做到如下一点：保证对相同 key 的访问会被发送到相同的服务器。很多方法可以实现这一点，最常用的方法是计算哈希。例如对于每次访问，可以按如下算法计算哈希值：
h = Hash(key) % 3  其中，Hash 是一个从字符串到正整数的哈希映射函数。这样，如果我们将 Memcached Server 分别编号为 0、1、2，那么就可以根据上述算式和 key 计算出服务器编号 h，然后去访问。
这个方法虽然解决了上面提到的两个问题，但是存在一些其他的问题，如果将上述方法抽象：
h = Hash(key) % N  这个算式计算每个 key 的请求应该被发送到哪台服务器，其中 N 为服务器的数量，并且服务器按照 0..(N-1) 进行编号。
这个算法的问题在于容错性和扩展性不好。所谓容错性是指当系统中某一个或几个服务器变得不可用时，整个系统是否可以正确高效运行；而扩展性是指当加入新的服务器后，整个系统是否可以正确高效运行。
现在假设有一台服务器宕机了，那么为了填补空缺，要将宕机的服务器从编号列表中移除，后面的服务器按顺序前移一位并将其编号值减一，此时每个 key 就要按 h = Hash(key) % (N-1) 重新计算；同样，如果新增了一台服务器，虽然原有服务器编号不用改变，但是要按 h = Hash(key) % (N+1) 重新计算哈希值。因此系统中一旦有服务器变更，大量的 key 会被重定位到不同有服务器从而造成大量的缓存不命中。而这种情况在分布式系统中是非常糟糕的。</description>
    </item>
    
    <item>
      <title>Django Admin 定制开发技巧</title>
      <link>http://hshsh.me/post/2017-06-13-django-admin-ticks/</link>
      <pubDate>Tue, 13 Jun 2017 23:00:00 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2017-06-13-django-admin-ticks/</guid>
      <description>过滤器（Filters） 下拉列表式过滤器 模版文件：
{% load i18n %} &amp;lt;script type=&amp;quot;text/javascript&amp;quot;&amp;gt;var go_from_select = function(opt) { window.location = window.location.pathname + opt };&amp;lt;/script&amp;gt; &amp;lt;h3&amp;gt;{% blocktrans with filter_title=title %} By {{ filter_title }} {% endblocktrans %}&amp;lt;/h3&amp;gt; &amp;lt;ul class=&amp;quot;admin-filter-{{ title|cut:&#39; &#39; }}&amp;quot;&amp;gt; {% if choices|slice:&amp;quot;4:&amp;quot; %} &amp;lt;li&amp;gt; &amp;lt;select style=&amp;quot;width: 95%;&amp;quot; onchange=&amp;quot;go_from_select(this.options[this.selectedIndex].value)&amp;quot;&amp;gt; {% for choice in choices %} &amp;lt;option{% if choice.selected %} selected=&amp;quot;selected&amp;quot;{% endif %} value=&amp;quot;{{ choice.query_string|iriencode }}&amp;quot;&amp;gt;{{ choice.display }}&amp;lt;/option&amp;gt; {% endfor %} &amp;lt;/select&amp;gt; &amp;lt;/li&amp;gt; {% else %} {% for choice in choices %} &amp;lt;li{% if choice.</description>
    </item>
    
    <item>
      <title>MacBook Pro上完美使用IKBC C87机械键盘</title>
      <link>http://hshsh.me/post/2017-03-04-ikbc-keyboard-on-mac/</link>
      <pubDate>Sat, 04 Mar 2017 14:50:00 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2017-03-04-ikbc-keyboard-on-mac/</guid>
      <description>最近也用上了MacBook Pro，除了吐槽一下这个Multi Touch Bar键盘敲起来没有一点手感，还有数字&amp;rdquo;1&amp;rdquo;左边那个丧心病狂的的&amp;rdquo;Section（§）&amp;rdquo;之外，其他方面用起来真心舒服。
适应了几天Mac的键位后，琢磨着把自己的IKBC C87机械键盘接上来用，主要参考了下面的文章：
 让机械键盘完美适配你的Mac！ Make Home &amp;amp; End keys behave like Windows on Mac OS X DefaultKeyBinding.dict  作为一枚死宅的程序猿，我的需求很简单：
 把数字&amp;rdquo;1&amp;rdquo;左边的Section键换回反引号键&amp;rdquo;`~&amp;ldquo;，把半残的左边Shift键右边的反引号键还原成Shift； 把IKBC C87上的Control、Win、Alt键映射成Mac的Control、Option、Command键位顺序； 把IKBC C87上的Home、End、PageUp、PageDown键功能都可以正常跳转  嗯，就这样，通过一番研（sou）究（suo）之后，发现可以通过如下组合实现上面的功能：
Karabiner Elements修改建伟映射、DefaultKeyBinding.dict修改组合键功能、Alfred控制快速切换Karabiner Elements的Profile。
具体过程略过，简要记录一下主要配置。
Karabiner Elements建立三个Profiles：
 &amp;ldquo;none&amp;rdquo;：原始键盘，不映射任何键位，这个主要是用作偶尔别人使用电脑不习惯改键或键位修改出现问题时，可以快速恢复； &amp;ldquo;internal&amp;rdquo;：这个用于映射内置键盘的&amp;rdquo;§±&amp;rdquo;和&amp;rdquo;`~&amp;ldquo;键； &amp;ldquo;ikbc&amp;rdquo;: 用于映射机械键盘上的Control、Win、Alt键的顺序；  这三个映射使用Karabiner Elements Profile Switcher这个Workflow可以快速切换。因为Karabiner Elements现在还不可以根据键盘关联自动调用对应的Profile。
我给每个Profile都制定了一个快捷键（Alfred Trigger Hotkey）：
 Ctrl + Option + Command + K：切换到 &amp;ldquo;none&amp;rdquo; Profile； 在内置键盘上使用 F12 切换到 &amp;ldquo;internal&amp;rdquo; Profile； 机械键盘上的 PrtSrc 键被 Karabiner Elements 识别出来是F13，所以就用了这个 &amp;ldquo;F13&amp;rdquo; 切换到 &amp;ldquo;ikbc&amp;rdquo; Profile。  我的编码习惯经常会用到Home、End、PageUp、PageDown这几个键进行快速跳转，接上机械键盘自然能把这些键配置起来是最好了。好在实现这些功能键不需要KE和Alfred的功能，直接使用系统配置就可以完成。</description>
    </item>
    
    <item>
      <title>解决Photoshop CC 2017无法安装扩展插件的问题</title>
      <link>http://hshsh.me/post/2016-12-22-photoshop-cc-2017-extensions-fix/</link>
      <pubDate>Thu, 22 Dec 2016 23:30:00 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-12-22-photoshop-cc-2017-extensions-fix/</guid>
      <description>安装Photoshop CC 2017后，打开PNG文件，弹出“无法加载扩展，因为它未正确签署”的错误提示，解决方法如下：
打开注册表编辑器，定位到路径“HKEY_CURRENT_USER/Software/Adobe/CSXS.7”，新建“字符串值”，名称为“PlayerDebugMode”，值为1。
关闭注册表，收工。重新打开Photoshop，可以正常安装加载扩展了。
或者可以保存以下内容为“ps2017_extensions_fix.reg”文件，并双击导入注册表：
Windows Registry Editor Version 5.00 [HKEY_CURRENT_USER\SOFTWARE\Adobe\CSXS.7] &amp;quot;PlayerDebugMode&amp;quot;=&amp;quot;1&amp;quot;  下面参考资料文章中有动图指导，推荐拜读原文。
参考资料：
 一招解决PSCC2017无法安装扩展插件(原创文章)  </description>
    </item>
    
    <item>
      <title>解决Fedora24上Pycharm中中文显示方块的问题</title>
      <link>http://hshsh.me/post/2016-09-07-fedora-24-pycharm-chinese-fonts/</link>
      <pubDate>Wed, 07 Sep 2016 10:00:00 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-09-07-fedora-24-pycharm-chinese-fonts/</guid>
      <description>先说说在Ubuntu上遇到的奇怪的TCP连接失败问题，在升级到Ubuntu16.04后，电脑突然连不上阿里云上的一台Ubuntu服务器了（所有TCP端口都无法连接），连接另外一台Windows服务器正常，办公室里另外一台Ubuntu16.04电脑也无法连接到这台Ubuntu服务器，但是其他Windows系统的电脑访问一切正常，另外还有两台Ubuntu14.04的工作站连接正常。后来又有一个同事的Ubuntu14.04也出现了连接不上的问题。而且还是有时候一连几个小时连接不上，有时候又一切正常的。
莫名其妙的问题，研(sou)究(suo)两天之后（Google关键字：ubuntu tcp retransmission），发现这个问题好像是Debian系列发行版的毛病，又好像会针对虚拟机才会出现，网上也有很多报告这个问题的，但是都没有合适的解决方案。所以决定换个发行版咯，就试试Fedora呗～
Fedora Workstation 24安装不复杂，一路next就高定了，dnf命令用起来跟apt-get也差不多。
但是用Pycharm编辑Markdown文件时候，发现预览页面很多中文字符都变成方块了，理论上这个是字体的问题，但是不知道需要的是哪一个字体，在Fedora 21安装以后的优化这篇文章中提到一些常用的字体，先装上试试。
sudo dnf install google-droid-sans-fonts sudo dnf install wqy-zenhei-fonts sudo dnf install adobe-source-han-sans-cn-fonts  另外，安装字体的时候，我还安装了LibreOffice的中文语言支持包，不知道里面有没有甬道的字体：
sudo dnf install libreoffice-langpack-zh-Hans  安装完字体，关掉所有已经打开的Pycharm程序，重新启动Pycharm，所有中文字体显示就正常了。</description>
    </item>
    
    <item>
      <title>Ubuntu14.04上DNS引发的血案</title>
      <link>http://hshsh.me/post/2016-08-09-ubuntu-14.04-dnsmasq/</link>
      <pubDate>Tue, 09 Aug 2016 11:03:30 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-08-09-ubuntu-14.04-dnsmasq/</guid>
      <description>作为技术备忘，为了纪念为此死去的千千万万脑细胞，先说问题：
公司搬家后，一台工作站上的爬虫程序突然莫名其妙的变慢，从一天20多万条数据萎缩到了14W-20W条不稳定，因为新办公室里所有部门共用一条网线，起初还以为是网络拥挤，属于正常情况，后来居然夸张的变成了每天3W条数据，经过跟踪发现这个爬虫全天候不分时间段的慢，这就很不对了。然后开始一系列的排查。
后来把爬虫里面用域名访问的代码改成使用IP地址访问，发现速度连接速度不慢，但是一使用域名就变龟速了，很明显域名解析出问题了，作为临时方案，我居然手写了一段IP地址缓存的代码，:(&amp;lt;
吐槽一下：Ubuntu系统配置DNS一点都不方便！！
发现工作站上的DNS服务器被配置成了8.8.8.8，然而这段时间Google的DNS抽风了，经常连接不上，所以解决办法就很简单了。
研究配置DNS的过程中，被灌输了一堆新名词：dnsmasq, resolv.conf, NetworkManager, resolvconf &amp;hellip;
没看全部的文档，根据查阅的部分资料，Ubuntu的DNS机制大概是这样的：
 NetworkManager管理DNS配置，图形界面“编辑连接”弹出来的就是这货，这里配置的DNS存储在/run/resolvconf/interface/NetworkManager文件中。 系统范围DNS的基本配置在/etc/resolvconf/resolv.conf.d/这个目录下面，这里有三个文件：base中是基本配置，系统动态生成resolv.conf文件时要包含的内容；head也是要包含在resolv.conf文件中的，里面主要包含一些说明，比如不要修改我会被覆盖之类的；tail里是要追加到动态生成的resolv.conf文件末尾的内容。 从Ubuntu 12.04开始，系统默认都会安装一个阉割版的dnsmasq服务（dnsmasq-base包，不是dnsmasq包），主要是为了解决DNS和VPN之间的什么什么暧昧问题的，监听在127.0.1.1:53地址上，系统DNS请求都会发到这个监听地址上。 系统启动或者执行resolvconf -u命令的时候，会根据配置动态生成DNS配置文件/ete/resolv.conf。  这个理解必然是不全面的，甚至有些完全就是错的，留待下次再搞DNS问题的时候研究吧。
相关的解决方法
安装完整功能的Dnsmasq：sudo apt-get install dnsmasq
配置Dnsmasq上游DNS服务器和本地监听地址，修改/etc/dnsmasq.conf文件：
listen-address=127.0.0.1,127.0.1.1 resolv-file=/etc/resolv.dnsmasq conf-dir=/etc/dnsmasq.d  /etc/resolv.dnsmasq文件里面写上游DNS服务器地址：
nameserver 114.114.114.114 nameserver 223.5.5.5 nameserver 114.114.115.115 nameserver 223.6.6.6  其他dnsmasq配置自由发挥就好啦～
修改系统DNS解析配置/etc/resolvconf/resolv.conf.d/base：
nameserver 127.0.0.1 nameserver 114.114.114.114 nameserver 223.5.5.5  然后启动dnsmasq服务：sudo service dnsmasq restart，更新DNS动态配置：sudo resolvconf -u。
以上步骤可能就解决问题了，也可能还会有问题，也可能会引入新的问题，比如参考资料4中提到的系统启动时候服务启动顺序的问题～～
Dnsmasq扩展阅读：
Dnsmasq提供DNS缓存和DHCP服务功能。作为域名解析服务器(DNS)，dnsmasq可以通过缓存DNS请求来提高对访问过的网址的连接速度。作为DHCP服务器，dnsmasq可以用于为局域网电脑分配内网ip地址和提供路由。DNS和DHCP两个功能可以同时或分别单独实现。dnsmasq轻量且易配置，适用于个人用户或少于50台主机的网络。此外它还自带了一个PXE服务器。&amp;gt;&amp;gt;&amp;gt;
不使用Dnsmasq直接设置系统的DNS服务器
修改/etc/resolvconf/resolv.conf.d/base文件中DNS配置：
nameserver 114.114.114.114 nameserver 223.5.5.5 nameserver 114.114.115.115 nameserver 223.6.6.6  参考资料：</description>
    </item>
    
    <item>
      <title>升级Windows导致Postgresql服务无法启动问题</title>
      <link>http://hshsh.me/post/2016-08-06-fix-postgresql-permission-issue-again/</link>
      <pubDate>Sat, 06 Aug 2016 10:17:25 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-08-06-fix-postgresql-permission-issue-again/</guid>
      <description>这两天升级系统到了Windows 10.1，结果Postgresql数据库又起不来了，之前重做Windows 7的时候就遇到过这个问题，但是不记得是怎么搞定的了。
查了一堆资料，基本断定问题是由于升级操作系统后，Windows建立新用户，用户SID改变导致的。可是网上始终也没有找到个有效的解决方法。记得上次就是这样，最后只能再次祭出大招，下载EnterpriseDB的安装包，重装一遍，查看它的服务进程登录用户和文件目录权限。
这里记录一下，以免下次又忘了：
 服务进程登录用户：&amp;rdquo;NT AUTHORITY\NetworkService&amp;rdquo; 服务进程启动命令：C:/path/to/pg_ctl.exe runservice -N &amp;ldquo;pgsql&amp;rdquo; -D &amp;ldquo;E:/path/to/data&amp;rdquo; -w 数据目录OWNER：Administrators (COMPUTER\Administrators)  另外数据目录上要确保下面两个权限：
 NETWORK SERVICE：完全控制 本地登录用户（COMPUTER\username）：完全控制  其他的权限系统默认就行了，具体权限设置就不记录了，从Cygwin开始就对着权限搞来搞去，现在Postgresql又搞权限问题，已经是轻车熟路了。</description>
    </item>
    
    <item>
      <title>遭遇时间不同步引起的BUG</title>
      <link>http://hshsh.me/post/2016-07-30-fix-bug-caused-by-un-synced-time/</link>
      <pubDate>Sat, 30 Jul 2016 07:30:00 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-07-30-fix-bug-caused-by-un-synced-time/</guid>
      <description>前段时间遇到一个诡异的bug，后端代码有使用到一个数据服务API，接口有时间戳验证。API服务跑在云服务器上，程序后台跑在本地Ubuntu服务器上。
两个服务都是我写的，API服务升级了时间戳验证功能后重新部署，本地服务器上的程序后台服务居然挂掉了，开发电脑上各种测试都正常，只好开启调试日志检查。结果发现是本地服务器上时间比API服务的时间快了两分钟多。
查看服务器配置，发现Ubuntu Server发行版默认居然没有安装NTP服务，Server版居然不开启时间同步服务？？？一万匹草泥马奔腾而过……
找到问题，解决就简单了，安装NTP服务，顺便把时间服务器配置到国内某云的时间服务器~
重点强调：部署Server服务器后一定要检查NTP服务起了没？一定要起时间同步服务，并且要配置速度比较快的时间同步服务器。</description>
    </item>
    
    <item>
      <title>Ubuntu 14.04 使用阿里云源</title>
      <link>http://hshsh.me/post/2016-07-30-using-aliyun-mirror-for-ubuntu-14.04/</link>
      <pubDate>Sat, 30 Jul 2016 07:20:00 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-07-30-using-aliyun-mirror-for-ubuntu-14.04/</guid>
      <description>Ubuntu的官方源也是慢的不行不行的，怎么办？换阿里云的源，速度杠杠的！
sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak # 备份 sudo vim /etc/apt/sources.list # 修改配置 sudo apt-get clean &amp;amp;&amp;amp; sudo apt-get update # 更新列表  修改源配置如下：
deb http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiverse deb-src http://mirrors.</description>
    </item>
    
    <item>
      <title>使用清华大学Anaconda镜像</title>
      <link>http://hshsh.me/post/2016-07-30-using-tsinghua-anaconda-mirror/</link>
      <pubDate>Sat, 30 Jul 2016 07:00:00 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-07-30-using-tsinghua-anaconda-mirror/</guid>
      <description>Python的Anaconda发行版用起来真是舒服，可是官方源的速度真可谓是龟速，一直也没找到国内的镜像源。 早上看到IPython更新到5.0LTS版本，看起来很爽，不死心的又查了一下，发现清华大学2016年4月27日新增了Anaconda的镜像，果断切换。
看这里：https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/
conda config --add channels &#39;https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/&#39; conda config --set show_channel_urls yes  </description>
    </item>
    
    <item>
      <title>Ubuntu &amp; Linux 常用命令笔记</title>
      <link>http://hshsh.me/post/2016-04-27-ubuntu-linux-shell-commands-notes/</link>
      <pubDate>Wed, 27 Apr 2016 12:00:00 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-04-27-ubuntu-linux-shell-commands-notes/</guid>
      <description>系统配置 安装 Oracle JDK $ sudo add-apt-repository ppa:webupd8team/java $ sudo apt-get update $ sudo apt-get install oracle-java8-installer $ sudo apt-get install oracle-java8-set-default  永久性修改系统DNS 编辑/etc/network/interfaces文件，在最后添加一行：
dns-nameservers 8.8.8.8 8.8.4.4  或者可以修改/etc/resolvconf/resolv.conf.d/base文件，默认为空，在其中插入：
nameserver 8.8.8.8 nameserver 8.8.4.4  如果有多个DNS，就每行添加一个。
NOTE：亲测，以上设置，需要重启系统后生效！
常用命令行工具 查看进程 $ ps ax $ ps aux $ ps ax | less $ ps ax | grep ...  查看端口 $ netstat -tap | grep ... $ netstat -na | grep .</description>
    </item>
    
    <item>
      <title>MySQL &amp; SQL 常用命令笔记</title>
      <link>http://hshsh.me/post/2016-04-27-mysql-sql-commands-notes/</link>
      <pubDate>Wed, 27 Apr 2016 11:42:00 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-04-27-mysql-sql-commands-notes/</guid>
      <description>查看数据库和数据表结构 $ mysql -u user -p mysql&amp;gt; show databases; mysql&amp;gt; show tables; mysql&amp;gt; use {DATABASE_NAME}; mysql&amp;gt; show columns from {TABLE_NAME}; mysql&amp;gt; show create table {TABLE_NAME}; mysql&amp;gt;  变更数据表结构 # 重命名字段并可选修改字段类型 alter table {TABLE_NAME} change {OLD_COLUMN} {NEW_COLUMN} {COLUMN_TYPE}; # 修改字段类型不重命名字段 alter table {TABLE_NAME} modify {COLUMN_NAME} {COLUMN_TYPE}; # 增加字段 alter table {TABLE_NAME} add column {COLUMN_NAME} {COLUMN_TYPE}; # 删除字段 alter table {TABLE_NAME} drop column {COLUMN_NAME};  快速批量加载数据到数据库 NOTE: 下面命令中{...}表示必填参数，[...]表示选项参数
# 逗号分隔的 csv 文件, 字段列表可选 load data local infile &amp;quot;/path/to/file.</description>
    </item>
    
    <item>
      <title>XtraBackup热备份MySQL主从同步笔记</title>
      <link>http://hshsh.me/post/2016-04-26-mysql-hot-backup-with-xtrabackup/</link>
      <pubDate>Tue, 26 Apr 2016 18:30:00 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-04-26-mysql-hot-backup-with-xtrabackup/</guid>
      <description>公司的MySQL数据库单实例裸跑了一个多月，今天终于做了主从同步，暂时主要起备份作用，庆幸这段时间没有发生意外。
先说主要的参考资料，强烈推荐阅读：
 XtraBackup不停机不锁表搭建MySQL主从同步实践 使用 Xtrabackup 在线对MySQL做主从复制 通过XtraBackup实现不停机不锁表搭建主从同步  更新历史:
2016-04-26: 初稿.
2016-12-19:
 修复 innobackupex 命令错误: &amp;ldquo;xtrabackup: Error: &amp;ndash;defaults-file must be specified first on the command line&amp;rdquo;. 添加 &amp;ldquo;主从复制心跳和连接超时&amp;rdquo; 内容.  简介 转载一下主从同步和XtraBackup的简介：
MySQL主从同步原理
MySQL主从同步是在MySQL主从复制(Master-Slave Replication)基础上实现的，通过设置在Master MySQL上的binlog(使其处于打开状态)，Slave MySQL上通过一个I/O线程从Master MySQL上读取binlog，然后传输到Slave MySQL的中继日志中，然后Slave MySQL的SQL线程从中继日志中读取中继日志，然后应用到Slave MySQL的数据库中。这样实现了主从数据同步功能。
XtraBackup备份原理
innobackupex在后台线程不断追踪InnoDB的日志文件，然后复制InnoDB的数据文件。数据文件复制完成之后，日志的复制线程也会结束。这样就得到了不在同一时间点的数据副本和开始备份以后的事务日志。完成上面的步骤之后，就可以使用InnoDB崩溃恢复代码执行事务日志（redo log），以达到数据的一致性。
备份分为两个过程：
 backup，备份阶段，追踪事务日志和复制数据文件（物理备份）。 preparing，重放事务日志，使所有的数据处于同一个时间点，达到一致性状态。  XtraBackup的优点
 可以快速可靠的完成数据备份（复制数据文件和追踪事务日志） 数据备份过程中不会中断事务的处理（热备份） 节约磁盘空间和网络带宽 自动完成备份鉴定 因更快的恢复时间而提高在线时间  操作笔记 参考的两篇文章里面说的挺详细的，但是有部分命令和命令执行顺序写的不大明白，这里简单记录以下。
完整的步骤
 主、从服务器上都搭好MySQL服务，从服务器上MySQL版本大于等于主服务器，最好完全一致 在要做主从同步的服务器上分别安装XtraBackup 如果从服务器上有MySQL实例，停掉服务，备份删除数据库内容，保留数据库目录 配置主从服务器打开主从同步功能 主服务器上执行备份 传输备份文件到从服务器，并同步数据文件（apply-log） 从服务器上恢复备份 主服务器上授权同步帐号 从服务器上设置MASTER并开启同步  完成，可以检查同步状态了！</description>
    </item>
    
    <item>
      <title>[转载] 专栏：RabbitMQ从入门到精通</title>
      <link>http://hshsh.me/post/2016-04-24-rabbitmq-topic-articles/</link>
      <pubDate>Sun, 24 Apr 2016 22:48:11 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-04-24-rabbitmq-topic-articles/</guid>
      <description>最近有用到RabbitMQ，在网上搜到几篇介绍文章，除去CSDN的排版不说，文章内容还是很好的。
原文网址：http://blog.csdn.net/column/details/rabbitmq.html
下面是几篇文章的摘要，详情请跳转原文阅读。
文章摘要 RabbitMQ是一个在AMQP基础上完整的，可复用的企业消息系统。它可以用于大型软件系统各个模块之间的高效通信，支持高并发，支持可扩展。
RabbitMQ消息队列（一）: Detailed Introduction 详细介绍 对于一个大型的软件系统来说，它会有很多的组件或者说模块或者说子系统或者（subsystem or Component or submodule）。那么这些模块的如何通信？这和传统的IPC有很大的区别。传统的IPC很多都是在单一系统上的，模块耦合性很大，不适合扩展（Scalability）；如果使用socket那么不同的模块的确可以部署到不同的机器上，但是还是有很多问题需要解决。比如：
 信息的发送者和接收者如何维持这个连接，如果一方的连接中断，这期间的数据如何防止丢失？ 如何降低发送者和接收者的耦合度？ 如何让Priority高的接收者先接到数据？ 如何做到load balance？有效均衡接收者的负载？ 如何有效的将数据发送到相关的接收者？也就是说让接收者subscribe不同的数据，如何做有效的filter。 如何做到可扩展，甚至将这个通信模块发到cluster上？ 如何保证接收者接收到了完整，正确的数据？  AMDQ协议解决了以上的问题，而RabbitMQ实现了AMQP。
RabbitMQ消息队列（二）：“Hello, World” 首先复习一下上篇所学：RabbitMQ实现了AMQP定义的消息队列。它实现的功能“非常简单”：从Producer接收数据然后传递到Consumer。它能保证多并发，数据安全传递，可扩展。
和任何的Hello World一样，它们都不复杂。我们将会设计两个程序，一个发送Hello world，另一个接收这个数据并且打印到屏幕。
RabbitMQ消息队列（三）：任务分发机制 在上篇文章中，我们解决了从发送端（Producer）向接收端（Consumer）发送“Hello World”的问题。在实际的应用场景中，这是远远不够的。从本篇文章开始，我们将结合更加实际的应用场景来讲解更多的高级用法。
当有Consumer需要大量的运算时，RabbitMQ Server需要一定的分发机制来balance每个Consumer的load。试想一下，对于web application来说，在一个很多的HTTP request里是没有时间来处理复杂的运算的，只能通过后台的一些工作线程来完成。接下来我们分别讲解。
应用场景就是RabbitMQ Server会将queue的Message分发给不同的Consumer以处理计算密集型的任务。
RabbitMQ消息队列（四）：分发到多Consumer（Publish/Subscribe） 这篇文章中，我们将创建一个日志系统，它包含两个部分：第一个部分是发出log（Producer），第二个部分接收到并打印（Consumer）。我们将构建两个Consumer，第一个将log写到物理磁盘上；第二个将log输出的屏幕。
RabbitMQ 的Messaging Model就是Producer并不会直接发送Message到queue。实际上，Producer并不知道它发送的Message是否已经到达queue。
Producer发送的Message实际上是发到了Exchange中。它的功能也很简单：从Producer接收Message，然后投递到queue中。Exchange需要知道如何处理Message，是把它放到那个queue中，还是放到多个queue中？这个rule是通过Exchange的类型定义的。
我们知道有三种类型的Exchange：direct, topic 和fanout。fanout就是广播模式，会将所有的Message都放到它所知道的queue中。
RabbitMQ消息队列（五）：Routing 消息路由 上篇文章中，我们构建了一个简单的日志系统。接下来，我们将丰富它：能够使用不同的severity来监听不同等级的log。比如我们希望只有error的log才保存到磁盘上。
RabbitMQ消息队列（六）：使用主题进行消息分发 在上篇文章中，我们实现了一个简单的日志系统。Consumer可以监听不同severity的log。但是，这也是它之所以叫做简单日志系统的原因，因为是仅仅能够通过severity设定。不支持更多的标准。
比如syslog unix的日志工具，它可以通过severity (info/warn/crit&amp;hellip;) 和模块(auth/cron/kern&amp;hellip;)。这可能更是我们想要的：我们可以仅仅需要cron模块的log。
为了实现类似的功能，我们需要用到topic exchange。
RabbitMQ消息队列（七）：适用于云计算集群的远程调用（RPC） 在云计算环境中，很多时候需要用它其他机器的计算资源，我们有可能会在接收到Message进行处理时，会把一部分计算任务分配到其他节点来完成。那么，RabbitMQ如何使用RPC呢？在本篇文章中，我们将会通过其它节点来求斐波纳契完成示例。
为了展示一个RPC服务是如何使用的，我们将创建一段很简单的客户端class。它将会向外提供名字为call的函数，这个call会发送RPC请求并且阻塞知道收到RPC运算的结果。
RabbitMQ消息队列的小伙伴: ProtoBuf（Google Protocol Buffer） 什么是ProtoBuf？
一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。目前提供了 C++、Java、Python 三种语言的 API。</description>
    </item>
    
    <item>
      <title>welcome to my hugo blog</title>
      <link>http://hshsh.me/post/2016-04-24-welcome-to-my-hugo-blog/</link>
      <pubDate>Sun, 24 Apr 2016 11:36:14 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-04-24-welcome-to-my-hugo-blog/</guid>
      <description> Welcome to my new Hugo blog.
为什么又建一个博客 其实在07年开始的时候就建立了自己的博客，当时是在百度空间，内容主要是网上转载的学习资料。百度空间的排版真心费劲。后来百度空间倒下了，内容也都没有了。
后来，GAE开始流行，作为Google忠实的粉丝和一个不折腾会死星人，当然得尝试尝试了。博客搭建好了，但是却没写出什么东西来，一两年时间里，也只有寥寥数篇生活杂记。这时候，我还买了一个自己的域名。
时间大概是08年。又后来，Google退出中国，服务越来越难访问，也就没管那个站了，甚至现在都想不起来GAE上的那个二级域名是什么了。
然后，Github很火，很多人开始在Github Pages上搭建博客。了解之后，一下子又被吸引了，用Markdown书写的感觉特别爽，排版也漂亮，对代码高亮等扩展功能的支持也好。大概是13年的时候使用Jekyll在Github Pages上又搭建了一个博客，还把以前的一些乱七八糟的文字整理了过去，并且绑定了之前买的域名。这个网站现在还能访问：http://www.imwsh.net。
（号外：要是有人喜欢imwsh.net这个域名，联系我哈，便宜出售~ ~）
Github Pages上的博客搭好了，但是我却是跟IT越走越远，工作在传统行业，又跑去拉萨野了一趟，人又懒得很，也就不了了之了。
今年过完春节，贼心不死的我又在纠结工作的事情，最终在老婆和朋友的鼓励下，成功的找到了一份后端程序员的工作。于是乎又开始学习恶补，又惦记起来博客的事情，翻翻硬盘，那个博客的源码都已经被删了，而且对Ruby也不熟悉。当时正在学习Flask Web开发，干脆自己用Python写一个得了。
于是乎，互联网上便又多了一个博客程序：meblog。
这个博客程序简单漂亮，也非常好用，代码高亮、标签、分类什么的都支持，我自己对这个博客程序也很满意，还写了一个命令行推送发布工具，然后爽歪歪的把博客部署到SAE上去了，好事不长久，还不到一个月，在新浪云上充值的豆子就用完了，于是乎，拉倒吧……不过，这个meblog却真的是麻雀很小，五脏俱全，如果有人需要一个Python写的博客程序的话，我强烈推荐！如果还能打开的话，你可以看看她长什么样：http://hshsh.applinzi.com
然后，了解到了hugo这个静态博客生成器，markdown书写、单文件执行、没有依赖，同样可以方便的部署到Github Pages。看起来就很好用啊，官方网站做的还很漂亮，干脆再搭一个博客算了。这便是现在这个博客了，轻车熟路整理markdown文件，生成网站，调整CSS样式，绑定域名，半天时间，网站就上线了。
另外，我还简单写了一个自动部署脚本，可以自动转换发布Jupyter Notebook文件哦。用Jupyter Notebook写笔记，然后自动发布称漂亮的网站，简直爽的不要不要的 ~ ~
至于搭建博客的过程，并不复杂，这里就不复制粘贴了，直接看下面的参考资料吧。
参考资料  Hugo - Hosting on GitHub Pages：这个是主要参考资料 使用Hugo搭建免费个人Blog 使用Hugo + Github搭建个人博客  Hugo安装问题 Hugo官网上有编译好的二进制包发布，如果能直接下载到，最好不过了，放到执行文件路径里就可以了。
很可能的情况是根本下载不下来！！！这时候可以选择使用源码安装，也就是GO GET命令了：
go get -v github.com/spf13/hugo  基本上没有代理你是完不成这条命令的，有了代理，还需要设置GIT和WGET等工具使用这个代理，如下：
git config --global http.proxy http://127.0.0.1:7777 # 如果在Windows上，下面这条命令中的export换成set export HTTP_PROXY=http://127.0.0.1:7777  </description>
    </item>
    
    <item>
      <title>Pandas学习笔记（二）：基本数据结构</title>
      <link>http://hshsh.me/post/2016-04-13-python-pandas-notes-02/</link>
      <pubDate>Wed, 13 Apr 2016 18:02:04 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-04-13-python-pandas-notes-02/</guid>
      <description>Pandas的开发者是：Wes McKinney，这位大牛工作的时候没有顺手的工具，就决定自己顺手写一个出来。
Pandas具有但不限于一下特点：
 具备按轴自动或显式数据对齐功能的数据结构，这可以防止许多由于数据没有对齐以及来自不同数据源（索引方式不同）的数据而导致的常见错误； 集成时间序列功能； 既能处理时间序列数据也能处理非时间序列数据的数据结构； 数学运算和约简（比如对某个轴求和）可以根据不同的元数据（轴编号）执行； 可以灵活处理缺失数据； 合并及其他出现在常见数据库（例如基于SQL的）中的关系型运算。  Pandas的导入约定：
from pandas import Series, DataFrame import pandas as pd import numpy as np  Series Series可以使用列表初始化，初始化时还可以指定索引名称。
Series可以被看成时一个定长的有序字典，因为它时索引值到数据值的一个映射，它可以用在许多原本需要字典参数的函数中。
如果数据被存放在一个Python字典中，也可以直接通过这个字典来创建Series。通过指定 index 可以只选择需要的对象，缺失值使用NaN自动填充。
obj = Series([4, 7, -5, 3]) obj, obj.values, obj.index obj = Series([4, 7, -5, 3], index=[&#39;d&#39;, &#39;b&#39;, &#39;a&#39;, &#39;c&#39;]) obj[obj &amp;gt; 0] obj * 2 np.exp(obj) &#39;b&#39; in obj sdata = {&#39;Ohio&#39;: 35000, &#39;Texas&#39;: 71000, &#39;Oregon&#39;: 16000, &#39;Utah&#39;: 5000} sdata = Series(sdata) sdata  Ohio 35000 Oregon 16000 Texas 71000 Utah 5000 dtype: int64  states = [&#39;California&#39;, &#39;Ohio&#39;, &#39;Oregon&#39;, &#39;Texas&#39;] obj = Series(sdata, index=states) obj  California NaN Ohio 35000.</description>
    </item>
    
    <item>
      <title>Pandas学习笔记（一）：CSV数据加载保存</title>
      <link>http://hshsh.me/post/2016-04-12-python-pandas-notes-01/</link>
      <pubDate>Tue, 12 Apr 2016 18:52:25 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-04-12-python-pandas-notes-01/</guid>
      <description>加载CSV数据 很多数据都存储在CSV文件中，Pandas 为读取提供了一个强大的 read_csv 函数，这个函数接受很多可选参数，通过参数控制数据加载的方式，以及一些基本的清理工作。
pd.read_csv(filepath_or_buffer, sep=&#39;,&#39;, delimiter=None, header=&#39;infer&#39;, names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, iterator=False, chunksize=None, compression=&#39;infer&#39;, thousands=None, decimal=&#39;.&#39;, lineterminator=None, quotechar=&#39;&amp;quot;&#39;, quoting=0, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=False, error_bad_lines=True, warn_bad_lines=True, skip_footer=0, doublequote=True, delim_whitespace=False, as_recarray=False, compact_ints=False, use_unsigned=False, low_memory=True, buffer_lines=None, memory_map=False, float_precision=None) Returns result : DataFrame or TextParser  很多参数都是非常有用的，简要记录一下（详细文档请参考 help(pd.read_csv) 及官方文档）：</description>
    </item>
    
    <item>
      <title>Python代理类两例</title>
      <link>http://hshsh.me/post/2016-04-10-python-proxy-class-examples/</link>
      <pubDate>Sun, 10 Apr 2016 23:30:00 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-04-10-python-proxy-class-examples/</guid>
      <description>最近遇到MySQL的连接断开，MySQLdb报&amp;rsquo;(2006, MySQL server has gone away.)&amp;lsquo;错误的问题。
问题发生的环境是，客户端使用了长连接，程序启动的时候使用MySQLdb模块的connect方法建立一个数据库连接，程序运行期间一直使用这个连接。对于请求比较多的服务程序来说，这个方法不会出现问题，因为MySQL默认连接超时的时间设定是8小时，所以连接不会超时断开，也就不会报这个错误了。但是作为一个调试服务，请求频率可能低于8小时，就导致错误了。
由于不想修改很多具体实现的代码，所以使用代理类的方法对这个程序打了个补丁解决问题。
另外 SQLAlchemy 中的 scoped_session 也是一个代理类，实现也很有意思，这里一起分享一下这两个代理类。
MySQLdb Connection 代理类 先看看出问题的代码：
import MySQLdb conn = MySQLdb.connect(host=&#39;host&#39;, port=&#39;port&#39;, user=&#39;user&#39;, passwd=&#39;passwd&#39;, db=&#39;db&#39;) def use_mysql(): cursor = conn.cursor() cursor.execute(&#39;do something with mysql database;&#39;) cursor.close()  MySQLdb.connect函数返回的是一个Connection对象，当操作频率大于8个小时的时候，MySQL服务器就会关闭连接，然后下一次执行 cursor = conn.cursor() 的时候，就会报连接丢失的错误。为了既使用长连接，又能避免这个错误，我们可以封装一个Connection类的代理类，重载 cursor 方法，当有新的请求的时候，先检查连接是否还在，如果连接丢失的话，就重新连接数据库，然后再调用 Connection 类的 cursor 方法并返回结果：
import MySQLdb from MySQLdb.connections import Connection class ProxyConnection(Connection): def __init__(self, *args, **kwargs): # 保存数据库连接参数以备丢失时候重新连接 self._proxy_args = args self._proxy_kwargs = kwargs super(ProxyConnection, self).</description>
    </item>
    
    <item>
      <title>[转载] Python类引入机制</title>
      <link>http://hshsh.me/post/2016-04-08-python-import-schema/</link>
      <pubDate>Fri, 08 Apr 2016 09:27:54 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-04-08-python-import-schema/</guid>
      <description>本文转载自刘畅的博客，原文地址：https://github.com/Liuchang0812/slides/tree/master/pycon2015cn。本文所涉及到的代码在github上。
概述 Python 是一门优美简单、功能强大的动态语言。在刚刚接触这门语言时，我们会被其优美的格式、简洁的语法和无穷无尽的类库所震撼。在真正的将python应用到实际的项目中，你会遇到一些无法避免的问题。最让人困惑不解的问题有二类，一个 编码问题，另一个则是引用问题。
本文主要讨论关于Python中import的机制与实现、以及介绍一些有意思的Python Hooks。
Python 类库引入机制 首先，看一个简单的例子：
&amp;quot;&amp;quot;&amp;quot; 目录结构如下： ├── __init__.py ├── main.py └── string.py &amp;quot;&amp;quot;&amp;quot; # main.py 内容如下 import string print string.a # string.py 内容如下 a = 2  现在，考虑一下：
 当我们执行main.py的时候，会发生什么事情？ 在main.py文件执行到import string的时候，解释器导入的string类库是当前文件夹下的string.py还是系统标准库的string.py呢？ 如果明确的指明自己要引用的类库？  为了搞清楚上面的问题，我们需要了解关于Python类库引入的机制。
Python的两种引入机制 Python 提供了二种引入机制：
 relative import absolute import  relative import relative import 也叫作相对引入，在Python2.5及之前是默认的引入方法。它的使用方法如下：
from .string import a from ..string import a from ...string import a  这种引入方式使用一个点号来标识引入类库的精确位置。与linux的相对路径表示相似，一个点表示当前目录，每多一个点号则代表向上一层目录。</description>
    </item>
    
    <item>
      <title>MySQL配置文件参考</title>
      <link>http://hshsh.me/post/2016-04-05-mysql-configuration-notes/</link>
      <pubDate>Tue, 05 Apr 2016 14:48:25 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-04-05-mysql-configuration-notes/</guid>
      <description>最近公式生产环境中使用MySQL做数据存储，把数据库跑起来不复杂，但是各种参数的设置调优可真是技术活。
这是配置MySQL的学习笔记，大部分内容出自：http://wsgzao.github.io/post/ltmp/
MySQL数据库配置 MySQL客户端配置： [client] # 客户端连接默认字符集 default-character-set = utf8 port = 3306 socket = /tmp/mysql.sock [mysql] #prompt=&amp;quot;(\u:HOSTNAME:)[\d]&amp;gt; &amp;quot; #mysql提示符中显示当前用户、数据库、时间等信息 prompt=&amp;quot;\u@\h \R:\m:\s [\d]&amp;gt; &amp;quot; #no-auto-rehash # 自动补全功能，取消自动补全可以提高启动速度  MySQL服务端配置 [mysqld] # 唯一的服务标识号，主从同步会涉及 server-id = 1 port = 3306 user = mysql basedir = /app/local/mysql datadir = /app/data/mysql/data socket = /tmp/mysql.sock log-error = /app/data/mysql/mysql_error.log pid-file = /app/data/mysql/mysql.pid sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES # 默认存储引擎 default-storage-engine = InnoDB # 设置最大并发连接数，如果前端程序是PHP，可适当加大，但不可过大。 # 如果前端程序采用连接池，可适当调小，避免连接数过大 max_connections = 512 # 最大连接错误次数，可适当加大，防止频繁连接错误后，前端host被mysql拒绝掉 max_connect_errors = 100000 # 所有线程所打开表的数量 table_open_cache = 512 # 不允许外部文件级别的锁.</description>
    </item>
    
    <item>
      <title>SQLAlchemy学习笔记（一）</title>
      <link>http://hshsh.me/post/2016-04-04-sqlalchemy-study-notes-01/</link>
      <pubDate>Mon, 04 Apr 2016 19:18:09 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-04-04-sqlalchemy-study-notes-01/</guid>
      <description>这篇笔记部分内容来自网络1，部分来自对《Essential SQLAlchemy》的学习和使用经验。
SQLAlchemy是 Python 编程语言下的一款开源软件。提供了SQL工具包及对象关系映射（ORM）工具，使用MIT许可证发行。
SQLAlchemy“采用简单的Python语言，为高效和高性能的数据库访问设计，实现了完整的企业级持久模型”。
SQLAlchemy的理念是，SQL数据库的量级和性能重要于对象集合；而对象集合的抽象又重要于表和行。因此，SQLAlchmey 采用了类似于Java里 Hibernate 的数据映射模型，而不是其他ORM框架采用的 Active Record 模型。不过，Elixir 和 declarative 等可选插件可以让用户使用声明语法。
SQLAlchemy首次发行于2006年2月，并迅速地在Python社区中最广泛使用的ORM工具之一，不亚于Django的ORM框架。
以上摘自维基百科。
使用SQLAlchemy有三种方式：
 使用 Raw SQL 使用 SQL Expression 使用 ORM  前两种方式可以统称为 core 方式。
对于绝大多数应用，推荐使用 SQLAlchemy，即使是使用 Raw SQL，也可以带来如下好处。
 内建数据库连接池。注意：如果是 SQLAlchemy + cx_oracle 的话，需要禁用 Connection Pool，否则会有异常。方法是设置sqlalchemy.poolclass为sqlalchemy.pool.NullPool 强大的日志功能（log） 数据库无关的写法，包括：SQL参数写法、LIMIT语法等 特别提一下，WHERE 条件的 == value 写法，如果value等于None，真正的SQL会转为 IS NULL  SQLAlchemy 的 Raw SQL 和 SQL Expression 比较：
 SQL Expression 的写法是纯 Python 代码，阅读性更好，尤其是在使用 insert() 方法时，字段名和取值成对出现。 Raw SQL 比 SQL Expression 更灵活，如果 SQL/DDL 很复杂，Raw SQL 就更有优势了。  常用数据库连接字符串 import sqlalchemy from sqlalchemy import create_engine # file database # sqlite = create_engine(&#39;sqlite:////absolute/path/to/database.</description>
    </item>
    
    <item>
      <title>Linux常用打包解包命令备忘</title>
      <link>http://hshsh.me/post/2016-04-02-linux-pack-and-unpack/</link>
      <pubDate>Sat, 02 Apr 2016 22:33:15 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-04-02-linux-pack-and-unpack/</guid>
      <description>.tar 打包： tar cvf file.tar dirname
解包： tar xvf filename.tar
.gz 打包： gzip dirname
解包： gzip -d filename.gz
.tar.gz 打包： tar zcvf file.tar.gz dirname
解包： tar zxvf file.tar.gz
.tar.bz2 打包： tar jcvf file.tar.bz2 dirname
解包： tar jxvf filename.tar.bz2 or tar jxvf filename.tar.bz
.zip 打包： zip file.zip dirname
解包： unzip filename.zip
.rar 安装： sudo apt-get install rar
打包： rar e dirname
解包： rar a filename.rar
.z 打包： compress dirname
解包： uncompress filename.</description>
    </item>
    
    <item>
      <title>使用Pandoc制作精美的EPUB、MOBI、PDF电子书</title>
      <link>http://hshsh.me/post/2014-01-13-make-beautiful-ebooks-with-pandoc/</link>
      <pubDate>Mon, 13 Jan 2014 10:38:00 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2014-01-13-make-beautiful-ebooks-with-pandoc/</guid>
      <description>缘起 在亚马逊上购买了一部 Kindle Paperwhite，设备还没有拿到手，就先在网上找一找电子书。结果是网络上的电子书资源确实是不少，但是质量确是参差不齐，只有少数书籍排版精美，书签完整，大多数简直惨不忍睹，可以说严重影响阅读体验。
之前接触到了Pandoc这个文本格式转换的瑞士军刀，知道它可以简便的输出HTML、PDF、EPUB、DOCX等等各类格式，所以就想着能不能自己动手制作排版精美的电子书。经过一番探索，发现可以使用Pandoc和KindleGen非常方便的制作排版精良的电子书。本着造福买不起纸质书的广大群众的意愿，将所得方法及操作技巧发布出来，于是有了本文。
简便制作和精美电子书的定义 简便制作：
 从网络上到处可以下载到的TXT格式可以方便的转换为排好版的电子书 源文件一次整理，可以直接输出为不同格式的电子文档，比如通过一条命名，或者一两次鼠标点击 简单方便的控制书籍排版格式（全书格式统一），比如通过一个CSS文件 修改书籍内容时，不影响书籍排版格式  精美电子书：
 带有目录，支持EPUB、MOBI阅读软件导航功能 标题、正文、引用字段使用不同字体区分，便于识别阅读 段落距离、缩进与行距设置适中，便于识别阅读 整书排版格式统一，清晰易读，无过多分散注意力的元素  相关技术说明 这里应该先从Markdown说起，作为一种轻量级的标记语言，Markdown很适合用来写作，能够让作者把精力集中在书籍的内容撰写上面，而不会被排版等问题分散精力。在一份典型的Markdown文件中，作者使用简单的符号来标记诸如标题、段落、引用、列表、图表等等结构元素，而不记录排版相关的内容。具体的排版工作留给编译器来处理，也就是在从Markdown源文件输出HTML、PDF、EPUB等出版格式的时候才确定，并且可以简便的通过格式控制指令（CSS文件、Latex模板）来控制排版格式。Pandoc就是这么一个编译器，它与其它Markdown解释器相比，有它自己的优点：扩展的Markdown的语法，弥补了Markdown语法结构简单的问题；可以生成很多种格式的文件，甚至Word文档也可以；使用模板控制输出结果，定制简单，等等。关于Markdown和Pandoc的更多信息，请参阅本节的拓展阅读和文末的参考资料。
对于使用OCR或者网络下载的TXT文件制作电子书来说，并不需要掌握Markdown和Pandoc的全部知识，只需要知道基本的Markdown语法就行了：标题、段落、引用、列表、图表等，这些很简单，查看Markdown的语法说明，一会儿就能掌握。按照Markdown的语法对TXT文件进行格式化，然后使用本文中展示的模板文件和编译命令，就可以直接输出为EPUB、MOBI、PDF（A4、6寸，或者其它定制尺寸）文件了。
生成EPUB是Pandoc原生自带的功能，只需要在源文件中填入相关的metadata属性，整理好书籍内容，直接一条命名就OK了。
生成MOBI使用亚马逊的KindleGen软件，由上面生成的EPUB文件为参数转换而来，生成的MOBI文件同时包含旧的MOBI格式和新的KF8格式，可以直接向Amazon网上商店发布，也可以使用Calibre软件（需要安装MobiUnpack插件）拆分为单独的MOBI和AZW3文件。
生成6寸或A4尺寸的PDF，也是Pandoc原生自带的功能，Pandoc根据源文件生成Latex文件，然后使用Latex来生成PDF文件。Latex生成的PDF质量高、排版精美是众所周知的，但是它的语法非常复杂，学习曲线陡峭，把一般用户远远地挡在了大门外。现在有了Pandoc，我们就可以以非常少的工作享用Latex输出的高质量PDF了，当然了这需要Latex软件，只要安装Pandoc用户指南中推荐的MikTex即可。不同尺寸的PDF只需要在运行pandoc命令时指定相应的模板文件就行了。
 关于Markdown和Pandoc的拓展阅读：
 Markdown - 维基百科 Markdown+Pandoc 最佳写作拍档  操作步骤 前面已经把使用到的程序，相关的原理都已经交代了，实际上真正操作起来，步骤非常简单。
安装使用到的软件 安装Pandoc：
从Pandoc官方网站下载适用于Windows的安装包安装，下一步、下一步，就OK了1。
安装MikTex：
从MikTex官方网站下载MikTex安装包进行安装，可以选择安装版本或者便携版本均可。 如果选择使用便携版本，需要把MikTex的执行文件路径添加到系统路径中。
整理源文件 用Pandoc的语法把文章的结构标记出来，各章节标题、引用、表格、插图等等。并把书籍内容文件命名为 book.md 。如果对正则表达式熟悉的话，这里可以使用sed或者其它编辑器对文件进行批量处理。这不是本文的讨论内容，这里就不再详述，如果有兴趣，请自行查阅相关资料。
添加EPUB元数据到 meta.md 文件，这里使用markdown文件中内嵌的YAML来书写，详见Pandoc用户指南中的EPUB元数据部分和本文样书源文件中的 meta.md 。
下载或制作一张封面图片命名为 cover.jpg 与书籍的源文件放在同一个目录。
根据需要修改模板文件 本文附件模板中的 epub.css 文件是控制生成的EPUB和MOBI文件排版格式的，如果有需要，请根据需要进行修改，比如正文字体、引用段落字体、各级标题字体字号等。需要注意的是MOBI并不能支持完整的CSS规范，详情请参阅参考资料 Amazon Kindle Publishing Guidelines。
附件模板中的 default.epub 文件是Pandoc生成EPUB时使用的HTML模板，需要放在用户数据目录下的 templates 文件夹中，详细见参考资料 Pandoc User&amp;rsquo;s Guide 中的 --data-dir 部分。如有需要，也可以进行修改。</description>
    </item>
    
  </channel>
</rss>