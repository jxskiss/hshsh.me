<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on hshsh&#39;s little site</title>
    <link>http://hshsh.me/tags/python/</link>
    <description>Recent content in Python on hshsh&#39;s little site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 13 Jun 2017 23:00:00 +0800</lastBuildDate>
    
	<atom:link href="http://hshsh.me/tags/python/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Django Admin 定制开发技巧</title>
      <link>http://hshsh.me/post/2017-06-13-django-admin-ticks/</link>
      <pubDate>Tue, 13 Jun 2017 23:00:00 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2017-06-13-django-admin-ticks/</guid>
      <description>过滤器（Filters） 下拉列表式过滤器 模版文件：
{% load i18n %} &amp;lt;script type=&amp;quot;text/javascript&amp;quot;&amp;gt;var go_from_select = function(opt) { window.location = window.location.pathname + opt };&amp;lt;/script&amp;gt; &amp;lt;h3&amp;gt;{% blocktrans with filter_title=title %} By {{ filter_title }} {% endblocktrans %}&amp;lt;/h3&amp;gt; &amp;lt;ul class=&amp;quot;admin-filter-{{ title|cut:&#39; &#39; }}&amp;quot;&amp;gt; {% if choices|slice:&amp;quot;4:&amp;quot; %} &amp;lt;li&amp;gt; &amp;lt;select style=&amp;quot;width: 95%;&amp;quot; onchange=&amp;quot;go_from_select(this.options[this.selectedIndex].value)&amp;quot;&amp;gt; {% for choice in choices %} &amp;lt;option{% if choice.selected %} selected=&amp;quot;selected&amp;quot;{% endif %} value=&amp;quot;{{ choice.query_string|iriencode }}&amp;quot;&amp;gt;{{ choice.display }}&amp;lt;/option&amp;gt; {% endfor %} &amp;lt;/select&amp;gt; &amp;lt;/li&amp;gt; {% else %} {% for choice in choices %} &amp;lt;li{% if choice.</description>
    </item>
    
    <item>
      <title>Ubuntu14.04上DNS引发的血案</title>
      <link>http://hshsh.me/post/2016-08-09-ubuntu-14.04-dnsmasq/</link>
      <pubDate>Tue, 09 Aug 2016 11:03:30 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-08-09-ubuntu-14.04-dnsmasq/</guid>
      <description>作为技术备忘，为了纪念为此死去的千千万万脑细胞，先说问题：
公司搬家后，一台工作站上的爬虫程序突然莫名其妙的变慢，从一天20多万条数据萎缩到了14W-20W条不稳定，因为新办公室里所有部门共用一条网线，起初还以为是网络拥挤，属于正常情况，后来居然夸张的变成了每天3W条数据，经过跟踪发现这个爬虫全天候不分时间段的慢，这就很不对了。然后开始一系列的排查。
后来把爬虫里面用域名访问的代码改成使用IP地址访问，发现速度连接速度不慢，但是一使用域名就变龟速了，很明显域名解析出问题了，作为临时方案，我居然手写了一段IP地址缓存的代码，:(&amp;lt;
吐槽一下：Ubuntu系统配置DNS一点都不方便！！
发现工作站上的DNS服务器被配置成了8.8.8.8，然而这段时间Google的DNS抽风了，经常连接不上，所以解决办法就很简单了。
研究配置DNS的过程中，被灌输了一堆新名词：dnsmasq, resolv.conf, NetworkManager, resolvconf &amp;hellip;
没看全部的文档，根据查阅的部分资料，Ubuntu的DNS机制大概是这样的：
 NetworkManager管理DNS配置，图形界面“编辑连接”弹出来的就是这货，这里配置的DNS存储在/run/resolvconf/interface/NetworkManager文件中。 系统范围DNS的基本配置在/etc/resolvconf/resolv.conf.d/这个目录下面，这里有三个文件：base中是基本配置，系统动态生成resolv.conf文件时要包含的内容；head也是要包含在resolv.conf文件中的，里面主要包含一些说明，比如不要修改我会被覆盖之类的；tail里是要追加到动态生成的resolv.conf文件末尾的内容。 从Ubuntu 12.04开始，系统默认都会安装一个阉割版的dnsmasq服务（dnsmasq-base包，不是dnsmasq包），主要是为了解决DNS和VPN之间的什么什么暧昧问题的，监听在127.0.1.1:53地址上，系统DNS请求都会发到这个监听地址上。 系统启动或者执行resolvconf -u命令的时候，会根据配置动态生成DNS配置文件/ete/resolv.conf。  这个理解必然是不全面的，甚至有些完全就是错的，留待下次再搞DNS问题的时候研究吧。
相关的解决方法
安装完整功能的Dnsmasq：sudo apt-get install dnsmasq
配置Dnsmasq上游DNS服务器和本地监听地址，修改/etc/dnsmasq.conf文件：
listen-address=127.0.0.1,127.0.1.1 resolv-file=/etc/resolv.dnsmasq conf-dir=/etc/dnsmasq.d  /etc/resolv.dnsmasq文件里面写上游DNS服务器地址：
nameserver 114.114.114.114 nameserver 223.5.5.5 nameserver 114.114.115.115 nameserver 223.6.6.6  其他dnsmasq配置自由发挥就好啦～
修改系统DNS解析配置/etc/resolvconf/resolv.conf.d/base：
nameserver 127.0.0.1 nameserver 114.114.114.114 nameserver 223.5.5.5  然后启动dnsmasq服务：sudo service dnsmasq restart，更新DNS动态配置：sudo resolvconf -u。
以上步骤可能就解决问题了，也可能还会有问题，也可能会引入新的问题，比如参考资料4中提到的系统启动时候服务启动顺序的问题～～
Dnsmasq扩展阅读：
Dnsmasq提供DNS缓存和DHCP服务功能。作为域名解析服务器(DNS)，dnsmasq可以通过缓存DNS请求来提高对访问过的网址的连接速度。作为DHCP服务器，dnsmasq可以用于为局域网电脑分配内网ip地址和提供路由。DNS和DHCP两个功能可以同时或分别单独实现。dnsmasq轻量且易配置，适用于个人用户或少于50台主机的网络。此外它还自带了一个PXE服务器。&amp;gt;&amp;gt;&amp;gt;
不使用Dnsmasq直接设置系统的DNS服务器
修改/etc/resolvconf/resolv.conf.d/base文件中DNS配置：
nameserver 114.114.114.114 nameserver 223.5.5.5 nameserver 114.114.115.115 nameserver 223.6.6.6  参考资料：</description>
    </item>
    
    <item>
      <title>使用清华大学Anaconda镜像</title>
      <link>http://hshsh.me/post/2016-07-30-using-tsinghua-anaconda-mirror/</link>
      <pubDate>Sat, 30 Jul 2016 07:00:00 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-07-30-using-tsinghua-anaconda-mirror/</guid>
      <description>Python的Anaconda发行版用起来真是舒服，可是官方源的速度真可谓是龟速，一直也没找到国内的镜像源。 早上看到IPython更新到5.0LTS版本，看起来很爽，不死心的又查了一下，发现清华大学2016年4月27日新增了Anaconda的镜像，果断切换。
看这里：https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/
conda config --add channels &#39;https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/&#39; conda config --set show_channel_urls yes  </description>
    </item>
    
    <item>
      <title>[转载] 专栏：RabbitMQ从入门到精通</title>
      <link>http://hshsh.me/post/2016-04-24-rabbitmq-topic-articles/</link>
      <pubDate>Sun, 24 Apr 2016 22:48:11 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-04-24-rabbitmq-topic-articles/</guid>
      <description>最近有用到RabbitMQ，在网上搜到几篇介绍文章，除去CSDN的排版不说，文章内容还是很好的。
原文网址：http://blog.csdn.net/column/details/rabbitmq.html
下面是几篇文章的摘要，详情请跳转原文阅读。
文章摘要 RabbitMQ是一个在AMQP基础上完整的，可复用的企业消息系统。它可以用于大型软件系统各个模块之间的高效通信，支持高并发，支持可扩展。
RabbitMQ消息队列（一）: Detailed Introduction 详细介绍 对于一个大型的软件系统来说，它会有很多的组件或者说模块或者说子系统或者（subsystem or Component or submodule）。那么这些模块的如何通信？这和传统的IPC有很大的区别。传统的IPC很多都是在单一系统上的，模块耦合性很大，不适合扩展（Scalability）；如果使用socket那么不同的模块的确可以部署到不同的机器上，但是还是有很多问题需要解决。比如：
 信息的发送者和接收者如何维持这个连接，如果一方的连接中断，这期间的数据如何防止丢失？ 如何降低发送者和接收者的耦合度？ 如何让Priority高的接收者先接到数据？ 如何做到load balance？有效均衡接收者的负载？ 如何有效的将数据发送到相关的接收者？也就是说让接收者subscribe不同的数据，如何做有效的filter。 如何做到可扩展，甚至将这个通信模块发到cluster上？ 如何保证接收者接收到了完整，正确的数据？  AMDQ协议解决了以上的问题，而RabbitMQ实现了AMQP。
RabbitMQ消息队列（二）：“Hello, World” 首先复习一下上篇所学：RabbitMQ实现了AMQP定义的消息队列。它实现的功能“非常简单”：从Producer接收数据然后传递到Consumer。它能保证多并发，数据安全传递，可扩展。
和任何的Hello World一样，它们都不复杂。我们将会设计两个程序，一个发送Hello world，另一个接收这个数据并且打印到屏幕。
RabbitMQ消息队列（三）：任务分发机制 在上篇文章中，我们解决了从发送端（Producer）向接收端（Consumer）发送“Hello World”的问题。在实际的应用场景中，这是远远不够的。从本篇文章开始，我们将结合更加实际的应用场景来讲解更多的高级用法。
当有Consumer需要大量的运算时，RabbitMQ Server需要一定的分发机制来balance每个Consumer的load。试想一下，对于web application来说，在一个很多的HTTP request里是没有时间来处理复杂的运算的，只能通过后台的一些工作线程来完成。接下来我们分别讲解。
应用场景就是RabbitMQ Server会将queue的Message分发给不同的Consumer以处理计算密集型的任务。
RabbitMQ消息队列（四）：分发到多Consumer（Publish/Subscribe） 这篇文章中，我们将创建一个日志系统，它包含两个部分：第一个部分是发出log（Producer），第二个部分接收到并打印（Consumer）。我们将构建两个Consumer，第一个将log写到物理磁盘上；第二个将log输出的屏幕。
RabbitMQ 的Messaging Model就是Producer并不会直接发送Message到queue。实际上，Producer并不知道它发送的Message是否已经到达queue。
Producer发送的Message实际上是发到了Exchange中。它的功能也很简单：从Producer接收Message，然后投递到queue中。Exchange需要知道如何处理Message，是把它放到那个queue中，还是放到多个queue中？这个rule是通过Exchange的类型定义的。
我们知道有三种类型的Exchange：direct, topic 和fanout。fanout就是广播模式，会将所有的Message都放到它所知道的queue中。
RabbitMQ消息队列（五）：Routing 消息路由 上篇文章中，我们构建了一个简单的日志系统。接下来，我们将丰富它：能够使用不同的severity来监听不同等级的log。比如我们希望只有error的log才保存到磁盘上。
RabbitMQ消息队列（六）：使用主题进行消息分发 在上篇文章中，我们实现了一个简单的日志系统。Consumer可以监听不同severity的log。但是，这也是它之所以叫做简单日志系统的原因，因为是仅仅能够通过severity设定。不支持更多的标准。
比如syslog unix的日志工具，它可以通过severity (info/warn/crit&amp;hellip;) 和模块(auth/cron/kern&amp;hellip;)。这可能更是我们想要的：我们可以仅仅需要cron模块的log。
为了实现类似的功能，我们需要用到topic exchange。
RabbitMQ消息队列（七）：适用于云计算集群的远程调用（RPC） 在云计算环境中，很多时候需要用它其他机器的计算资源，我们有可能会在接收到Message进行处理时，会把一部分计算任务分配到其他节点来完成。那么，RabbitMQ如何使用RPC呢？在本篇文章中，我们将会通过其它节点来求斐波纳契完成示例。
为了展示一个RPC服务是如何使用的，我们将创建一段很简单的客户端class。它将会向外提供名字为call的函数，这个call会发送RPC请求并且阻塞知道收到RPC运算的结果。
RabbitMQ消息队列的小伙伴: ProtoBuf（Google Protocol Buffer） 什么是ProtoBuf？
一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。目前提供了 C++、Java、Python 三种语言的 API。</description>
    </item>
    
    <item>
      <title>Pandas学习笔记（二）：基本数据结构</title>
      <link>http://hshsh.me/post/2016-04-13-python-pandas-notes-02/</link>
      <pubDate>Wed, 13 Apr 2016 18:02:04 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-04-13-python-pandas-notes-02/</guid>
      <description>Pandas的开发者是：Wes McKinney，这位大牛工作的时候没有顺手的工具，就决定自己顺手写一个出来。
Pandas具有但不限于一下特点：
 具备按轴自动或显式数据对齐功能的数据结构，这可以防止许多由于数据没有对齐以及来自不同数据源（索引方式不同）的数据而导致的常见错误； 集成时间序列功能； 既能处理时间序列数据也能处理非时间序列数据的数据结构； 数学运算和约简（比如对某个轴求和）可以根据不同的元数据（轴编号）执行； 可以灵活处理缺失数据； 合并及其他出现在常见数据库（例如基于SQL的）中的关系型运算。  Pandas的导入约定：
from pandas import Series, DataFrame import pandas as pd import numpy as np  Series Series可以使用列表初始化，初始化时还可以指定索引名称。
Series可以被看成时一个定长的有序字典，因为它时索引值到数据值的一个映射，它可以用在许多原本需要字典参数的函数中。
如果数据被存放在一个Python字典中，也可以直接通过这个字典来创建Series。通过指定 index 可以只选择需要的对象，缺失值使用NaN自动填充。
obj = Series([4, 7, -5, 3]) obj, obj.values, obj.index obj = Series([4, 7, -5, 3], index=[&#39;d&#39;, &#39;b&#39;, &#39;a&#39;, &#39;c&#39;]) obj[obj &amp;gt; 0] obj * 2 np.exp(obj) &#39;b&#39; in obj sdata = {&#39;Ohio&#39;: 35000, &#39;Texas&#39;: 71000, &#39;Oregon&#39;: 16000, &#39;Utah&#39;: 5000} sdata = Series(sdata) sdata  Ohio 35000 Oregon 16000 Texas 71000 Utah 5000 dtype: int64  states = [&#39;California&#39;, &#39;Ohio&#39;, &#39;Oregon&#39;, &#39;Texas&#39;] obj = Series(sdata, index=states) obj  California NaN Ohio 35000.</description>
    </item>
    
    <item>
      <title>Pandas学习笔记（一）：CSV数据加载保存</title>
      <link>http://hshsh.me/post/2016-04-12-python-pandas-notes-01/</link>
      <pubDate>Tue, 12 Apr 2016 18:52:25 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-04-12-python-pandas-notes-01/</guid>
      <description>加载CSV数据 很多数据都存储在CSV文件中，Pandas 为读取提供了一个强大的 read_csv 函数，这个函数接受很多可选参数，通过参数控制数据加载的方式，以及一些基本的清理工作。
pd.read_csv(filepath_or_buffer, sep=&#39;,&#39;, delimiter=None, header=&#39;infer&#39;, names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, iterator=False, chunksize=None, compression=&#39;infer&#39;, thousands=None, decimal=&#39;.&#39;, lineterminator=None, quotechar=&#39;&amp;quot;&#39;, quoting=0, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=False, error_bad_lines=True, warn_bad_lines=True, skip_footer=0, doublequote=True, delim_whitespace=False, as_recarray=False, compact_ints=False, use_unsigned=False, low_memory=True, buffer_lines=None, memory_map=False, float_precision=None) Returns result : DataFrame or TextParser  很多参数都是非常有用的，简要记录一下（详细文档请参考 help(pd.read_csv) 及官方文档）：</description>
    </item>
    
    <item>
      <title>Python代理类两例</title>
      <link>http://hshsh.me/post/2016-04-10-python-proxy-class-examples/</link>
      <pubDate>Sun, 10 Apr 2016 23:30:00 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-04-10-python-proxy-class-examples/</guid>
      <description>最近遇到MySQL的连接断开，MySQLdb报&amp;rsquo;(2006, MySQL server has gone away.)&amp;lsquo;错误的问题。
问题发生的环境是，客户端使用了长连接，程序启动的时候使用MySQLdb模块的connect方法建立一个数据库连接，程序运行期间一直使用这个连接。对于请求比较多的服务程序来说，这个方法不会出现问题，因为MySQL默认连接超时的时间设定是8小时，所以连接不会超时断开，也就不会报这个错误了。但是作为一个调试服务，请求频率可能低于8小时，就导致错误了。
由于不想修改很多具体实现的代码，所以使用代理类的方法对这个程序打了个补丁解决问题。
另外 SQLAlchemy 中的 scoped_session 也是一个代理类，实现也很有意思，这里一起分享一下这两个代理类。
MySQLdb Connection 代理类 先看看出问题的代码：
import MySQLdb conn = MySQLdb.connect(host=&#39;host&#39;, port=&#39;port&#39;, user=&#39;user&#39;, passwd=&#39;passwd&#39;, db=&#39;db&#39;) def use_mysql(): cursor = conn.cursor() cursor.execute(&#39;do something with mysql database;&#39;) cursor.close()  MySQLdb.connect函数返回的是一个Connection对象，当操作频率大于8个小时的时候，MySQL服务器就会关闭连接，然后下一次执行 cursor = conn.cursor() 的时候，就会报连接丢失的错误。为了既使用长连接，又能避免这个错误，我们可以封装一个Connection类的代理类，重载 cursor 方法，当有新的请求的时候，先检查连接是否还在，如果连接丢失的话，就重新连接数据库，然后再调用 Connection 类的 cursor 方法并返回结果：
import MySQLdb from MySQLdb.connections import Connection class ProxyConnection(Connection): def __init__(self, *args, **kwargs): # 保存数据库连接参数以备丢失时候重新连接 self._proxy_args = args self._proxy_kwargs = kwargs super(ProxyConnection, self).</description>
    </item>
    
    <item>
      <title>[转载] Python类引入机制</title>
      <link>http://hshsh.me/post/2016-04-08-python-import-schema/</link>
      <pubDate>Fri, 08 Apr 2016 09:27:54 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-04-08-python-import-schema/</guid>
      <description>本文转载自刘畅的博客，原文地址：https://github.com/Liuchang0812/slides/tree/master/pycon2015cn。本文所涉及到的代码在github上。
概述 Python 是一门优美简单、功能强大的动态语言。在刚刚接触这门语言时，我们会被其优美的格式、简洁的语法和无穷无尽的类库所震撼。在真正的将python应用到实际的项目中，你会遇到一些无法避免的问题。最让人困惑不解的问题有二类，一个 编码问题，另一个则是引用问题。
本文主要讨论关于Python中import的机制与实现、以及介绍一些有意思的Python Hooks。
Python 类库引入机制 首先，看一个简单的例子：
&amp;quot;&amp;quot;&amp;quot; 目录结构如下： ├── __init__.py ├── main.py └── string.py &amp;quot;&amp;quot;&amp;quot; # main.py 内容如下 import string print string.a # string.py 内容如下 a = 2  现在，考虑一下：
 当我们执行main.py的时候，会发生什么事情？ 在main.py文件执行到import string的时候，解释器导入的string类库是当前文件夹下的string.py还是系统标准库的string.py呢？ 如果明确的指明自己要引用的类库？  为了搞清楚上面的问题，我们需要了解关于Python类库引入的机制。
Python的两种引入机制 Python 提供了二种引入机制：
 relative import absolute import  relative import relative import 也叫作相对引入，在Python2.5及之前是默认的引入方法。它的使用方法如下：
from .string import a from ..string import a from ...string import a  这种引入方式使用一个点号来标识引入类库的精确位置。与linux的相对路径表示相似，一个点表示当前目录，每多一个点号则代表向上一层目录。</description>
    </item>
    
    <item>
      <title>SQLAlchemy学习笔记（一）</title>
      <link>http://hshsh.me/post/2016-04-04-sqlalchemy-study-notes-01/</link>
      <pubDate>Mon, 04 Apr 2016 19:18:09 +0800</pubDate>
      
      <guid>http://hshsh.me/post/2016-04-04-sqlalchemy-study-notes-01/</guid>
      <description>这篇笔记部分内容来自网络1，部分来自对《Essential SQLAlchemy》的学习和使用经验。
SQLAlchemy是 Python 编程语言下的一款开源软件。提供了SQL工具包及对象关系映射（ORM）工具，使用MIT许可证发行。
SQLAlchemy“采用简单的Python语言，为高效和高性能的数据库访问设计，实现了完整的企业级持久模型”。
SQLAlchemy的理念是，SQL数据库的量级和性能重要于对象集合；而对象集合的抽象又重要于表和行。因此，SQLAlchmey 采用了类似于Java里 Hibernate 的数据映射模型，而不是其他ORM框架采用的 Active Record 模型。不过，Elixir 和 declarative 等可选插件可以让用户使用声明语法。
SQLAlchemy首次发行于2006年2月，并迅速地在Python社区中最广泛使用的ORM工具之一，不亚于Django的ORM框架。
以上摘自维基百科。
使用SQLAlchemy有三种方式：
 使用 Raw SQL 使用 SQL Expression 使用 ORM  前两种方式可以统称为 core 方式。
对于绝大多数应用，推荐使用 SQLAlchemy，即使是使用 Raw SQL，也可以带来如下好处。
 内建数据库连接池。注意：如果是 SQLAlchemy + cx_oracle 的话，需要禁用 Connection Pool，否则会有异常。方法是设置sqlalchemy.poolclass为sqlalchemy.pool.NullPool 强大的日志功能（log） 数据库无关的写法，包括：SQL参数写法、LIMIT语法等 特别提一下，WHERE 条件的 == value 写法，如果value等于None，真正的SQL会转为 IS NULL  SQLAlchemy 的 Raw SQL 和 SQL Expression 比较：
 SQL Expression 的写法是纯 Python 代码，阅读性更好，尤其是在使用 insert() 方法时，字段名和取值成对出现。 Raw SQL 比 SQL Expression 更灵活，如果 SQL/DDL 很复杂，Raw SQL 就更有优势了。  常用数据库连接字符串 import sqlalchemy from sqlalchemy import create_engine # file database # sqlite = create_engine(&#39;sqlite:////absolute/path/to/database.</description>
    </item>
    
  </channel>
</rss>