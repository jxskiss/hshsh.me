<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-cn" lang="zh-cn">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.16-DEV" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> hshsh&#39;s little site &middot; hshsh&#39;s little site </title>

  
  <link rel="stylesheet" href="http://hshsh.me/css/poole.css">
  <link rel="stylesheet" href="http://hshsh.me/css/syntax.css">
  <link rel="stylesheet" href="http://hshsh.me/css/hyde.css">
  <link rel="stylesheet" href="//fonts.useso.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <link rel="stylesheet" href="//cdn.bootcss.com/highlight.js/9.2.0/styles/default.min.css">
  <link rel="stylesheet" href="http://hshsh.me/css/style.css">

  
  <script type="text/javascript" src="//cdn.bootcss.com/highlight.js/9.2.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="http://hshsh.me/index.xml" rel="alternate" type="application/rss+xml" title="hshsh&#39;s little site" />
</head>

<body class=" ">

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="http://hshsh.me/"><h1>hshsh&#39;s little site</h1></a>
      <p class="lead">
       假行僧 
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="/">Home</a> </li>
      <li><a href="/post/"><span class="navlink"> Blog </span></a></li>
    </ul>

    <p>&copy; 2014 - 2016<br>All rights reserved. </p>
  </div>
</div>


    <div class="content container">
<div class="posts">

      
  <div class="post">
    <h1 class="post-title">
      <a href="http://hshsh.me/post/2016-04-27-ubuntu-linux-shell-commands-notes/">
        Ubuntu &amp; Linux 常用命令笔记
      </a>
    </h1>

    <span class="post-date">Wed, Apr 27, 2016</span>

    

<h2 id="系统配置">系统配置</h2>

<h3 id="安装-oracle-jdk">安装 Oracle JDK</h3>

<pre><code class="language-sh">$ sudo add-apt-repository ppa:webupd8team/java
$ sudo apt-get update
$ sudo apt-get install oracle-java8-installer
$ sudo apt-get install oracle-java8-set-default
</code></pre>

<h3 id="永久性修改系统dns">永久性修改系统DNS</h3>

<p>编辑<code>/etc/network/interfaces</code>文件，在最后添加一行：</p>

<pre><code class="language-conf">dns-nameservers 8.8.8.8 8.8.4.4
</code></pre>

<p>或者可以修改<code>/etc/resolvconf/resolv.conf.d/base</code>文件，默认为空，在其中插入：</p>

<pre><code class="language-conf">nameserver 8.8.8.8
nameserver 8.8.4.4
</code></pre>

<p>如果有多个DNS，就每行添加一个。</p>

<p>NOTE：亲测，以上设置，需要重启系统后生效！</p>

<h2 id="常用命令行工具">常用命令行工具</h2>

<h3 id="查看进程">查看进程</h3>

<pre><code class="language-sh">$ ps ax
$ ps aux
$ ps ax | less
$ ps ax | grep ...
</code></pre>

<h3 id="查看端口">查看端口</h3>

<pre><code class="language-sh">$ netstat -tap | grep ...
$ netstat -na | grep ...
$ ss -tln | grep ...
</code></pre>

<p>查看指定进程占用的端口号：</p>

<pre><code class="language-sh">$ ps -ef | grep &quot;process name&quot;
</code></pre>

<p>根据进程ID查看招用端口号：</p>

<pre><code class="language-sh"># redhat
$ netstat -nltp | grep pid
# ubuntu
$ netstat -anp | grep pid
</code></pre>

<p>查看占用某个端口的进程：</p>

<pre><code class="language-sh">$ lsof -i:port
</code></pre>

<h3 id="监控日志文件">监控日志文件</h3>

<pre><code class="language-sh">$ tail -f /path/to/file.log
</code></pre>

<h3 id="重启-x-server">重启 X Server</h3>

<pre><code class="language-sh">$ cat /etc/X11/default-display-manager
$ sudo restart {DISPLAY_MANAGER}
</code></pre>

<h3 id="输出重定向">输出重定向</h3>

<pre><code class="language-sh">$ cat foo &gt; foo.txt  # 重定向标准输出到文件
$ cat foo 2&gt; foo.txt  # 重定向错误输出到文件
$ cat foo 2&gt;&amp;1  # 重定向错误输出到标准输出
$ cat foo &gt; foo.txt 2&gt;&amp;1  # 重定向标准输出和错误输出到文件
</code></pre>

<p>如果要写入的文件权限不够，可以这样（<code>-a</code>选项表示追加内容到文件）：</p>

<pre><code class="language-sh">$ sudo sh -c &quot;echo 'xxx'&quot; &gt; /path/to/somefile
$ echo 'xxx' | sudo tee -a /path/to/somefile
</code></pre>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://hshsh.me/post/2016-04-27-mysql-sql-commands-notes/">
        MySQL &amp; SQL 常用命令笔记
      </a>
    </h1>

    <span class="post-date">Wed, Apr 27, 2016</span>

    

<h2 id="查看数据库和数据表结构">查看数据库和数据表结构</h2>

<pre><code class="language-sh">$ mysql -u user -p
mysql&gt; show databases;
mysql&gt; show tables;
mysql&gt; use {DATABASE_NAME};
mysql&gt; show columns from {TABLE_NAME};
mysql&gt; show create table {TABLE_NAME};
mysql&gt;
</code></pre>

<h2 id="变更数据表结构">变更数据表结构</h2>

<pre><code class="language-sql"># 重命名字段并可选修改字段类型
alter table {TABLE_NAME} change {OLD_COLUMN} {NEW_COLUMN} {COLUMN_TYPE};
# 修改字段类型不重命名字段
alter table {TABLE_NAME} modify {COLUMN_NAME} {COLUMN_TYPE};
# 增加字段
alter table {TABLE_NAME} add column {COLUMN_NAME} {COLUMN_TYPE};
# 删除字段
alter table {TABLE_NAME} drop column {COLUMN_NAME};
</code></pre>

<h2 id="快速批量加载数据到数据库">快速批量加载数据到数据库</h2>

<p><strong>NOTE</strong>: 下面命令中<code>{...}</code>表示必填参数，<code>[...]</code>表示选项参数</p>

<pre><code class="language-sql"># 逗号分隔的 csv 文件, 字段列表可选
load data local infile &quot;/path/to/file.csv&quot; into table {TABLE_NAME}
fileds terminated by ',' [(field1, field2, field3, ...)];
# 制表符分隔的 txt 文件, 字段列表可选
laod data local infile &quot;/path/to/file.txt&quot; into table {TABLE_NAME}
[(field1, field2, field3, ...)];
</code></pre>

<p>可能会遇到<code>ERROR 1148 (42000): The used command is not allowed with this
MySQL version</code>的错误提示，错误原因是编译安装<code>mysql</code>的时候没有指定<code>--enable-local-infile</code>
选项，除了重新编译安装加上上面的参数外，还可以直接使用命令行执行：</p>

<pre><code class="language-sh">$ mysql -u user -p {DATABASE_NAME} --local-infile=1 -e 'load data local \
  infile &quot;/path/to/file.txt&quot; into {TABLE_NAME} [(field1, field2, field3, ...)];'
</code></pre>

<h2 id="数据库导出操作">数据库导出操作</h2>

<p>导出全部数据库备份到本地目录:</p>

<pre><code class="language-sh">$ mysqldump -u$USER -p$PASSWORD -h127.0.0.1 -P3306 --routines \
  --default-character-set=utf8 --locak-all-tables --add-drop-database -A \
  &gt; db.all.sql
</code></pre>

<p>导出指定数据库到本地目录：</p>

<pre><code class="language-sh">$ mysqldump -u$USER -p$PASSWORD -h127.0.0.1 -P3306 --routines \
  --default-character-set=utf8 --databases {DATABASE_NAME} &gt; db.sql
</code></pre>

<p>导出某个数据库的表到本地目录：</p>

<pre><code class="language-sh">$ mysqldump -u$USER -p$PASSWORD -h127.0.0.1 -P3306 --routines \
  --default-character-set=utf8 --tables {DATABASE_NAME} {TABLE_NAME} \
  &gt; db.table.sql
</code></pre>

<p>导出指定数据库的表（仅数据，可带过滤条件）到本地目录：</p>

<pre><code class="language-sh">$ mysqldump -u$USER -p$PASSWORD -h127.0.0.1 -P3306 --routines \
  --default-character-set=utf8 --no-create-db --no-create-info \
  --tables {DATABASE_NAME} {TABLE_NAME} \
  [--where=&quot;host='localhost'&quot;] &gt; db.table.sql
</code></pre>

<p>导出数据库的所有表结构：</p>

<pre><code class="language-sh">$ mysqldump -u$USER -p$PASSWORD -h127.0.0.1 -P3306 --routines \
  --default-character-set=utf8 --no-data --databases {DATABASE_NAME} \
  &gt; db.nodata.sql
</code></pre>

<p>导出某个查询SQL的数据为 txt 格式文件到本地目录，各数据值之间用制表符分隔：</p>

<pre><code class="language-sh">$ mysql -u$USER -p$PASSWORD -h127.0.0.1 -P3306 --default-character-set=utf8 \
  --skip-column-names -B -e 'select ... from ... ;' &gt; /path/to/file.txt
</code></pre>

<p>导出某个查询SQL的数据为 csv 格式文件到服务器：</p>

<pre><code class="language-sh">$ mysql -u$USER -p$PASSWORD -h127.0.0.1 -P3306 --default-character-set=UTF8
mysql&gt; select ... from ... into outfile '/path/to/file.csv' fields terminated by ',';
mysql&gt;
</code></pre>

<h2 id="数据库导入操作">数据库导入操作</h2>

<p>恢复全库数据到MySQL，因为包含mysql库的权限表，导入完成后需要执行
<code>FLUSH PRIVILEGES;</code>命令：</p>

<pre><code class="language-sh">$ mysql -u$USER -p$PASSWORD -h127.0.0.1 -P3306 \
  --default-character-set=UTF8 &lt; db.all.sql

# 方法二
$ mysql -u$USER -p$PASSWORD -h127.0.0.1 -P3306 --default-=character-set=UTF8
mysql&gt; source /path/to/db.all.sql
mysql&gt; flush privileges;
</code></pre>

<p>恢复某个数据库：</p>

<pre><code class="language-sh">$ mysql -u$USER -p$PASSWORD -h$HOST -P$PORT --default-character-set=UTF8 \
  {DATABASE_NAME} &lt; db.table.sql

# 方法二
$ mysql -u$USER -p$PASSWORD -h$HOST -P$PORT --default-character-set=UTF8
mysql&gt; use {DATABASE_NAME};
mysql&gt; source /path/to/db.table.sql;
</code></pre>

<p>恢复MySQL服务器上面的 txt 格式文件（需要FILE权限，数据值之间用制表符分隔）：</p>

<pre><code class="language-sh">$ mysql -u$USER -p$PASSWORD -h$HOST -P$PORT --default-character-set=UTF8
mysql&gt; use {DATABASE_NAME};
mysql&gt; load data infile '/path/to/file.txt' into table {TABLE_NAME};
mysql&gt;
</code></pre>

<p>恢复MySQL服务器上的 csv 格式文件（需要FILE权限，数据值之间用逗号分隔）：</p>

<pre><code class="language-sh">$ mysql -u$USER -p$PASSWORD -h$HOST -P$PORT --default-character-set=UTF8
mysql&gt; use {DATABASE_NAME};
mysql&gt; load data infile '/path/to/file.csv' into table {TABLE_NAME}
mysql&gt; fields terminated by ',';
mysql&gt;
</code></pre>

<p>恢复本地的 txt 或 csv 文件到MySQL：</p>

<pre><code class="language-sh">$ mysql -u$USER -p$PASSWORD -h$HOST -P$PORT --default-character-set=UTF8
mysql&gt; use {DATABASE_NAME};
mysql&gt; load data local infile '/path/to/file.txt' into table {TABLE_NAME};
mysql&gt; load data local infile '/path/to/file.csv' into table {TABLE_NAME}
mysql&gt; fields terminated by ',';
mysql&gt;
</code></pre>

<h2 id="常用命令参数说明">常用命令参数说明</h2>

<p>mysqldump参数说明：</p>

<ol>
<li><strong>-A</strong>: 全库备份</li>
<li><strong>&ndash;routines</strong>: 备份存储过程和函数</li>
<li><strong>&ndash;default-character-set=utf8</strong>: 设置连接字符集</li>
<li><strong>&ndash;lock-all-tables</strong>: 全局一致性锁</li>
<li><strong>&ndash;add-drop-database</strong>: 在每次执行建表语句之前，先执行<code>drop table if exist</code>语句</li>
<li><strong>&ndash;no-create-db</strong>: 不输出<code>create database</code>语句</li>
<li><strong>&ndash;no-create-info</strong>: 不输出<code>create table</code>语句</li>
<li><strong>&ndash;databases</strong>: 将后面的参数都解析为数据库名</li>
<li><strong>&ndash;tables</strong>: 第一个参数为数据库名，后续参数为数据表名</li>
</ol>

<p>mysql参数说明：</p>

<ol>
<li><strong>&ndash;skip-column-names</strong>: 不显示数据列的名字</li>
<li><strong>-B</strong>: 以批处理的方式运行mysql程序，查询结果将显示为制表符间隔格式</li>
<li><strong>-e</strong>: 执行命令后退出</li>
</ol>

<p><code>LOAD DATA</code>语法：</p>

<ol>
<li>如果<code>LOAD DATA</code>语句不带<code>LOCAL</code>关键字，就在MySQL的服务器上直接读取文件，
需要具有FILE权限</li>
<li>如果带有<code>LOCAL</code>关键字，就在客户端本地读取数据文件，通过网络传输到MySQL</li>
<li><code>LOAD DATA</code>语句，同样会被记录到<code>binlog</code>，不过是MySQL内部的机制</li>
</ol>

<h2 id="设置默认使用utf8编码">设置默认使用utf8编码</h2>

<pre><code class="language-conf"># configuration in file /etc/mysql/my.cnf
[client]
# 客户端连接
default-character-set = utf8
[mysql]
# 命令行工具
default-character-set = utf8
[mysqld]
# 服务器默认字符集
character-set-server = utf8
</code></pre>

<h2 id="mysqld-服务管理">mysqld 服务管理</h2>

<pre><code class="language-sh">$ sudo service mysql {start | stop | restart}
$ sudo /etc/init.d/mysql {start | stop | restart}

# safe 模式启动
$ sudo safe_mysqld &amp;

# mysqld 守护进程管理程序.
$ mysqladmin shutdown
$ mysqladmin --help
</code></pre>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://hshsh.me/post/2016-04-26-mysql-hot-backup-with-xtrabackup/">
        XtraBackup热备份MySQL主从同步笔记
      </a>
    </h1>

    <span class="post-date">Tue, Apr 26, 2016</span>

    

<p>公司的MySQL数据库单实例裸跑了一个多月，今天终于做了主从同步，暂时主要起备份作用，庆幸这段时间没有发生意外。</p>

<p>先说主要的参考资料，强烈推荐阅读：</p>

<ul>
<li><a href="http://wsgzao.github.io/post/xtrabackup/">XtraBackup不停机不锁表搭建MySQL主从同步实践</a></li>
<li><a href="http://seanlook.com/2015/12/14/mysql-replicas/">使用 Xtrabackup 在线对MySQL做主从复制</a></li>
<li><a href="https://segmentfault.com/a/1190000002575399">通过XtraBackup实现不停机不锁表搭建主从同步</a></li>
</ul>

<h2 id="简介">简介</h2>

<p>转载一下主从同步和XtraBackup的简介：</p>

<p><strong>MySQL主从同步原理</strong></p>

<p>MySQL主从同步是在MySQL主从复制(Master-Slave Replication)基础上实现的，通过设置在Master MySQL上的binlog(使其处于打开状态)，Slave MySQL上通过一个I/O线程从Master MySQL上读取binlog，然后传输到Slave MySQL的中继日志中，然后Slave MySQL的SQL线程从中继日志中读取中继日志，然后应用到Slave MySQL的数据库中。这样实现了主从数据同步功能。</p>

<p><strong>XtraBackup备份原理</strong></p>

<p>innobackupex在后台线程不断追踪InnoDB的日志文件，然后复制InnoDB的数据文件。数据文件复制完成之后，日志的复制线程也会结束。这样就得到了不在同一时间点的数据副本和开始备份以后的事务日志。完成上面的步骤之后，就可以使用InnoDB崩溃恢复代码执行事务日志（redo log），以达到数据的一致性。</p>

<p>备份分为两个过程：</p>

<ol>
<li>backup，备份阶段，追踪事务日志和复制数据文件（物理备份）。</li>
<li>preparing，重放事务日志，使所有的数据处于同一个时间点，达到一致性状态。</li>
</ol>

<p><strong>XtraBackup的优点</strong></p>

<ol>
<li>可以快速可靠的完成数据备份（复制数据文件和追踪事务日志）</li>
<li>数据备份过程中不会中断事务的处理（热备份）</li>
<li>节约磁盘空间和网络带宽</li>
<li>自动完成备份鉴定</li>
<li>因更快的恢复时间而提高在线时间</li>
</ol>

<h2 id="操作笔记">操作笔记</h2>

<p>参考的两篇文章里面说的挺详细的，但是有部分命令和命令执行顺序写的不大明白，这里简单记录以下。</p>

<p><strong>完整的步骤</strong></p>

<ol>
<li>主、从服务器上都搭好MySQL服务，从服务器上MySQL版本大于等于主服务器，最好完全一致</li>
<li>在要做主从同步的服务器上分别安装XtraBackup</li>
<li>如果从服务器上有MySQL实例，停掉服务，备份删除数据库内容，保留数据库目录</li>
<li>配置主从服务器打开主从同步功能</li>
<li>主服务器上执行备份</li>
<li>传输备份文件到从服务器，并同步数据文件（apply-log）</li>
<li>从服务器上恢复备份</li>
<li>主服务器上授权同步帐号</li>
<li>从服务器上设置MASTER并开启同步</li>
</ol>

<p>完成，可以检查同步状态了！</p>

<p><strong>具体操作过程</strong></p>

<p>NOTE：以下命令以普通用户权限运行，如果需要ROOT权限，均使用<code>sudo</code>执行。默认均使用Ubuntu发行版仓库中的MySQL，版本比较旧，如果使用官方发行版本，需要注意相关选项、目录等配置。</p>

<p>一、主从服务器上搭建MySQL服务，并检查MySQL版本：</p>

<pre><code class="language-bash"># master &amp; slave
sudo apt-get install mysql-server
mysql --version
</code></pre>

<blockquote>
<p>mysql  Ver 14.14 Distrib 5.5.49, for debian-linux-gnu (x86_64) using readline 6.3</p>
</blockquote>

<p>二、主从服务器上分别安装XtraBackup，根据官方网站指导使用打包好的二进制，选择最新的稳定版2.4：</p>

<pre><code class="language-bash"># master &amp; slave
wget https://repo.percona.com/apt/percona-release_0.1-3.$(lsb_release -sc)_all.deb
sudo dpkg -i percona-release_0.1-3.$(lsb_release -sc)_all.deb
sudo apt-get update
sudo apt-get install percona-xtrabackup-24
</code></pre>

<p>三、停掉从服务器上MySQL服务，备份原有数据库，并删除原有数据库内容：</p>

<pre><code class="language-bash">mysqldump -u$USER -p$PASSWORD -h127.0.0.1 -P3306 --routines \
  --default-character-set=utf8 --locak-all-tables --add-drop-database -A \
  db.all.sql
sudo service mysql stop
sudo cd /var/lib/mysql
# 下面这句千万别打错了，后果会很严重
sudo rm -rf ./*
</code></pre>

<p>四、配置MySQL打开主从同步功能</p>

<p>主服务器上编辑<code>/etc/mysql/my.conf</code>文件：</p>

<pre><code class="language-conf">[mysqld]
# 注意主从之间的server-id不能相同
server-id    = 1
log_bin      = /var/log/mysql/mysql-bin.log
</code></pre>

<p>如果主服务器上MySQL是已经上线的系统，需要重启一下（实测<code>/etc/init.d/mysql reload</code>不起作用）：</p>

<pre><code class="language-bash">sudo service mysql restart
</code></pre>

<p>从服务器上编辑<code>/etc/mysql/my.conf</code>文件：</p>

<pre><code class="language-conf">[mysqld]
# 注意主从之间的server-id不能相同
server-id    = 2
# 最好设置从服务器为只读
# 注意：即使这里设置了只读，使用具有super权限的用户登录，也还是可以做写操作的
read_only    = ON
</code></pre>

<p>查询主从服务器状态：</p>

<pre><code class="language-bash">mysql -u USER -p PASSWD -e &quot;show global variables like 'server-id';&quot;
    +---------------+-------+
    | Variable_name | Value |
    +---------------+-------+
    | server_id     | 1     |
    +---------------+-------+

mysql -u USER -p PASSWD -e &quot;show global variables like 'log_bin';&quot;
    +---------------+-------+
    | Variable_name | Value |
    +---------------+-------+
    | log_bin       | ON    |
    +---------------+-------+
</code></pre>

<p>五、主服务器上执行备份操作</p>

<pre><code class="language-bash">sudo innobackupex --user=USER --password --defaults-file=/etc/mysql/my.cnf \
  --parallel=4 /tmp/mybackup
</code></pre>

<p>命令输出的最后几行通常类似这样：</p>

<pre><code>innobackupex: Backup created in directory '/tmp/mybackup/2016-04-26_17-41-51'
innobackupex: MySQL binlog position: filename 'mysql-bin.000003', position 1946
111225 00:00:53 innobackupex: completed OK!
</code></pre>

<p>命令执行完在<code>/tmp/mybackup</code>目录下生成的<code>2016-04-26_17-41-51</code>目录，里面存储的是备份的数据，下一步要传输到从服务器上的即是这个文件夹。</p>

<p>输出中的<code>MySQL binlog position: filename 'mysql-bin.000003', position 1946</code>里面的两个数字，要记录以下，后面恢复到从服务器上的时候要用到。</p>

<p>六、传输并同步备份数据</p>

<p>读取备份数据需要ROOT权限，下面的命令需要使用sudo执行。</p>

<pre><code class="language-bash">mkdir /tmp/mybackup
sudo scp -r /tmp/mybackup/2016-04-26_17-44-49 USER@SLAVE:/tmp/mybackup/2016-04-26
</code></pre>

<p>在从服务器上执行：</p>

<pre><code class="language-bash">sudo innobackupex --apply-log /tmp/mybackup/2016-04-26
</code></pre>

<p>七、从服务器上恢复备份数据</p>

<pre><code class="language-bash"># 恢复数据
sudo innobackupex --user=USER --password --defaults-file=/etc/mysql/my.cnf \
  --copy-back /tmp/mybackup/2016-04-26/
# 需要恢复权限给mysql
sudo chown -R mysql:mysql /var/lib/mysql
# 启动MySQL
sudo service mysql start
</code></pre>

<p>NOTE: 如果从数据库存在多个MySQL，执行命令有所不同，请另行查阅相关资料。</p>

<p>八、主服务器上授权同步帐号</p>

<pre><code class="language-sql">mysql -u USER -p PASSWD -h HOST -P PORT
&gt; grant replication slave on *.* to 'slave'@'10.10.16.24' identified by 'slave_passport';
&gt; flush privileges;
&gt; 
&gt; select distinct concat('User: ''',user,'''@''',host,''';') as query from mysql.user;
&gt; 
</code></pre>

<p>最后一条语句查询当前数据库中的用户信息，检查<code>slave_passport</code>是否在其中。</p>

<p>九、配置从服务器开启同步</p>

<pre><code class="language-sql">mysql -u USER -p PASSWD -h HOST -p PORT
&gt; change master to
&gt; master_host = '10.10.16.51',
&gt; master_user = 'slave',
&gt; master_password = 'slave_password',
&gt; master_port = 3306,
&gt; master_log_file = 'mysql-bin.000003',
&gt; master_log_pos = 1946;
&gt; 
&gt; start slave;
</code></pre>

<p>查看主库同步状态：</p>

<pre><code class="language-bash">mysql -u USER -p PASSWD -h MASTER_HOST -P MASTER_PORT \
  -e &quot;show master status \G;&quot;
mysql -u USER -p PASSWD -h SLAVE_HOST -P SLAVE_PORT \
  -e &quot;show processlist \G;&quot; | grep -i 'master'
</code></pre>

<p>检查第二条命令输出是否类似“State: Master has sent all binlog to slave; waiting for binlog to be updated”这样。</p>

<p>查看从库同步状态：</p>

<pre><code class="language-bash">mysql -u USER -p PASSWD -h SLAVE_HOST -P SLAVE_PORT \
  -e &quot;show slave status \G;&quot;
mysql -u USER -p PASSWD -h SLAVE_HOST -P SLAVE_PORT \
  -e &quot;show processlist \G;&quot; | egrep -i '(master|slave)'
</code></pre>

<p>检查命令输出是否包含类似下面这样的语句：</p>

<pre><code>Slave_IO_State: Waiting for master to send event
Slave_IO_Running: Yes
Slave_SQL_Running: Yes
Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it

State: Waiting for master to send event
State: Slave has read all relay log; waiting for the slave I/O thread to update it
</code></pre>

<h2 id="mysql主从切换">MySQL主从切换</h2>

<p>这里暂时还没有用到主从切换，不过参考资料<a href="http://wsgzao.github.io/post/xtrabackup/">XtraBackup不停机不锁表搭建MySQL主从同步实践</a>中有写到主从切换的过程，复制粘贴一下以后好找：</p>

<pre><code class="language-sql">#查看主库状态
show processlist;
Master has sent all binlog to slave; waiting for binlog to be updated
show master status \G

#从库停止 IO_THREAD 线程
stop slave IO_THREAD;
show processlist;
Slave has read all relay log; waiting for the slave I/O thread to update it
show slave status \G

#从库切换为主库
stop slave;
reset master;
reset slave all;
show master status \G

#激活帐户
SELECT DISTINCT CONCAT('User: ''',user,'''@''',host,''';') AS query FROM mysql.user;
GRANT REPLICATION SLAVE ON *.* TO 'slave_passport'@'10.10.16.51' IDENTIFIED BY 'slave_passport';
FLUSH PRIVILEGES;

#切换原有主库为从库
reset master;
reset slave all;

CHANGE MASTER TO
MASTER_HOST='10.10.16.24',
MASTER_USER='slave_passport',
MASTER_PASSWORD='slave_passport',
MASTER_PORT=3306,
MASTER_LOG_FILE='mysql-bin.000001',
MASTER_LOG_POS=804497686;

#检查主库
SHOW PROCESSLIST;
show master status \G

#启动从库
SHOW PROCESSLIST;
start slave;
show slave status \G
</code></pre>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://hshsh.me/post/2016-04-24-rabbitmq-topic-articles/">
        [转载] 专栏：RabbitMQ从入门到精通
      </a>
    </h1>

    <span class="post-date">Sun, Apr 24, 2016</span>

    

<p>最近有用到RabbitMQ，在网上搜到几篇介绍文章，除去CSDN的排版不说，文章内容还是很好的。</p>

<p>原文网址：<a href="http://blog.csdn.net/column/details/rabbitmq.html">http://blog.csdn.net/column/details/rabbitmq.html</a></p>

<p>下面是几篇文章的摘要，详情请跳转原文阅读。</p>

<h2 id="文章摘要">文章摘要</h2>

<p>RabbitMQ是一个在AMQP基础上完整的，可复用的企业消息系统。它可以用于大型软件系统各个模块之间的高效通信，支持高并发，支持可扩展。</p>

<h3 id="rabbitmq消息队列-一-detailed-introduction-详细介绍-http-blog-csdn-net-anzhsoft-article-details-19563091"><a href="http://blog.csdn.net/anzhsoft/article/details/19563091">RabbitMQ消息队列（一）: Detailed Introduction 详细介绍</a></h3>

<p>对于一个大型的软件系统来说，它会有很多的组件或者说模块或者说子系统或者（subsystem or Component or submodule）。那么这些模块的如何通信？这和传统的IPC有很大的区别。传统的IPC很多都是在单一系统上的，模块耦合性很大，不适合扩展（Scalability）；如果使用socket那么不同的模块的确可以部署到不同的机器上，但是还是有很多问题需要解决。比如：</p>

<ol>
<li>信息的发送者和接收者如何维持这个连接，如果一方的连接中断，这期间的数据如何防止丢失？</li>
<li>如何降低发送者和接收者的耦合度？</li>
<li>如何让Priority高的接收者先接到数据？</li>
<li>如何做到load balance？有效均衡接收者的负载？</li>
<li>如何有效的将数据发送到相关的接收者？也就是说让接收者subscribe不同的数据，如何做有效的filter。</li>
<li>如何做到可扩展，甚至将这个通信模块发到cluster上？</li>
<li>如何保证接收者接收到了完整，正确的数据？</li>
</ol>

<p>AMDQ协议解决了以上的问题，而RabbitMQ实现了AMQP。</p>

<h3 id="rabbitmq消息队列-二-hello-world-http-blog-csdn-net-anzhsoft-article-details-19570187"><a href="http://blog.csdn.net/anzhsoft/article/details/19570187">RabbitMQ消息队列（二）：“Hello, World”</a></h3>

<p>首先复习一下上篇所学：RabbitMQ实现了AMQP定义的消息队列。它实现的功能“非常简单”：从Producer接收数据然后传递到Consumer。它能保证多并发，数据安全传递，可扩展。</p>

<p>和任何的Hello World一样，它们都不复杂。我们将会设计两个程序，一个发送Hello world，另一个接收这个数据并且打印到屏幕。</p>

<h3 id="rabbitmq消息队列-三-任务分发机制-http-blog-csdn-net-anzhsoft-article-details-19607841"><a href="http://blog.csdn.net/anzhsoft/article/details/19607841">RabbitMQ消息队列（三）：任务分发机制</a></h3>

<p>在上篇文章中，我们解决了从发送端（Producer）向接收端（Consumer）发送“Hello World”的问题。在实际的应用场景中，这是远远不够的。从本篇文章开始，我们将结合更加实际的应用场景来讲解更多的高级用法。</p>

<p>当有Consumer需要大量的运算时，RabbitMQ Server需要一定的分发机制来balance每个Consumer的load。试想一下，对于web application来说，在一个很多的HTTP request里是没有时间来处理复杂的运算的，只能通过后台的一些工作线程来完成。接下来我们分别讲解。</p>

<p>应用场景就是RabbitMQ Server会将queue的Message分发给不同的Consumer以处理计算密集型的任务。</p>

<h3 id="rabbitmq消息队列-四-分发到多consumer-publish-subscribe-http-blog-csdn-net-anzhsoft-article-details-19617305"><a href="http://blog.csdn.net/anzhsoft/article/details/19617305">RabbitMQ消息队列（四）：分发到多Consumer（Publish/Subscribe）</a></h3>

<p>这篇文章中，我们将创建一个日志系统，它包含两个部分：第一个部分是发出log（Producer），第二个部分接收到并打印（Consumer）。我们将构建两个Consumer，第一个将log写到物理磁盘上；第二个将log输出的屏幕。</p>

<p>RabbitMQ 的Messaging Model就是Producer并不会直接发送Message到queue。实际上，Producer并不知道它发送的Message是否已经到达queue。</p>

<p>Producer发送的Message实际上是发到了Exchange中。它的功能也很简单：从Producer接收Message，然后投递到queue中。Exchange需要知道如何处理Message，是把它放到那个queue中，还是放到多个queue中？这个rule是通过Exchange的类型定义的。</p>

<p>我们知道有三种类型的Exchange：direct, topic 和fanout。fanout就是广播模式，会将所有的Message都放到它所知道的queue中。</p>

<h3 id="rabbitmq消息队列-五-routing-消息路由-http-blog-csdn-net-anzhsoft-article-details-19630147"><a href="http://blog.csdn.net/anzhsoft/article/details/19630147">RabbitMQ消息队列（五）：Routing 消息路由</a></h3>

<p>上篇文章中，我们构建了一个简单的日志系统。接下来，我们将丰富它：能够使用不同的severity来监听不同等级的log。比如我们希望只有error的log才保存到磁盘上。</p>

<h3 id="rabbitmq消息队列-六-使用主题进行消息分发-http-blog-csdn-net-anzhsoft-article-details-19633079"><a href="http://blog.csdn.net/anzhsoft/article/details/19633079">RabbitMQ消息队列（六）：使用主题进行消息分发</a></h3>

<p>在上篇文章中，我们实现了一个简单的日志系统。Consumer可以监听不同severity的log。但是，这也是它之所以叫做简单日志系统的原因，因为是仅仅能够通过severity设定。不支持更多的标准。</p>

<p>比如syslog unix的日志工具，它可以通过severity (info/warn/crit&hellip;) 和模块(auth/cron/kern&hellip;)。这可能更是我们想要的：我们可以仅仅需要cron模块的log。</p>

<p>为了实现类似的功能，我们需要用到topic exchange。</p>

<h3 id="rabbitmq消息队列-七-适用于云计算集群的远程调用-rpc-http-blog-csdn-net-anzhsoft-article-details-19633107"><a href="http://blog.csdn.net/anzhsoft/article/details/19633107">RabbitMQ消息队列（七）：适用于云计算集群的远程调用（RPC）</a></h3>

<p>在云计算环境中，很多时候需要用它其他机器的计算资源，我们有可能会在接收到Message进行处理时，会把一部分计算任务分配到其他节点来完成。那么，RabbitMQ如何使用RPC呢？在本篇文章中，我们将会通过其它节点来求斐波纳契完成示例。</p>

<p>为了展示一个RPC服务是如何使用的，我们将创建一段很简单的客户端class。它将会向外提供名字为call的函数，这个call会发送RPC请求并且阻塞知道收到RPC运算的结果。</p>

<h3 id="rabbitmq消息队列的小伙伴-protobuf-google-protocol-buffer-http-blog-csdn-net-anzhsoft-article-details-19771671"><a href="http://blog.csdn.net/anzhsoft/article/details/19771671">RabbitMQ消息队列的小伙伴: ProtoBuf（Google Protocol Buffer）</a></h3>

<p>什么是ProtoBuf？</p>

<p>一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。目前提供了 C++、Java、Python 三种语言的 API。</p>

<p>它可以作为RabbitMQ的Message的数据格式进行传输，由于是结构化的数据，这样就极大的方便了Consumer的数据高效处理。当然了你可能说使用XML不也可以吗？与XML相比，ProtoBuf有以下优势：</p>

<ol>
<li>简单</li>
<li>size小了3-10倍</li>
<li>速度快了20-100倍</li>
<li>易于编程</li>
<li>减小了语义的歧义</li>
</ol>

<h3 id="rabbitmq消息队列-九-publisher的消息确认机制-http-blog-csdn-net-anzhsoft-article-details-21603479"><a href="http://blog.csdn.net/anzhsoft/article/details/21603479">RabbitMQ消息队列（九）：Publisher的消息确认机制</a></h3>

<p>在前面的文章中提到了queue和consumer之间的消息确认机制：通过设置ack。那么Publisher能不到知道他post的Message有没有到达queue，甚至更近一步，是否被某个Consumer处理呢？毕竟对于一些非常重要的数据，可能Publisher需要确认某个消息已经被正确处理。</p>

<p>在我们的系统中，我们没有是实现这种确认，也就是说，不管Message是否被Consume了，Publisher不会去关心。他只是将自己的状态publish给上层，由上层的逻辑去处理。如果Message没有被正确处理，可能会导致某些状态丢失。但是由于提供了其他强制刷新全部状态的机制，因此这种异常情况的影响也就可以忽略不计了。</p>

<p>对于某些异步操作，比如客户端需要创建一个FileSystem，这个可能需要比较长的时间，甚至要数秒钟。这时候通过RPC可以解决这个问题。因此也就不存在Publisher端的确认机制了。</p>

<p>那么，有没有一种机制能保证Publisher能够感知它的Message有没有被处理的？答案肯定的。</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://hshsh.me/post/2016-04-24-welcome-to-my-hugo-blog/">
        welcome to my hugo blog
      </a>
    </h1>

    <span class="post-date">Sun, Apr 24, 2016</span>

    

<p>Welcome to my new <a href="https://gohugo.io/">Hugo</a> blog.</p>

<h2 id="为什么又建一个博客">为什么又建一个博客</h2>

<p>其实在07年开始的时候就建立了自己的博客，当时是在百度空间，内容主要是网上转载的学习资料。百度空间的排版真心费劲。后来百度空间倒下了，内容也都没有了。</p>

<p>后来，GAE开始流行，作为Google忠实的粉丝和一个不折腾会死星人，当然得尝试尝试了。博客搭建好了，但是却没写出什么东西来，一两年时间里，也只有寥寥数篇生活杂记。这时候，我还买了一个自己的域名。</p>

<p>时间大概是08年。又后来，Google退出中国，服务越来越难访问，也就没管那个站了，甚至现在都想不起来GAE上的那个二级域名是什么了。</p>

<p>然后，Github很火，很多人开始在Github Pages上搭建博客。了解之后，一下子又被吸引了，用Markdown书写的感觉特别爽，排版也漂亮，对代码高亮等扩展功能的支持也好。大概是13年的时候使用Jekyll在Github Pages上又搭建了一个博客，还把以前的一些乱七八糟的文字整理了过去，并且绑定了之前买的域名。这个网站现在还能访问：<a href="http://www.imwsh.net">http://www.imwsh.net</a>。</p>

<p>（号外：要是有人喜欢<code>imwsh.net</code>这个域名，联系我哈，便宜出售~ ~）</p>

<p>Github Pages上的博客搭好了，但是我却是跟IT越走越远，工作在传统行业，又跑去拉萨野了一趟，人又懒得很，也就不了了之了。</p>

<p>今年过完春节，贼心不死的我又在纠结工作的事情，最终在老婆和朋友的鼓励下，成功的找到了一份后端程序员的工作。于是乎又开始学习恶补，又惦记起来博客的事情，翻翻硬盘，那个博客的源码都已经被删了，而且对Ruby也不熟悉。当时正在学习Flask Web开发，干脆自己用Python写一个得了。</p>

<p>于是乎，互联网上便又多了一个博客程序：<a href="https://github.com/jxskiss/meblog">meblog</a>。</p>

<p>这个博客程序简单漂亮，也非常好用，代码高亮、标签、分类什么的都支持，我自己对这个博客程序也很满意，还写了一个命令行推送发布工具，然后爽歪歪的把博客部署到SAE上去了，好事不长久，还不到一个月，在新浪云上充值的豆子就用完了，于是乎，拉倒吧……不过，这个meblog却真的是麻雀很小，五脏俱全，如果有人需要一个Python写的博客程序的话，我强烈推荐！如果还能打开的话，你可以看看她长什么样：<a href="http://hshsh.applinzi.com">http://hshsh.applinzi.com</a></p>

<p>然后，了解到了hugo这个静态博客生成器，markdown书写、单文件执行、没有依赖，同样可以方便的部署到Github Pages。看起来就很好用啊，官方网站做的还很漂亮，干脆再搭一个博客算了。这便是现在这个博客了，轻车熟路整理markdown文件，生成网站，调整CSS样式，绑定域名，半天时间，网站就上线了。</p>

<p>另外，我还简单写了一个<a href="https://github.com/jxskiss/hshsh.me/blob/master/fabfile.py">自动部署脚本</a>，可以自动转换发布Jupyter Notebook文件哦。用Jupyter Notebook写笔记，然后自动发布称漂亮的网站，简直爽的不要不要的 ~ ~</p>

<p>至于搭建博客的过程，并不复杂，这里就不复制粘贴了，直接看下面的参考资料吧。</p>

<h2 id="参考资料">参考资料</h2>

<ul>
<li><a href="https://gohugo.io/tutorials/github-pages-blog/">Hugo - Hosting on GitHub Pages</a>：这个是主要参考资料</li>
<li><a href="http://ulricqin.com/post/how-to-use-hugo/">使用Hugo搭建免费个人Blog</a></li>
<li><a href="http://www.jianshu.com/p/b66754c0baa6">使用Hugo + Github搭建个人博客</a></li>
</ul>

<h2 id="hugo安装问题">Hugo安装问题</h2>

<p>Hugo官网上有编译好的二进制包发布，如果能直接下载到，最好不过了，放到执行文件路径里就可以了。</p>

<p>很可能的情况是根本下载不下来！！！这时候可以选择使用源码安装，也就是GO GET命令了：</p>

<pre><code class="language-bash">go get -v github.com/spf13/hugo
</code></pre>

<p>基本上没有代理你是完不成这条命令的，有了代理，还需要设置GIT和WGET等工具使用这个代理，如下：</p>

<pre><code class="language-bash">git config --global http.proxy http://127.0.0.1:7777
# 如果在Windows上，下面这条命令中的export换成set
export HTTP_PROXY=http://127.0.0.1:7777
</code></pre>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://hshsh.me/post/2016-04-13-python-pandas-notes-02/">
        Pandas学习笔记（二）：基本数据结构
      </a>
    </h1>

    <span class="post-date">Wed, Apr 13, 2016</span>

    

<p>Pandas的开发者是：<a href="http://wesmckinney.com">Wes McKinney</a>，这位大牛工作的时候没有顺手的工具，就决定自己顺手写一个出来。</p>

<p>Pandas具有但不限于一下特点：</p>

<ol>
<li>具备按轴自动或显式数据对齐功能的数据结构，这可以防止许多由于数据没有对齐以及来自不同数据源（索引方式不同）的数据而导致的常见错误；</li>
<li>集成时间序列功能；</li>
<li>既能处理时间序列数据也能处理非时间序列数据的数据结构；</li>
<li>数学运算和约简（比如对某个轴求和）可以根据不同的元数据（轴编号）执行；</li>
<li>可以灵活处理缺失数据；</li>
<li>合并及其他出现在常见数据库（例如基于SQL的）中的关系型运算。</li>
</ol>

<p>Pandas的导入约定：</p>

<pre><code class="language-python">from pandas import Series, DataFrame
import pandas as pd
import numpy as np
</code></pre>

<h2 id="series">Series</h2>

<p>Series可以使用列表初始化，初始化时还可以指定索引名称。</p>

<p>Series可以被看成时一个定长的有序字典，因为它时索引值到数据值的一个映射，它可以用在许多原本需要字典参数的函数中。</p>

<p>如果数据被存放在一个Python字典中，也可以直接通过这个字典来创建Series。通过指定 <code>index</code> 可以只选择需要的对象，缺失值使用NaN自动填充。</p>

<pre><code class="language-python">obj = Series([4, 7, -5, 3])
obj, obj.values, obj.index

obj = Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])

obj[obj &gt; 0]
obj * 2
np.exp(obj)

'b' in obj

sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}
sdata = Series(sdata)
sdata
</code></pre>

<pre><code>Ohio      35000
Oregon    16000
Texas     71000
Utah       5000
dtype: int64
</code></pre>

<pre><code class="language-python">states = ['California', 'Ohio', 'Oregon', 'Texas']
obj = Series(sdata, index=states)
obj
</code></pre>

<pre><code>California        NaN
Ohio          35000.0
Oregon        16000.0
Texas         71000.0
dtype: float64
</code></pre>

<p>对很多应用而言，Series最重要的一个功能是：在算数运算中会自动对齐不同索引的数据。</p>

<p>Series对象本身及其索引都有一个 <code>name</code> 属性，该属性跟Pandas其他的关键功能关系非常密切。</p>

<p>Series的索引可以通过赋值的方式就地修改。</p>

<pre><code class="language-python">sdata + obj
</code></pre>

<pre><code>California         NaN
Ohio           70000.0
Oregon         32000.0
Texas         142000.0
Utah               NaN
dtype: float64
</code></pre>

<pre><code class="language-python">obj.name = 'population'
obj.index.name = 'state'
obj
</code></pre>

<pre><code>state
California        NaN
Ohio          35000.0
Oregon        16000.0
Texas         71000.0
Name: population, dtype: float64
</code></pre>

<pre><code class="language-python">obj.index = ['Tibet', 'Beijing', 'Tianjin', 'Henan']
obj
</code></pre>

<pre><code>Tibet          NaN
Beijing    35000.0
Tianjin    16000.0
Henan      71000.0
Name: population, dtype: float64
</code></pre>

<h2 id="dataframe">DataFrame</h2>

<p>DataFrame是一个表格型的数据结构，含有一组有序的列，每列可以时不同的值类型（数值、字符串、布尔值等）。</p>

<p>DataFrame既有行索引也有列索引，它可以被看成由Series组成的字典（共用同一个索引）。</p>

<p>跟其他类似的数据结构相比（如R的 <code>data.frame</code>），DataFrame中面向行和面向列的操作基本上时平衡的。</p>

<p>构建DataFrame的办法有很多种，最常用的一种是直接传入一个由等长列表或NumPy数组组成的字典，DataFrame会自动加上索引，且所有列会被有序排列：</p>

<pre><code class="language-python">data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],
            'year': [2000, 2001, 2002, 2001, 2002],
            'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}
frame = DataFrame(data)
frame
</code></pre>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pop</th>
      <th>state</th>
      <th>year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.5</td>
      <td>Ohio</td>
      <td>2000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.7</td>
      <td>Ohio</td>
      <td>2001</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3.6</td>
      <td>Ohio</td>
      <td>2002</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2.4</td>
      <td>Nevada</td>
      <td>2001</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2.9</td>
      <td>Nevada</td>
      <td>2002</td>
    </tr>
  </tbody>
</table>
</div>

<p>如果指定了列序列，则DataFrame的列就会按照指定顺序进行排列，如果传入的列在数据中找不到，则会产生NaN值。</p>

<p>除了指定列序列，还可以指定行索引序列，而不是使用默认的数字索引。</p>

<pre><code class="language-python">frame2 = DataFrame(data, columns=['year', 'state', 'pop', 'debt'],
                   index=['one', 'two', 'three', 'four', 'five'])
frame2
</code></pre>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>year</th>
      <th>state</th>
      <th>pop</th>
      <th>debt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>one</th>
      <td>2000</td>
      <td>Ohio</td>
      <td>1.5</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>two</th>
      <td>2001</td>
      <td>Ohio</td>
      <td>1.7</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>three</th>
      <td>2002</td>
      <td>Ohio</td>
      <td>3.6</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>four</th>
      <td>2001</td>
      <td>Nevada</td>
      <td>2.4</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>five</th>
      <td>2002</td>
      <td>Nevada</td>
      <td>2.9</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>

<p>通过类似字典标记的方式或属性的方式，可以将DataFrame的列获取为一个Series。</p>

<p>返回的Series拥有原DataFrame相同的索引，而且其 <code>name</code> 属性也已经被相应的设置好狼（抽取的列名）。</p>

<pre><code class="language-python">frame2['state']  # or frame2.state
</code></pre>

<pre><code>one        Ohio
two        Ohio
three      Ohio
four     Nevada
five     Nevada
Name: state, dtype: object
</code></pre>

<p>行也可以通过位置或名称的方式进行获取，比如用索引字段 <code>ix</code> 获取：</p>

<pre><code class="language-python">frame2.ix['three']  # or frame2.ix[2]
</code></pre>

<pre><code>year     2002
state    Ohio
pop       3.6
debt      NaN
Name: three, dtype: object
</code></pre>

<p>列可以通过赋值的方式进行修改：</p>

<ul>
<li>使用标量赋值会赋值给整列；</li>
<li>使用列表或数组给某个列赋值时，其长度必须跟DataFrame的长度相匹配（如果不匹配，会抛出ValueError异常）；</li>
<li>如果赋值的是一个Series，会精确匹配DataFrame的索引，所有空位都将被填上缺失值 <code>NaN</code>；</li>
<li>为不存在的列赋值会创建出一个新列；</li>
<li>关键字 <code>del</code> 用于删除列。</li>
</ul>

<p>需要注意的是：通过索引方式返回的列只是相应数据的视图而已，并不是副本。因此，任何对返回的Series的就地修改全部会反应到源DataFrame上。通过Series的 <code>copy</code> 方法可以显式的复制列。</p>

<p>除了接受由数组组成的字典外，DataFrame还可以接受其他很多中数据输入：</p>

<ol>
<li>嵌套字典，如：{&lsquo;Nevada&rsquo;: {2001: 2.4, 2002: 2.9}, &lsquo;Ohio&rsquo;: {2000: 1.5, 2001: 1.7, 2002: 3.6}}</li>
<li>二维ndarray：数据矩阵，还可以传入行标和列标</li>
<li>由数组、列表或元组组成的字典：每个序列会变成DataFrame的一列，所有序列的长度必须相同</li>
<li>NumPy的结构化／记录数组：类似于“由数组组成的字典”</li>
<li>由Series组成的字典：每个Series会成为一列，如果没有显式指定索引，则各Series的索引会被合并成结果的行索引</li>
<li>字典或Series的列表：各项将会成为DataFrame的一行，字典键或Series索引的并集将会成为DataFrame的列标</li>
<li>由列表或元组组成的列表：类似于“二维ndarray“</li>
<li>另一个DataFrame：该DataFrame的索引将会被沿用，除非显式的指定列其他索引</li>
<li>NumPy的MaskedArray：类似于“二维ndarray”的情况，只是掩码值在结果DataFrame中会变成NA/缺失值</li>
</ol>

<h2 id="索引对象">索引对象</h2>

<p>Index对象是Pandas数据模型的重要组成部分。</p>

<p>Pandas的索引对象负责管理轴标签和其他元数据（比如轴名称等）。构建Series或DataFrame时，所用到的任何数组或其他序列的标签都会被转换成一个Index对象。</p>

<p>Index对象是不可修改的（immutable），因此用户不能对其进行修改。Index对象的不可修改性非常重要，因为这样才能使Index对象在多个数据结构之间安全共享。</p>

<p>Pandas库中内置了一些常用的Index类：</p>

<ul>
<li>Index：最泛化的Index对象，将轴标签表示为一个由Python对象组成的NumPy数组，</li>
<li>Int64Index：针对整数的特殊Index，</li>
<li>MultiIndex：“层次化”索引对象，表示单个轴上的多层索引，可以看作由元组组成的数组，</li>
<li>DatetimeIndex：存储纳秒级的时间戳（用NumPy的 <code>datetime64</code> 类型表示），</li>
<li>PeriodIndex：针对Period数据（时间间隔）的特殊Index</li>
</ul>

<p>除了这些常用的Index类型，Index甚至还可以被继承从而实现特别的轴索引功能。</p>

<p>Index对象长得很像数组，功能也很类似一个固定大小的集合。</p>

<pre><code class="language-python">frame2.index.name = 'number'
frame2
</code></pre>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>year</th>
      <th>state</th>
      <th>pop</th>
      <th>debt</th>
    </tr>
    <tr>
      <th>number</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>one</th>
      <td>2000</td>
      <td>Ohio</td>
      <td>1.5</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>two</th>
      <td>2001</td>
      <td>Ohio</td>
      <td>1.7</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>three</th>
      <td>2002</td>
      <td>Ohio</td>
      <td>3.6</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>four</th>
      <td>2001</td>
      <td>Nevada</td>
      <td>2.4</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>five</th>
      <td>2002</td>
      <td>Nevada</td>
      <td>2.9</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">'state' in frame2.columns or 'one' in frame2.index
</code></pre>

<pre><code>True
</code></pre>

<p>每个索引都有一些方法和属性，它们可用于设置逻辑并回答有关该索引所包含的数据的常见问题。下面是一些比较常用的函数：</p>

<ul>
<li>append：连接另一个Index对象，产生一个新的 Index</li>
<li>diff：计算差集，并得到一个新的 Index</li>
<li>intersection：计算交集</li>
<li>union：计算并集</li>
<li>isin：计算一个指示各值是否都包含在参数集合中的布尔型数组</li>
<li>delete：删除指定索引处的元素，并得到新的 Index</li>
<li>drop：删除传入的值，并得到新的 Index</li>
<li>insert：将元素插入到索引处，并得到新的 Index</li>
<li>is_monotonic：当各元素均大于等于前一个元素时，返回 True</li>
<li>is_unique：当Index没有重复值时，返回 True</li>
<li>unique：计算Index中唯一值的数组</li>
</ul>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://hshsh.me/post/2016-04-12-python-pandas-notes-01/">
        Pandas学习笔记（一）：CSV数据加载保存
      </a>
    </h1>

    <span class="post-date">Tue, Apr 12, 2016</span>

    

<h2 id="加载csv数据">加载CSV数据</h2>

<p>很多数据都存储在CSV文件中，Pandas 为读取提供了一个强大的 <code>read_csv</code> 函数，这个函数接受很多可选参数，通过参数控制数据加载的方式，以及一些基本的清理工作。</p>

<pre><code class="language-python">pd.read_csv(filepath_or_buffer, sep=',', delimiter=None, header='infer',
    names=None, index_col=None, usecols=None, squeeze=False, prefix=None,
    mangle_dupe_cols=True, dtype=None, engine=None, converters=None,
    true_values=None, false_values=None, skipinitialspace=False,
    skiprows=None, skipfooter=None, nrows=None, na_values=None,
    keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True,
    parse_dates=False, infer_datetime_format=False, keep_date_col=False,
    date_parser=None, dayfirst=False, iterator=False, chunksize=None,
    compression='infer', thousands=None, decimal='.', lineterminator=None,
    quotechar='&quot;', quoting=0, escapechar=None, comment=None, encoding=None,
    dialect=None, tupleize_cols=False, error_bad_lines=True,
    warn_bad_lines=True, skip_footer=0, doublequote=True,
    delim_whitespace=False, as_recarray=False, compact_ints=False,
    use_unsigned=False, low_memory=True, buffer_lines=None, memory_map=False,
    float_precision=None)

Returns
    result : DataFrame or TextParser
</code></pre>

<p>很多参数都是非常有用的，简要记录一下（详细文档请参考 <code>help(pd.read_csv)</code> 及官方文档）：</p>

<ol>
<li><code>filepath_or_buffer</code>：参数名字本身反映了功能

<ul>
<li>这里可以接受一个文件名，或者一个URL，</li>
<li>也可以接受一个打开的文件句柄，</li>
<li>或者其他任何提供了<code>read</code>方法的对象，</li>
<li>举个栗子：某个URL输出CSV，但是需要验证密码，那么就没法让 <code>read_csv</code> 直接读取URL，但是可以使用 <code>urlopen</code> 发送附带了验证信息的Request，并把返回的 Response 对象传给 <code>read_csv</code> 函数，进而通过 Response 对象的 <code>read</code> 方法加载数据；</li>
</ul></li>
<li><code>sep</code> 和 <code>delimiter</code>：这两个参数是一个意思，<code>delimiter</code>是<code>sep</code>的别名；如果指定为 <code>\t</code>（制表符）的话，就可以实现 <code>read_table</code> 的默认功能；支持使用正则表达式来匹配某些不标准的CSV文件；</li>
<li><code>header</code> 和 <code>names</code>：配合使用指定加载后的列名；</li>
<li><code>parse_dates</code>：boolean or list of ints or names or list of lists or dict, default False. 这个参数指定对CSV文件中日期序列的处理方式：

<ul>
<li>默认为False，原样加载，不解析日期时间，</li>
<li>可以为True，尝试解析日期索引，</li>
<li>可以为数字或 <code>names</code> 的列表，解析指定的列为时间序列，</li>
<li>可以为以列表为元素的列表，解析每个子列表中的字段组合为时间序列，</li>
<li>可以为值为列表的字典，解析每个列表中的字段组合为时间序列，并命名为字典中对应的键值；</li>
</ul></li>
<li><code>date_parser</code>：可以指定一个自定义函数解析日期；</li>
<li><code>keep_date_col</code>：解析出日期序列后，是否保留原来的列；</li>
<li><code>dayfirst</code>：boolean, default False, DD/MM format dates, international and European format；</li>
<li><code>iterator</code>：boolean, default False，Return TextFileReader object for iteration or getting chunks with <code>get_chunk()</code>；</li>
<li><code>encoding</code>：指定读取或写入CSV文件时使用的字符集，支持 <a href="https://docs.python.org/3/library/codecs.html#standard-encodings">codecs 包中的标准字符集</a>；</li>
<li><code>index_col</code>：数字、列名或列表，数字或列名指定某一列作为索引，列表制定某几列作为 DataFrame 的层次索引；</li>
<li>可以使用<code>skip_initialspace</code>, <code>skiprows</code>, <code>skipfooter</code>, <code>comment</code>, <code>float_precision</code>等参数做一些基本的清理动作。</li>
</ol>

<p>下面举个例子来简单演示一下 <code>parse_dates</code> 和 <code>data_parser</code> 的使用：</p>

<pre><code class="language-python">import pandas as pd
from tempfile import TemporaryFile

mycsv = [&quot;date,hour,A1,A2,A3,A4,A5,A6,date2,hour2&quot;,
    &quot;20150102,1,117,85,109,132,166,113,20160102,2&quot;,
    &quot;20150102,2,88,34,82,100,126,85,20160102,3&quot;,
    &quot;20150102,3,48,54,38,50,55,46,20160102,4&quot;,
    &quot;20150102,4,141,120,154,148,175,114,20160102,5&quot;,
    &quot;20150102,5,91,64,74,71,84,70,20160102,6&quot;,
    &quot;20150102,6,45,10,46,20,68,44,20160102,7&quot;]

tmp_csv_file = TemporaryFile()
tmp_csv_file.write('\n'.join(mycsv))
tmp_csv_file.flush()
</code></pre>

<pre><code class="language-python">tmp_csv_file.seek(0)
df = pd.read_csv(tmp_csv_file)
df
</code></pre>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>hour</th>
      <th>A1</th>
      <th>A2</th>
      <th>A3</th>
      <th>A4</th>
      <th>A5</th>
      <th>A6</th>
      <th>date2</th>
      <th>hour2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>20150102</td>
      <td>1</td>
      <td>117</td>
      <td>85</td>
      <td>109</td>
      <td>132</td>
      <td>166</td>
      <td>113</td>
      <td>20160102</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20150102</td>
      <td>2</td>
      <td>88</td>
      <td>34</td>
      <td>82</td>
      <td>100</td>
      <td>126</td>
      <td>85</td>
      <td>20160102</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>20150102</td>
      <td>3</td>
      <td>48</td>
      <td>54</td>
      <td>38</td>
      <td>50</td>
      <td>55</td>
      <td>46</td>
      <td>20160102</td>
      <td>4</td>
    </tr>
    <tr>
      <th>3</th>
      <td>20150102</td>
      <td>4</td>
      <td>141</td>
      <td>120</td>
      <td>154</td>
      <td>148</td>
      <td>175</td>
      <td>114</td>
      <td>20160102</td>
      <td>5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>20150102</td>
      <td>5</td>
      <td>91</td>
      <td>64</td>
      <td>74</td>
      <td>71</td>
      <td>84</td>
      <td>70</td>
      <td>20160102</td>
      <td>6</td>
    </tr>
    <tr>
      <th>5</th>
      <td>20150102</td>
      <td>6</td>
      <td>45</td>
      <td>10</td>
      <td>46</td>
      <td>20</td>
      <td>68</td>
      <td>44</td>
      <td>20160102</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
</div>

<p>从上面的示例中，可以看到使用默认参数时，<code>read_csv</code> 函数不会尝试解析日期，这样可以提高文件的加载速度。</p>

<p>但是第一列日期和第二列小时构成了我们需要的时间戳，加载了CSV后我们需要进行处理，那能不能在加载CSV的时候就直接解析出来呢？我们可以试一试 <code>parse_dates</code> 参数，把第一列和第二列的索引组成一个列表传给 <code>parse_dates</code> 参数：</p>

<pre><code class="language-python">tmp_csv_file.seek(0)
df = pd.read_csv(tmp_csv_file, parse_dates=[0, 1])
df
</code></pre>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>hour</th>
      <th>A1</th>
      <th>A2</th>
      <th>A3</th>
      <th>A4</th>
      <th>A5</th>
      <th>A6</th>
      <th>date2</th>
      <th>hour2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2015-01-02</td>
      <td>1</td>
      <td>117</td>
      <td>85</td>
      <td>109</td>
      <td>132</td>
      <td>166</td>
      <td>113</td>
      <td>20160102</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2015-01-02</td>
      <td>2</td>
      <td>88</td>
      <td>34</td>
      <td>82</td>
      <td>100</td>
      <td>126</td>
      <td>85</td>
      <td>20160102</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2015-01-02</td>
      <td>3</td>
      <td>48</td>
      <td>54</td>
      <td>38</td>
      <td>50</td>
      <td>55</td>
      <td>46</td>
      <td>20160102</td>
      <td>4</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2015-01-02</td>
      <td>4</td>
      <td>141</td>
      <td>120</td>
      <td>154</td>
      <td>148</td>
      <td>175</td>
      <td>114</td>
      <td>20160102</td>
      <td>5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2015-01-02</td>
      <td>5</td>
      <td>91</td>
      <td>64</td>
      <td>74</td>
      <td>71</td>
      <td>84</td>
      <td>70</td>
      <td>20160102</td>
      <td>6</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2015-01-02</td>
      <td>6</td>
      <td>45</td>
      <td>10</td>
      <td>46</td>
      <td>20</td>
      <td>68</td>
      <td>44</td>
      <td>20160102</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
</div>

<p>我们看到第一列被成功的解析成了日期数据，但是并没有按照我们想象的那样把第一列和第二列一起解析成一个日期时间对象。</p>

<p>这是因为我们<strong>传递参数的姿势不对</strong>，正确的应该是这样：<code>parse_dates=[[0, 1]]</code>，再试一下：</p>

<pre><code class="language-python">tmp_csv_file.seek(0)
df = pd.read_csv(tmp_csv_file, parse_dates=[[0, 1]])
df
</code></pre>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date_hour</th>
      <th>A1</th>
      <th>A2</th>
      <th>A3</th>
      <th>A4</th>
      <th>A5</th>
      <th>A6</th>
      <th>date2</th>
      <th>hour2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>20150102 1</td>
      <td>117</td>
      <td>85</td>
      <td>109</td>
      <td>132</td>
      <td>166</td>
      <td>113</td>
      <td>20160102</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20150102 2</td>
      <td>88</td>
      <td>34</td>
      <td>82</td>
      <td>100</td>
      <td>126</td>
      <td>85</td>
      <td>20160102</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>20150102 3</td>
      <td>48</td>
      <td>54</td>
      <td>38</td>
      <td>50</td>
      <td>55</td>
      <td>46</td>
      <td>20160102</td>
      <td>4</td>
    </tr>
    <tr>
      <th>3</th>
      <td>20150102 4</td>
      <td>141</td>
      <td>120</td>
      <td>154</td>
      <td>148</td>
      <td>175</td>
      <td>114</td>
      <td>20160102</td>
      <td>5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>20150102 5</td>
      <td>91</td>
      <td>64</td>
      <td>74</td>
      <td>71</td>
      <td>84</td>
      <td>70</td>
      <td>20160102</td>
      <td>6</td>
    </tr>
    <tr>
      <th>5</th>
      <td>20150102 6</td>
      <td>45</td>
      <td>10</td>
      <td>46</td>
      <td>20</td>
      <td>68</td>
      <td>44</td>
      <td>20160102</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
</div>

<p>Opps！虽然第一列和第二列合并到一起了，但是并没有成功的解析成日期时间对象。因为这个格式真的没人看得懂是一个日期时间对象啊！！！</p>

<p>那就没有办法在加载CSV的时候就解析时间序列的方法了吗？</p>

<p>答案是有的。<code>read_csv</code> 还有一个参数：<code>date_parser</code>，我们可以自己写一个日期时间对象解析函数。</p>

<pre><code class="language-python">from datetime import datetime

def my_date_parser(dt, hour):
    return datetime(int(dt[0:4]), int(dt[4:6]), int(dt[6:8]), int(hour))

tmp_csv_file.seek(0)
df = pd.read_csv(tmp_csv_file, date_parser=my_date_parser,
                 parse_dates={'time': [0, 1], 'time2': ['date2', 'hour2']},
                 index_col='time')
df
</code></pre>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>time2</th>
      <th>A1</th>
      <th>A2</th>
      <th>A3</th>
      <th>A4</th>
      <th>A5</th>
      <th>A6</th>
    </tr>
    <tr>
      <th>time</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2015-01-02 01:00:00</th>
      <td>2016-01-02 02:00:00</td>
      <td>117</td>
      <td>85</td>
      <td>109</td>
      <td>132</td>
      <td>166</td>
      <td>113</td>
    </tr>
    <tr>
      <th>2015-01-02 02:00:00</th>
      <td>2016-01-02 03:00:00</td>
      <td>88</td>
      <td>34</td>
      <td>82</td>
      <td>100</td>
      <td>126</td>
      <td>85</td>
    </tr>
    <tr>
      <th>2015-01-02 03:00:00</th>
      <td>2016-01-02 04:00:00</td>
      <td>48</td>
      <td>54</td>
      <td>38</td>
      <td>50</td>
      <td>55</td>
      <td>46</td>
    </tr>
    <tr>
      <th>2015-01-02 04:00:00</th>
      <td>2016-01-02 05:00:00</td>
      <td>141</td>
      <td>120</td>
      <td>154</td>
      <td>148</td>
      <td>175</td>
      <td>114</td>
    </tr>
    <tr>
      <th>2015-01-02 05:00:00</th>
      <td>2016-01-02 06:00:00</td>
      <td>91</td>
      <td>64</td>
      <td>74</td>
      <td>71</td>
      <td>84</td>
      <td>70</td>
    </tr>
    <tr>
      <th>2015-01-02 06:00:00</th>
      <td>2016-01-02 07:00:00</td>
      <td>45</td>
      <td>10</td>
      <td>46</td>
      <td>20</td>
      <td>68</td>
      <td>44</td>
    </tr>
  </tbody>
</table>
</div>

<p>Bingo！是不是搞定了。这样加载并解析时间序列的效率也比加载后使用循环或列表解析处理的效率高的多了。</p>

<p>上面这段示例代码中，还演示了解析多列时间序列，可以按照列的索引指定要解析的列，也可以按照列名来制定要解析的列，另外，还演示了使用 <code>index_col</code> 参数指定 DataFrame 索引的用法。</p>

<h2 id="保存csv数据">保存CSV数据</h2>

<p>除了加载CSV数据很方便之外，Pandas 的 DataFrame 类一个很方便的 <code>to_csv</code> 方法，可以把数据保存到CSV文件中。</p>

<pre><code class="language-python">pd.DataFrame.to_csv(self, path_or_buf=None, sep=',', na_rep='',
    float_format=None, columns=None, header=True, index=True, index_label=None,
    mode='w', encoding=None, compression=None, quoting=None, quotechar='&quot;', 
    line_terminator='\n', chunksize=None, tupleize_cols=False, date_format=None,
    doublequote=True, escapechar=None, decimal='.', **kwds)
    
unbound pandas.core.frame.DataFrame method:
    Write DataFrame to a comma-separated values (csv) file
</code></pre>

<p>嗯，默认输出就是标准的逗号分割的CSV文件，跟 <code>read_csv</code> 函数一样，这个函数同样有很多可选参数控制输出。</p>

<p>除了输出到CSV外，DataFrame 还有很多输出到其他格式的方法：</p>

<pre><code class="language-python">[meth for meth in dir(pd.DataFrame) if meth.startswith('to_')]
</code></pre>

<pre><code>['to_clipboard',
 'to_csv',
 'to_dense',
 'to_dict',
 'to_excel',
 'to_gbq',
 'to_hdf',
 'to_html',
 'to_json',
 'to_latex',
 'to_msgpack',
 'to_panel',
 'to_period',
 'to_pickle',
 'to_records',
 'to_sparse',
 'to_sql',
 'to_stata',
 'to_string',
 'to_timestamp',
 'to_wide',
 'to_xarray']
</code></pre>

<p>路漫漫其修远兮～～我将慢慢去求索～～</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://hshsh.me/post/2016-04-10-python-proxy-class-examples/">
        Python代理类两例
      </a>
    </h1>

    <span class="post-date">Sun, Apr 10, 2016</span>

    

<p>最近遇到MySQL的连接断开，MySQLdb报&rsquo;(2006, MySQL server has gone away.)&lsquo;错误的问题。</p>

<p>问题发生的环境是，客户端使用了长连接，程序启动的时候使用MySQLdb模块的connect方法建立一个数据库连接，程序运行期间一直使用这个连接。对于请求比较多的服务程序来说，这个方法不会出现问题，因为MySQL默认连接超时的时间设定是8小时，所以连接不会超时断开，也就不会报这个错误了。但是作为一个调试服务，请求频率可能低于8小时，就导致错误了。</p>

<p>由于不想修改很多具体实现的代码，所以使用<strong>代理类</strong>的方法对这个程序打了个补丁解决问题。</p>

<p>另外 SQLAlchemy 中的 <code>scoped_session</code> 也是一个代理类，实现也很有意思，这里一起分享一下这两个代理类。</p>

<h2 id="mysqldb-connection-代理类">MySQLdb Connection 代理类</h2>

<p>先看看出问题的代码：</p>

<pre><code class="language-python">import MySQLdb
conn = MySQLdb.connect(host='host', port='port',
                       user='user', passwd='passwd', db='db')

def use_mysql():
    cursor = conn.cursor()
    cursor.execute('do something with mysql database;')
    cursor.close()
</code></pre>

<p><code>MySQLdb.connect</code>函数返回的是一个<code>Connection</code>对象，当操作频率大于8个小时的时候，MySQL服务器就会关闭连接，然后下一次执行 <code>cursor = conn.cursor()</code> 的时候，就会报连接丢失的错误。为了既使用长连接，又能避免这个错误，我们可以封装一个<code>Connection</code>类的代理类，重载 <code>cursor</code> 方法，当有新的请求的时候，先检查连接是否还在，如果连接丢失的话，就重新连接数据库，然后再调用 <code>Connection</code> 类的 <code>cursor</code> 方法并返回结果：</p>

<pre><code class="language-python">import MySQLdb
from MySQLdb.connections import Connection

class ProxyConnection(Connection):
    def __init__(self, *args, **kwargs):
        # 保存数据库连接参数以备丢失时候重新连接
        self._proxy_args = args
        self._proxy_kwargs = kwargs
        super(ProxyConnection, self).__init__(*args, **kwargs)

    def cursor(self, cursorclass=None):
        try:
            self.ping()
        except MySQLdb.OperationalError:
            super(ProxyConnection, self).__init__(*self._proxy_args, **self._proxy_kwargs)
        return super(ProxyConnection, self).cursor(cursorclass)

conn = ProxyConnection(host='host', port='port',
                       user='user', passwd='passwd', db='db')

def use_mysql():
    cursor = conn.cursor()
    cursor.execute('do something with mysql database;')
    cursor.close()
</code></pre>

<p>这个代理类，只重载 <code>cursor()</code> 这一个方法，其他方法直接继承自父类 <code>Connection</code>。</p>

<p>这样就解决了MySQL数据库连接丢失的问题。但是这个方法并不是完美的，主要的缺点是每次处理新的请求时，都要 <code>ping()</code> 一下，当请求比较多的时候，会增加不必要的开支。不过如果真的请求多的话，也就不会出现这个问题了。作为低请求频次服务的解决方法，这个代理类用起来还是很方便的。</p>

<p>相对于这种粗糙的方法，SQLAlchemy中 <code>scoped_session</code> 类对 Session 类的代理实现就精巧的多了，而且它并没有从被代理的 <code>session</code> 类继承，而是一个完全独立的类（实际上<code>scoped_session</code>是一种 <a href="http://martinfowler.com/eaaCatalog/registry.html">Registry 设计模式</a>，实现了一些高级功能，不过这里暂时只看它的代理作用）。</p>

<hr />

<p>2016-05-20更新：</p>

<p>除了每次调用cursor方法获取游标时，都ping一下之外，还可以给MySQL的连接代理类增加一个超时机制，只有超时的时候才触发ping动作，这样可以兼顾道效率。</p>

<p>下面是实现代码：</p>

<pre><code class="language-python">class MySQLConnection(MySQLdb.connections.Connection):
    def __init__(self, *args, **kwargs):
        # set timeout to avoid (2006, 'MySQL server has gone away') problem
        # mysql close connection after 8 hours without activity by default
        self._recycle = kwargs.pop('recycle', None) or 7.5 * 3600
        self._last_time = time.time()
        # store connection parameters to reconnect
        self._proxy_args = args
        self._proxy_kwargs = kwargs
        super(MySQL, self).__init__(*args, **kwargs)

    def cursor(self, cursorclass=None):
        if time.time() - self._last_time &gt; self._recycle:
            try:
                self.ping()
            except MySQLdb.OperationalError:
                super(MySQLConnection, self).__init__(*self._proxy_args, **self._proxy_kwargs)
        self._last_time = time.time()
        return super(MySQL, self).cursor(cursorclass)
</code></pre>

<hr />

<h2 id="sqlalchemy-代理类-scoped-session">SQLAlchemy 代理类 <code>scoped_session</code></h2>

<p>SQLAlchemy 的 Session 类是 SQLAlchemy ORM 模型对数据库操作的主要接口，定义了<code>query</code>, <code>execute</code>, <code>flush</code>, <code>commit</code>, <code>rollback</code>, <code>refresh</code>, <code>close</code>, <code>remove</code>等方法。</p>

<pre><code class="language-pyhton"># file: sqlalchemy/orm/session.py

class Session(_SessionClassMethods):
    &quot;&quot;&quot;Manages persistence operations for ORM-mapped objects.

    The Session's usage paradigm is described at :doc:`/orm/session`.
    &quot;&quot;&quot;

    public_methods = (
        '__contains__', '__iter__', 'add', 'add_all', 'begin', 'begin_nested',
        'close', 'commit', 'connection', 'delete', 'execute', 'expire',
        'expire_all', 'expunge', 'expunge_all', 'flush', 'get_bind',
        'is_modified', 'bulk_save_objects', 'bulk_insert_mappings',
        'bulk_update_mappings',
        'merge', 'query', 'refresh', 'rollback',
        'scalar')

    # def __init__(...):
    #     ...
</code></pre>

<p>Session类的对象可以通过 <code>sessionmaker(bind=engine)()</code> 建立，由使用数据库的模块建立、使用，并自行管理。<code>scoped_session</code> 类给 Session 对象提供了一层透明代理，既可以像使用 Session 对象一样使用，又可以对 Session 对象进行统一管理。</p>

<p><code>scoped_session</code> 类的实现代码并不复杂，而且相当的精巧，下面是全部代码（为方便阅读，已删除文档字符串）：</p>

<pre><code># file: sqlalchemy/orm/scoping.py

class scoped_session(object):
    session_factory = None

    def __init__(self, session_factory, scopefunc=None):
        self.session_factory = session_factory
        if scopefunc:
            self.registry = ScopedRegistry(session_factory, scopefunc)
        else:
            self.registry = ThreadLocalRegistry(session_factory)

    def __call__(self, **kw):
        if kw:
            scope = kw.pop('scope', False)
            if scope is not None:
                if self.registry.has():
                    raise sa_exc.InvalidRequestError(
                        &quot;Scoped session is already present; &quot;
                        &quot;no new arguments may be specified.&quot;)
                else:
                    sess = self.session_factory(**kw)
                    self.registry.set(sess)
                    return sess
            else:
                return self.session_factory(**kw)
        else:
            return self.registry()

    def remove(self):
        if self.registry.has():
            self.registry().close()
        self.registry.clear()

    def configure(self, **kwargs):
        if self.registry.has():
            warn('At least one scoped session is already present. '
                 ' configure() can not affect sessions that have '
                 'already been created.')

        self.session_factory.configure(**kwargs)

    def query_property(self, query_cls=None):
        class query(object):
            def __get__(s, instance, owner):
                try:
                    mapper = class_mapper(owner)
                    if mapper:
                        if query_cls:
                            # custom query class
                            return query_cls(mapper, session=self.registry())
                        else:
                            # session's configured query class
                            return self.registry().query(mapper)
                except orm_exc.UnmappedClassError:
                    return None
        return query()

&quot;&quot;&quot;Old name for backwards compatibility.&quot;&quot;&quot;
ScopedSession = scoped_session


def instrument(name):
    def do(self, *args, **kwargs):
        return getattr(self.registry(), name)(*args, **kwargs)
    return do

for meth in Session.public_methods:
    setattr(scoped_session, meth, instrument(meth))


def makeprop(name):
    def set(self, attr):
        setattr(self.registry(), name, attr)

    def get(self):
        return getattr(self.registry(), name)

    return property(get, set)

for prop in ('bind', 'dirty', 'deleted', 'new', 'identity_map',
             'is_active', 'autoflush', 'no_autoflush', 'info'):
    setattr(scoped_session, prop, makeprop(prop))


def clslevel(name):
    def do(cls, *args, **kwargs):
        return getattr(Session, name)(*args, **kwargs)
    return classmethod(do)

for prop in ('close_all', 'object_session', 'identity_key'):
    setattr(scoped_session, prop, clslevel(prop))
</code></pre>

<p>撇开复杂的部分不说，暂时之看这个类的透明代理性质：</p>

<ol>
<li>当程序需要连接数据库的时候，可以使用 <code>scoped_session</code>，像使用<code>Session</code>类一样，<code>Session</code>支持的方法都支持；</li>
<li><strong><code>scoped_session</code> 类通过 <code>instrument</code>, <code>makeprop</code>, <code>clslevel</code> 这几个函数把 <code>Session</code> 类的方法属性“挂接”到自己身上，实现了对 <code>Session</code> 类的透明代理</strong>；</li>
<li><code>scoped_session</code> 类通过维护 Registry 来实现程序的不同部分可以共用 Session，既节省资源，又方便不同部分共享数据（不知道这样理解对不对？）；</li>
<li><code>scoped_session</code> 对象的作用域是线程局部的，类内部通过以 <code>threading.local()</code> 作为 Registry 的键值，来区分不同线程中的对象，不同的线程不共享Session对象，这样就实现了线程安全，写程序的时候只需要使用 <code>scoped_session</code> 来操作就好了，而不需要再关注线程相关的东西了。</li>
</ol>

<hr />

<p>有时候，代理类用着还是很方便的。我是一个初学者，如果本文中有谬误，请不吝指正。</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://hshsh.me/post/2016-04-08-python-import-schema/">
        [转载] Python类引入机制
      </a>
    </h1>

    <span class="post-date">Fri, Apr 8, 2016</span>

    

<p>本文转载自<a href="http://liuchang0812.com">刘畅的博客</a>，原文地址：<a href="https://github.com/Liuchang0812/slides/tree/master/pycon2015cn">https://github.com/Liuchang0812/slides/tree/master/pycon2015cn</a>。本文所涉及到的代码在<a href="https://github.com/Liuchang0812/slides/tree/master/pycon2015cn">github</a>上。</p>

<h2 id="概述">概述</h2>

<p>Python 是一门优美简单、功能强大的动态语言。在刚刚接触这门语言时，我们会被其优美的格式、简洁的语法和无穷无尽的类库所震撼。在真正的将python应用到实际的项目中，你会遇到一些无法避免的问题。最让人困惑不解的问题有二类，一个 编码问题，另一个则是引用问题。</p>

<p>本文主要讨论关于Python中import的机制与实现、以及介绍一些有意思的Python Hooks。</p>

<h2 id="python-类库引入机制">Python 类库引入机制</h2>

<p>首先，看一个简单的例子：</p>

<pre><code class="language-python">&quot;&quot;&quot;
目录结构如下：
├── __init__.py
├── main.py
└── string.py
&quot;&quot;&quot;
# main.py 内容如下
import string
print string.a
# string.py 内容如下
a = 2
</code></pre>

<p>现在，考虑一下：</p>

<ol>
<li>当我们执行main.py的时候，会发生什么事情？</li>
<li>在main.py文件执行到<code>import string</code>的时候，解释器导入的string类库是当前文件夹下的string.py还是系统标准库的string.py呢？</li>
<li>如果明确的指明自己要引用的类库？</li>
</ol>

<p>为了搞清楚上面的问题，我们需要了解关于Python类库引入的机制。</p>

<h2 id="python的两种引入机制">Python的两种引入机制</h2>

<p>Python 提供了二种引入机制：</p>

<ol>
<li>relative import</li>
<li>absolute import</li>
</ol>

<h3 id="relative-import">relative import</h3>

<p>relative import 也叫作相对引入，在Python2.5及之前是默认的引入方法。它的使用方法如下：</p>

<pre><code class="language-python">from .string import a
from ..string import a
from ...string import a
</code></pre>

<p>这种引入方式使用一个点号来标识引入类库的精确位置。与linux的相对路径表示相似，一个点表示当前目录，每多一个点号则代表向上一层目录。</p>

<pre><code class="language-python">&quot;&quot;&quot;
├── __init__.py
├── foo.py
└── main.py
&quot;&quot;&quot;
# foo.py
a = 2
# main.py
print __name__
from .foo import a
print a
</code></pre>

<p>相对引入，那么我们需要知道相对什么来引入。相对引入使用被引入文件的<code>__name__</code>属性来决定该文件在整个包结构的位置。那么如果文件的<code>__name__</code>没有包含任何包的信息，例如<code>__name__</code>被设置为了<code>__main__</code>，则认为其为‘top level script&rsquo;，而不管该文件的位置，这个时候相对引入就没有引入的参考物。如上面的程序所示，当我们执行<code>python main.py</code>时，Python解释器会抛出 ValueError: Attempted relative import in non-package 的异常。</p>

<p>为了解决这个问题，<a href="https://www.python.org/dev/peps/pep-0366/">PEP 0366 &ndash; Main module explicit relative imports</a>提出了一个解决方案。允许用户使用<code>python -m ex2.main</code>的方式,来执行该文件。在这个方案下，引入了一个新的属性<code>__package__</code>。</p>

<pre><code class="language-bash">
╭─liuchang@localhost  ~/Codes/pycon
╰─$ cat ex2/main.py
print __name__
print __package__
from .foo import a
print a
╭─liuchang@localhost  ~/Codes/pycon
╰─$ python -m ex2.main
__main__
ex2
2
</code></pre>

<h3 id="absolute-import">absolute import</h3>

<p>absolute import 也叫作完全引入，非常类似于Java的引入进制，在Python2.5被完全实现，但是是需要通过<code>from __future__ import absolute_import</code>来打开该引入进制。在Python2.6之后以及Python3，完全引用成为Python的默认的引入机制。它的使用方法如下：</p>

<pre><code>from pkg import foo
from pkg.moduleA import foo
</code></pre>

<p>要注意的是，需要从包目录最顶层目录依次写下，而不能从中间开始。</p>

<p>在使用该引入方式时，我们碰到比较多的问题就是因为位置原因，Python找不到相应的库文件，抛出ImportError的异常。让我们看一个完全引用的例子:</p>

<pre><code class="language-python">&quot;&quot;&quot;
ex3
├── __init__.py
├── foo.py
└── main.py
&quot;&quot;&quot;
# foo.py
a = 2

# main.py
print __name__
print __package__
from ex2.foo import a
print a
</code></pre>

<p>我们尝试着去运行main.py文件，Python解释器会抛出ImportError。那么我们如何解决这个问题呢？</p>

<pre><code class="language-python">╰─$ python ex3/main.py
__main__
None
Traceback (most recent call last):
  File &quot;ex3/main.py&quot;, line 3, in &lt;module&gt;
    from ex2.foo import a
ImportError: No module named ex2.foo
</code></pre>

<p>首先，我们也可以使用前文所述的module的方式去运行程序，通过-m参数来告诉解释器<code>__package__</code>属性。如下：</p>

<pre><code class="language-python">╭─liuchang@liuchangdeMacBook-Pro  ~/Codes/pycon
╰─$ python -m ex3.main
__main__
ex3
2
</code></pre>

<p>另外，我们还有一个办法可以解决该问题，在描述之前，我们介绍一个关于Python的非常有用的小知识：<strong>Python解释器会自动将当前工作目录添加到PYTHONPATH</strong>。如下所示，可以看到我们打印出的<code>sys.path</code>已经包含了当前工作目录。</p>

<pre><code class="language-python">╭─liuchang@liuchangdeMacBook-Pro  ~/Codes/pycon/ex4
╰─$ cat main.py
import sys
print sys.path
╭─liuchang@liuchangdeMacBook-Pro  ~/Codes/pycon/ex4
╰─$ python main.py
['/Users/liuchang/Codes/pycon/ex4',
'/Library/Python/2.7/site-packages/pip-7.1.0-py2.7.egg',
'/Library/Python/2.7/site-packages/mesos-_PACKAGE_VERSION_-py2.7.egg',
'/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python27.zip',
'/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7',
'/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-darwin',
'/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac',
'/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac/lib-scriptpackages',
'/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python',
'/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk',
'/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-old',
'/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload',
'/Users/liuchang/Library/Python/2.7/lib/python/site-packages',
'/usr/local/lib/python2.7/site-packages',
'/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/PyObjC',
'/Library/Python/2.7/site-packages']
</code></pre>

<p>了解了Python解释器的这个特性后，我们就可以解决完全引用的找不到类库的问题：执行的时候，让解释器自动的将类库的目录添加到PYTHONPATH中。</p>

<p>我们可以在顶层目录中添加一个run_ex3.py的文件，文件内容和运行结果如下，可以看到Python解释器正确的执行了ex3.main文件。</p>

<pre><code class="language-python">╭─liuchang@liuchangdeMacBook-Pro  ~/Codes/pycon
╰─$ cat run_ex3.py
from ex3 import main
╭─liuchang@liuchangdeMacBook-Pro  ~/Codes/pycon
╰─$ python run_ex3.py
ex3.main
None
2
</code></pre>

<h2 id="一些实践经验">一些实践经验</h2>

<h3 id="相对引用还是绝对引用">相对引用还是绝对引用？</h3>

<p>上面介绍了Python的两种引用方式，都可以解决引入歧义的问题。那我们应该使用哪一种呢？</p>

<p>先说明一下Python的默认引用方式，在Python2.4及之前，Python只有相对引用这一种方式，在Python2.5中实现了绝对引用，但默认没有打开，需要用户自己指定使用该引用方式。在之后的版本和Python3版本，绝对引用已经成为默认的引用方式。</p>

<p>其次，二种引用方式各有利弊。绝对引用代码更加清晰明了，可以清楚的看到引入的包名和层次，但是，当包名修改的时候，我们需要手动修改所有的引用代码。相对引用则比较精简，不会被包名修改所影响，但是可读性较差，不如完全引用清晰。</p>

<p>最后，对于两种引用的方式选择，还是有争论的。在PEP8中，Python官方推荐的是绝对引用,详细理由可以参考<a href="https://www.python.org/dev/peps/pep-0008/#imports">这儿</a>。</p>

<blockquote>
<p>Absolute imports are recommended, as they are usually more readable and tend to be better behaved (or at least give better error messages) if the import system is incorrectly configured (such as when a directory inside a package ends up on sys.path ):</p>
</blockquote>

<pre><code class="language-python">import mypkg.sibling
from mypkg import sibling
from mypkg.sibling import example
</code></pre>

<blockquote>
<p>However, explicit relative imports are an acceptable alternative to absolute imports, especially when dealing with complex package layouts where using absolute imports would be unnecessarily verbose:</p>
</blockquote>

<pre><code class="language-python">from . import sibling
from .sibling import example
</code></pre>

<blockquote>
<p>Standard library code should avoid complex package layouts and always use absolute imports.
Implicit relative imports should never be used and have been removed in Python 3.</p>
</blockquote>

<h3 id="规范打包发布">规范打包发布</h3>

<p>为了别人使用自己代码的方便，应该尽量使用规范的包分发机制。为自己的Python包编写正确的setup.py文件，添加相应的README.md文件。对于提供一些可执行命令的包，则可以使用 console_entrypoint 的机制来提供。因为打包和分发不是本文重点，不再详细叙述，大家可以查看官方文档。</p>

<h3 id="使用virtualenv管理包依赖">使用virtualenv管理包依赖</h3>

<p>在使用Python的时候，尽量使用virtualenv来管理项目，所有的项目从编写到运行都在特定的virtualenv中。并且为自己的项目生成正确的依赖描述文件。</p>

<pre><code class="language-bash">pip freeze &gt; requirements.txt
</code></pre>

<p>关于virtualenv的用法，可以参考我之前的一篇文章<a href="http://lcblog-wordpress.stor.sinaapp.com/uploads/2015/10/virtualenv教程.pdf">virtualenv教程</a>。</p>

<h2 id="python-import实现">Python import实现</h2>

<p>Python 提供了 import 语句来实现类库的引用，下面我们详细介绍当执行了 import 语句的时候，内部究竟做了些什么事情。</p>

<p>当我们执行一行 <code>from package import module as mymodule</code>命令时，Python解释器会查找package这个包的module模块，并将该模块作为mymodule引入到当前的工作空间。所以import语句主要是做了二件事：</p>

<ol>
<li>查找相应的module</li>
<li>加载module到local namespace</li>
</ol>

<p>下面我们详细了解python是如何查找模块的。</p>

<h3 id="查找module的过程">查找module的过程</h3>

<p>在import的第一个阶段，主要是完成了查找要引入模块的功能，这个查找的过程如下：</p>

<ol>
<li>检查 sys.modules (保存了之前import的类库的缓存），如果module被找到，则⾛到第二步。</li>
<li>检查 sys.meta_path。meta_path 是一个 list，⾥面保存着一些 finder 对象，如果找到该module的话，就会返回一个finder对象。</li>
<li>检查⼀些隐式的finder对象，不同的python实现有不同的隐式finder，但是都会有 sys.path_hooks, sys.path_importer_cache 以及sys.path。</li>
<li>抛出 ImportError。</li>
</ol>

<h3 id="sys-modules">sys.modules</h3>

<p>对于第一步中sys.modules，我们可以打开Python来实际的查看一下其内容：</p>

<pre><code class="language-python">Python 2.7.10 (default, Aug 22 2015, 20:33:39)
[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.1)] on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import sys
&gt;&gt;&gt; sys.modules
{'copy_reg': &lt;module 'copy_reg' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/copy_reg.pyc'&gt;,
'sre_compile': &lt;module 'sre_compile' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/sre_compile.pyc'&gt;,
'_sre': &lt;module '_sre' (built-in)&gt;,
'encodings': &lt;module 'encodings' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/encodings/__init__.pyc'&gt;,
'site': &lt;module 'site' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site.pyc'&gt;,
'__builtin__': &lt;module '__builtin__' (built-in)&gt;,
'sysconfig': &lt;module 'sysconfig' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/sysconfig.pyc'&gt;,
'encodings.encodings': None,
'__main__': &lt;module '__main__' (built-in)&gt;,
'supervisor': &lt;module 'supervisor' (built-in)&gt;,
'abc': &lt;module 'abc' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/abc.pyc'&gt;,
'posixpath': &lt;module 'posixpath' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/posixpath.pyc'&gt;,
'_weakrefset': &lt;module '_weakrefset' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/_weakrefset.pyc'&gt;,
'errno': &lt;module 'errno' (built-in)&gt;,
'encodings.codecs': None,
'sre_constants': &lt;module 'sre_constants' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/sre_constants.pyc'&gt;,
're': &lt;module 're' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/re.pyc'&gt;,
'_abcoll': &lt;module '_abcoll' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/_abcoll.pyc'&gt;,
'types': &lt;module 'types' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/types.pyc'&gt;,
'_codecs': &lt;module '_codecs' (built-in)&gt;,
'encodings.__builtin__': None,
'_warnings': &lt;module '_warnings' (built-in)&gt;,
'genericpath': &lt;module 'genericpath' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/genericpath.pyc'&gt;,
'stat': &lt;module 'stat' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/stat.pyc'&gt;,
'zipimport': &lt;module 'zipimport' (built-in)&gt;,
'_sysconfigdata': &lt;module '_sysconfigdata' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/_sysconfigdata.pyc'&gt;,
'mpl_toolkits': &lt;module 'mpl_toolkits' (built-in)&gt;,
'warnings': &lt;module 'warnings' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/warnings.pyc'&gt;,
'UserDict': &lt;module 'UserDict' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/UserDict.pyc'&gt;,
'encodings.utf_8': &lt;module 'encodings.utf_8' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/encodings/utf_8.pyc'&gt;,
'sys': &lt;module 'sys' (built-in)&gt;,
'_osx_support': &lt;module '_osx_support' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/_osx_support.pyc'&gt;,
'codecs': &lt;module 'codecs' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.pyc'&gt;,
'readline': &lt;module 'readline' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/readline.so'&gt;,
'os.path': &lt;module 'posixpath' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/posixpath.pyc'&gt;,
'_locale': &lt;module '_locale' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_locale.so'&gt;,
'signal': &lt;module 'signal' (built-in)&gt;,
'traceback': &lt;module 'traceback' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/traceback.pyc'&gt;,
'linecache': &lt;module 'linecache' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/linecache.pyc'&gt;,
'posix': &lt;module 'posix' (built-in)&gt;,
'encodings.aliases': &lt;module 'encodings.aliases' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/encodings/aliases.pyc'&gt;,
'exceptions': &lt;module 'exceptions' (built-in)&gt;,
'sre_parse': &lt;module 'sre_parse' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/sre_parse.pyc'&gt;,
'os': &lt;module 'os' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.pyc'&gt;,
'_weakref': &lt;module '_weakref' (built-in)&gt;}
&gt;&gt;&gt; sys.modules['zlib'].__file__
'/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/zlib.so'
</code></pre>

<p>可以看到sys.modules已经保存了一些包的信息，由这些信息，我们就可以直接知道要查找的包的位置等信息。</p>

<h3 id="finder-loader和importer">finder、loader和importer</h3>

<p>在上文中，我们提到了sys.meta_path中保证了一些finder对象。在python中，不仅定义了finder的概念，还定义了loader和importor的概念。</p>

<ul>
<li>finder的任务是决定自己是否根据名字找到相应的模块，在py2中，finder对象必须实现find_module()方法，在py3中必须要实现find_module()或者find_loader（)方法。如果finder可以查找到模块，则会返回一个loader对象(在py3.4中，修改为返回一个module specs)。</li>
<li>loader则是负责加载模块，它必须实现一个load_module()的方法。</li>
<li>importer 则指一个对象，实现了finder和loader的方法。因为Python是duck type，只要实现了方法，就可以认为是该类。</li>
</ul>

<h3 id="sys-meta-path">sys.meta_path</h3>

<p>在Python查找的时候，如果在sys.modules没有查找到，就会依次调用sys.meta_path中的finder对象。默认的情况下，sys.meta_path是一个空列表，并没有任何finder对象。</p>

<pre><code class="language-bash">In [6]: sys.meta_path
Out[6]: []
</code></pre>

<p>我们可以向sys.meta_path中添加一些定义的finder，来实现对Python加载模块的修改。比如下例，我们实现了一个会将每次加载包的信息打印出来的finder。</p>

<pre><code class="language-python">from __future__ import print_function
import sys

class Watcher(object):
    @classmethod
    def find_module(cls, name, path, target=None):
        print(&quot;Importing&quot;, name, path, target)
        return None

sys.meta_path.insert(0, Watcher)

import socket
</code></pre>

<p>当我们执行的时候，就可以看到系统加载socket包时所发生的事情。</p>

<pre><code class="language-python"> ╭─liuchang@localhost  ~/Codes/pycon/ex5_meta_path
 ╰─$ python finder1.py
 Importing socket None None
 Importing _socket None None
 Importing functools None None
 Importing _functools None None
 Importing _ssl None None
 Importing cStringIO None None
</code></pre>

<h3 id="sys-path-hook">sys.path hook</h3>

<p>Python import的hook分为二类，一类是上一章节已经描述的meta hook，另一类是 path hook。</p>

<p>当处理sys.path（或者package.<strong>path</strong>)时，就会调用对应的一部分的 Pack hook。Path Hook是通过向sys.path_hooks 中添加一个importer生成器来注册的。</p>

<p>sys.path_hooks 是由可被调用的对象组成，它会顺序的检查以决定他们是否可以处理给定的sys.path的一项。每个对象会使用sys.path项的路径来作为参数被调用。如果它不能处理该路径，就必须抛出ImportError，如果可以，则会返回一个importer对象。之后，不会再尝试其它的sys.path_hooks对象，即使前一个importer出错了。</p>

<p>详细可以参考<a href="https://www.python.org/dev/peps/pep-0302/#specification-part-2-registering-hooks">registering-hooks</a>。</p>

<h2 id="python-import-hooks">python import hooks</h2>

<p>在介绍完Python的引用机制与一些实现方法后，接下来我们介绍一些关于如何根据自己的需求来扩展Python的引用机制。</p>

<p>在开始详细介绍前，给大家展示一个实用性不高，但是很有意思的例子：<strong>让Python在执行代码的时候自动安装缺失的类库</strong>。我们会实现一个autoinstall的模块，只要import了该模块，就可以打开该功能。如下所示，我们尝试引入tornado库的时候，iPython会提示我们没有安装。然后，我们引入了autoinstall，再尝试引入tornado，iPython就会自动的安装tornado库。</p>

<pre><code class="language-bash">In [1]: import tornado
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-1-3eac10687b7e&gt; in &lt;module&gt;()
----&gt; 1 import tornado

ImportError: No module named tornado

In [2]: import autoinstall

In [3]: import tornado
Installing tornado

Collecting tornado
  Downloading tornado-4.2.1.tar.gz (434kB)
Collecting backports.ssl-match-hostname (from tornado)
  Downloading http://182.92.2.186:7002/packages/backports.ssl_match_hostname-3.4.0.2-py2-none-any.whl
Collecting certifi (from tornado)
  Downloading certifi-2015.9.6.2-py2.py3-none-any.whl (371kB)
Installing collected packages: backports.ssl-match-hostname, certifi, tornado
  Running setup.py install for tornado
Successfully installed backports.ssl-match-hostname-3.4.0.2 certifi-2015.9.6.2 tornado-4.2.1
</code></pre>

<p>这个功能的实现其实很简单，利用了sys.meta_path。autoinstall的全部代码如下：</p>

<pre><code class="language-python">from __future__ import print_function
import sys
import subprocess


class AutoInstall(object):
    _loaded = set()

    @classmethod
    def find_module(cls, name, path, target=None):
        if path is None and name not in cls._loaded:
            cls._loaded.add(name)
            print(&quot;Installing&quot;, name)
            try:
                out = subprocess.check_output(
                    ['sudo', sys.executable, '-m', 'pip', 'install', name])
                print(out)
            except Exception as e:
                print(&quot;Failed&quot; + e.message)
        return None

sys.meta_path.append(AutoInstall)
</code></pre>

<h3 id="import-hook的重要性">import hook的重要性</h3>

<p>我们为什么需要Python import的hook呢？使用import的hook可以让我们做到很多事情，比如说当我们的Python包存储在一个非标准的文件中，或者Python程序存储在网络数据库中，或者像py2exe一样将Python程序打包成了一个文件，我们需要一种方法来正确的解析它们。</p>

<p>其次，我们希望在Python加载类库的时候，可以额外的做一些事情，比如上传审计信息，比如延迟加载，比如自动解决上例的依赖未安装的问题。</p>

<p>所以，import系统的Hook技术是值的花时间学习的。</p>

<h3 id="如何实现import-hooks">如何实现import hooks</h3>

<p>Python提供了一些方法，让我们可以在代码中动态的调用import。主要有如下几种：</p>

<ol>
<li>__import__ : Python的内置函数</li>
<li>imputil        : Python的import工具库，在py2.6被声明废弃，py3中彻底移除。</li>
<li>imp            : Python2 的一个import库，py3中移除</li>
<li>importlib      : Python3 中最新添加，backport到py2.7，但只有很小的子集（只有一个函数）。</li>
</ol>

<p>Python2 所有关于import的库的列表参见<a href="https://docs.python.org/2/library/modules.html">Importing Modules</a>。Python3 的可以参考<a href="https://docs.python.org/3/library/modules.html">Importing Modules</a>。</p>

<p><a href="https://www.python.org/dev/peps/pep-0302">PEP 0302 &ndash; New Import Hooks</a> 提案详细的描述了importlib的目的、用法。</p>

<h3 id="一些hook示例">一些Hook示例</h3>

<h3 id="lazy化库引入">Lazy化库引入</h3>

<p>使用Import Hook，我们可以达到Lazy Import的效果，当我们执行import的时候，实际上并没引入该库，只有真正的使用这个库的时候，才会将其引入到当前工作空间。
具体的代码可以参考<a href="https://github.com/noahmorrison/limp">github</a>。
实现的效果如下：</p>

<pre><code class="language-python">#!/usr/bin/python

import limp  # Lazy imports begin now

import json
import sys

print('json' in sys.modules)  # False
print(', '.join(json.loads('[&quot;Hello&quot;, &quot;World!&quot;]')))
print('json' in sys.modules)  # True
</code></pre>

<p>它的实现也很简单：</p>

<pre><code class="language-python">import sys
import imp

_lazy_modules = {}

class LazyModule():
    def __init__(self, name):
        self.name = name

    def __getattr__(self, attr):
        path = _lazy_modules[self.name]
        f, pathname, desc = imp.find_module(self.name, path)

        lf = sys.meta_path.pop()
        imp.load_module(self.name, f, pathname, desc)
        sys.meta_path.append(lf)


        self.__dict__ = sys.modules[self.name].__dict__
        return self.__dict__[attr]

class LazyFinder(object):

    def find_module(self, name, path):
        _lazy_modules[name] = path
        return self

    def load_module(self, name):
        return LazyModule(name)

sys.meta_path.append(LazyFinder())
</code></pre>

<h3 id="flask-插件库统一入口">Flask 插件库统一入口</h3>

<p>使用过Flask的同学都知道，Flask的对于插件提供了统一的入口。比如说我们安装了Flask_API这个库，然后我们可以直接<code>import flask_api</code>来使用这个库，同时Flask还允许我们采用<code>import flask.ext.api</code>的方式来引用该库。</p>

<p>这里Flask就是使用了import 的hook，当引入flask.ext的包时，就自动的引用相应的库。Flask实现了一个叫ExtensionImporter的类，这个类实现了find_module和load_module代码实现如下<a href="https://github.com/mitsuhiko/flask/blob/master/flask/exthook.py#L27">github</a>：</p>

<pre><code class="language-python">class ExtensionImporter(object):
    &quot;&quot;&quot;This importer redirects imports from this submodule to other locations.
    This makes it possible to transition from the old flaskext.name to the
    newer flask_name without people having a hard time.
    &quot;&quot;&quot;

    def __init__(self, module_choices, wrapper_module):
        self.module_choices = module_choices
        self.wrapper_module = wrapper_module
        self.prefix = wrapper_module + '.'
        self.prefix_cutoff = wrapper_module.count('.') + 1

    def __eq__(self, other):
        return self.__class__.__module__ == other.__class__.__module__ and \
               self.__class__.__name__ == other.__class__.__name__ and \
               self.wrapper_module == other.wrapper_module and \
               self.module_choices == other.module_choices

    def __ne__(self, other):
        return not self.__eq__(other)

    def install(self):
        sys.meta_path[:] = [x for x in sys.meta_path if self != x] + [self]

    def find_module(self, fullname, path=None):
        if fullname.startswith(self.prefix):
            return self

    def load_module(self, fullname):
        if fullname in sys.modules:
            return sys.modules[fullname]
        modname = fullname.split('.', self.prefix_cutoff)[self.prefix_cutoff]
        for path in self.module_choices:
            realname = path % modname
            try:
                __import__(realname)
            except ImportError:
                exc_type, exc_value, tb = sys.exc_info()
                # since we only establish the entry in sys.modules at the
                # very this seems to be redundant, but if recursive imports
                # happen we will call into the move import a second time.
                # On the second invocation we still don't have an entry for
                # fullname in sys.modules, but we will end up with the same
                # fake module name and that import will succeed since this
                # one already has a temporary entry in the modules dict.
                # Since this one &quot;succeeded&quot; temporarily that second
                # invocation now will have created a fullname entry in
                # sys.modules which we have to kill.
                sys.modules.pop(fullname, None)

                # If it's an important traceback we reraise it, otherwise
                # we swallow it and try the next choice.  The skipped frame
                # is the one from __import__ above which we don't care about
                if self.is_important_traceback(realname, tb):
                    reraise(exc_type, exc_value, tb.tb_next)
                continue
            module = sys.modules[fullname] = sys.modules[realname]
            if '.' not in modname:
                setattr(sys.modules[self.wrapper_module], modname, module)
            return module
        raise ImportError('No module named %s' % fullname)
</code></pre>

<p>然后在Flask的ext目录下的__init__.py文件中，初始化了该Importer。</p>

<pre><code class="language-python">
def setup():
    from ..exthook import ExtensionImporter
    importer = ExtensionImporter(['flask_%s', 'flaskext.%s'], __name__)
    importer.install()
    
</code></pre>

<h2 id="总结">总结</h2>

<p>希望坚持阅读到本处的你，能明白Python import的用法、实现和改造方法。准备仓促，难免会有错误，欢迎大家指正和PR。</p>

<p>本文使用CC-BY-SA协议。</p>

<h2 id="附录">附录</h2>

<ol>
<li><a href="https://www.python.org/dev/peps/pep-0302/">https://www.python.org/dev/peps/pep-0302/</a></li>
<li><a href="https://www.python.org/dev/peps/pep-0338/">https://www.python.org/dev/peps/pep-0338/</a></li>
<li><a href="https://www.python.org/dev/peps/pep-0328/">https://www.python.org/dev/peps/pep-0328/</a></li>
<li><a href="https://www.python.org/dev/peps/pep-0366/">https://www.python.org/dev/peps/pep-0366/</a></li>
<li><a href="https://github.com/noahmorrison/limp">https://github.com/noahmorrison/limp</a></li>
<li><a href="https://github.com/mitsuhiko/flask">https://github.com/mitsuhiko/flask</a></li>
</ol>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://hshsh.me/post/2016-04-05-mysql-configuration-notes/">
        MySQL配置文件参考
      </a>
    </h1>

    <span class="post-date">Tue, Apr 5, 2016</span>

    

<p>最近公式生产环境中使用MySQL做数据存储，把数据库跑起来不复杂，但是各种参数的设置调优可真是技术活。</p>

<p>这是配置MySQL的学习笔记，大部分内容出自：<a href="http://wsgzao.github.io/post/ltmp/">http://wsgzao.github.io/post/ltmp/</a></p>

<h2 id="mysql数据库配置">MySQL数据库配置</h2>

<h3 id="mysql客户端配置">MySQL客户端配置：</h3>

<pre><code class="language-ini">[client]
# 客户端连接默认字符集
default-character-set = utf8
port = 3306
socket = /tmp/mysql.sock

[mysql]
#prompt=&quot;(\u:HOSTNAME:)[\d]&gt; &quot;
#mysql提示符中显示当前用户、数据库、时间等信息
prompt=&quot;\u@\h \R:\m:\s [\d]&gt; &quot;
#no-auto-rehash  # 自动补全功能，取消自动补全可以提高启动速度
</code></pre>

<h3 id="mysql服务端配置">MySQL服务端配置</h3>

<pre><code class="language-ini">[mysqld]
# 唯一的服务标识号，主从同步会涉及
server-id = 1
port = 3306
user = mysql
basedir = /app/local/mysql
datadir = /app/data/mysql/data
socket = /tmp/mysql.sock
log-error = /app/data/mysql/mysql_error.log
pid-file = /app/data/mysql/mysql.pid
sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES

# 默认存储引擎
default-storage-engine = InnoDB
# 设置最大并发连接数，如果前端程序是PHP，可适当加大，但不可过大。
# 如果前端程序采用连接池，可适当调小，避免连接数过大
max_connections = 512
# 最大连接错误次数，可适当加大，防止频繁连接错误后，前端host被mysql拒绝掉
max_connect_errors = 100000
# 所有线程所打开表的数量
table_open_cache = 512
# 不允许外部文件级别的锁. 打开文件锁会对性能造成负面影响
external-locking = FALSE
# 服务所能处理的请求包的最大大小以及服务所能处理的最大的请求大小
max_allowed_packet = 32M
# 启用慢查询日志
slow_query_log = 1
slow_query_log_file = /app/data/mysql/slow.log
# MySQL打开的文件描述符限制
open_files_limit = 10240
# 操作系统在监听队列中所能保持的连接数
back_log = 600
# 每个连接都会分配的一些排序、连接等缓冲
sort_buffer_size = 16M
join_buffer_size = 16M
read_buffer_size = 16M
read_rnd_buffer_size = 16M
# 在cache中保留多少线程用于重用
thread_cache_size = 300
# 查询缓冲     
query_cache_size = 128M
# 只有小于此设定值的结果才会被缓冲     
query_cache_limit = 4M
# 设置查询缓存分配内存的最小单位
query_cache_min_res_unit = 2k
# 线程使用的堆大小
thread_stack = 512K
# 设置事务隔离级别为 READ-COMMITED，提高事务效率，通常都满足事务一致性要求
transaction_isolation = READ-COMMITTED
# 临时表的最大大小
tmp_table_size = 256M
# 独立的内存表所允许的最大容量
max_heap_table_size = 256M
# 设置慢查询阀值
long_query_time = 3
# 表示slave将复制事件写进自己的二进制日志
log-slave-updates
# 打开二进制日志功能
log-bin = /app/data/mysql/binlog/binlog
sync_binlog = 1
# 在一个事务中binlog为了记录SQL状态所持有的cache大小  
binlog_cache_size = 4M
# 设置混合模式
binlog_format = MIXED
# 表示的是binlog能够使用的最大cache 内存大小
max_binlog_cache_size = 8M
# binlog最大值
max_binlog_size = 1G
# 启用中继日志
relay-log-index = /app/data/mysql/relaylog/relaylog
relay-log-info-file = /app/data/mysql/relaylog/relaylog
relay-log = /app/data/mysql/relaylog/relaylog
# 设置了只保留7天binlog
expire_logs_days = 7
</code></pre>

<h3 id="myisam-相关选项">MyISAM 相关选项</h3>

<pre><code class="language-ini">#关键词缓冲的大小, 一般用来缓冲MyISAM表的索引块
key_buffer_size = 128M
#排序缓存
read_rnd_buffer_size = 64M
#限制每个进程中缓冲树的字节数
bulk_insert_buffer_size = 256M
#MyISAM表发生变化时重新排序所需的缓冲
myisam_sort_buffer_size = 256M
#MySQL重建索引时所允许的最大临时文件的大小
myisam_max_sort_file_size = 10G
#如果一个表拥有超过一个索引, MyISAM 可以通过并行排序使用超过一个线程去修复他们
myisam_repair_threads = 1
#自动检查和修复没有适当关闭的 MyISAM 表
myisam_recover
</code></pre>

<h3 id="innodb-相关选项">InnoDB 相关选项</h3>

<pre><code class="language-ini"># InnoDB存储数据字典、内部数据结构的缓冲池，16MB 已经足够大了     
innodb_additional_mem_pool_size = 16M
# InnoDB用于缓存数据、索引、锁、插入缓冲、数据字典等
# 如果是专用的DB服务器，且以InnoDB引擎为主的场景，通常可设置物理内存的50%
# 如果是非专用DB服务器，可以先尝试设置成内存的1/4，如果有问题再调整
innodb_buffer_pool_size = 4G
# InnoDB共享表空间初始化大小，默认是 10MB，也非常坑X，改成 1GB，并且自动扩展
innodb_data_file_path = ibdata1:1G:autoextend
# 如果将此参数设置为1，将在每次提交事务后将日志写入磁盘，能较好保护数据可靠性。
# 为提供性能可以设置为0或2，但要承担在发生故障时丢失数据的风险
innodb_flush_log_at_trx_commit = 1
# InnoDB的log buffer，通常设置为 64MB 就足够了
innodb_log_buffer_size = 64M
# InnoDB redo log大小，通常设置256MB 就足够了
innodb_log_file_size = 256M
# InnoDB redo log文件组，通常设置为 2 就足够了
innodb_log_files_in_group = 2
# Buffer_Pool中Dirty_Page所占的数量，直接影响InnoDB的关闭时间
innodb_max_dirty_pages_pct = 90
# 启用InnoDB的独立表空间模式，便于管理
innodb_file_per_table = 1
# 控制innodb是否对gap加锁
innodb_locks_unsafe_for_binlog = 0
# 设置连接超时阀值，如果前端程序采用短连接，建议缩短这2个值
# 如果前端程序采用长连接，可直接注释掉这两个选项，是用默认配置(8小时)
interactive_timeout = 120
wait_timeout = 120
# 不再进行反解析(ip不反解成域名)，这样可以加快数据库的反应时间
skip-name-resolve
# 主从复制跳过错误
slave-skip-errors = 1032,1062,126,1114,1146,1048,1396
</code></pre>

<h3 id="其他配置选项">其他配置选项</h3>

<pre><code class="language-ini">[mysqldump]
# 不要在将内存中的整个结果写入磁盘之前缓存. 在导出非常巨大的表时需要此项
quick
max_allowed_packet = 32M
</code></pre>

<h2 id="参考资料">参考资料</h2>

<ol>
<li><a href="http://wsgzao.github.io/post/ltmp/">LTMP手动编译安装以及全自动化部署实践</a></li>
<li><a href="http://dev.mysql.com/doc/refman/5.7/en/charset-configuration.html">MySQL 5.7 Reference Manual: Character Set Configuration</a></li>
</ol>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://hshsh.me/post/2016-04-04-sqlalchemy-study-notes-01/">
        SQLAlchemy学习笔记（一）
      </a>
    </h1>

    <span class="post-date">Mon, Apr 4, 2016</span>

    

<p>这篇笔记部分内容来自网络<sup class="footnote-ref" id="fnref:fn-copyright"><a rel="footnote" href="#fn:fn-copyright">1</a></sup>，部分来自对《Essential SQLAlchemy》的学习和使用经验。</p>

<hr />

<p><a href="http://www.sqlalchemy.org/">SQLAlchemy</a>是 Python 编程语言下的一款开源软件。提供了SQL工具包及对象关系映射（ORM）工具，使用MIT许可证发行。</p>

<p>SQLAlchemy“采用简单的Python语言，为高效和高性能的数据库访问设计，实现了完整的企业级持久模型”。</p>

<p>SQLAlchemy的理念是，SQL数据库的量级和性能重要于对象集合；而对象集合的抽象又重要于表和行。因此，SQLAlchmey 采用了类似于Java里 Hibernate 的数据映射模型，而不是其他ORM框架采用的 Active Record 模型。不过，Elixir 和 declarative 等可选插件可以让用户使用声明语法。</p>

<p>SQLAlchemy首次发行于2006年2月，并迅速地在Python社区中最广泛使用的ORM工具之一，不亚于Django的ORM框架。</p>

<p>以上摘自<a href="https://zh.wikipedia.org/wiki/SQLAlchemy">维基百科</a>。</p>

<hr />

<p>使用<code>SQLAlchemy</code>有三种方式：</p>

<ul>
<li>使用 Raw SQL</li>
<li>使用 SQL Expression</li>
<li>使用 ORM</li>
</ul>

<p>前两种方式可以统称为 core 方式。</p>

<p>对于绝大多数应用，推荐使用 <code>SQLAlchemy</code>，即使是使用 Raw SQL，也可以带来如下好处。</p>

<ol>
<li>内建数据库连接池。<strong>注意：</strong>如果是 SQLAlchemy + cx_oracle 的话，需要禁用 Connection Pool，否则会有异常。方法是设置<code>sqlalchemy.poolclass</code>为<code>sqlalchemy.pool.NullPool</code></li>
<li>强大的日志功能（log）</li>
<li>数据库无关的写法，包括：SQL参数写法、LIMIT语法等</li>
<li>特别提一下，WHERE 条件的 <code>== value</code> 写法，如果<code>value</code>等于<code>None</code>，真正的SQL会转为 <code>IS NULL</code></li>
</ol>

<p>SQLAlchemy 的 Raw SQL 和 SQL Expression 比较：</p>

<ol>
<li>SQL Expression 的写法是纯 Python 代码，阅读性更好，尤其是在使用 <code>insert()</code> 方法时，字段名和取值成对出现。</li>
<li>Raw SQL 比 SQL Expression 更灵活，如果 SQL/DDL 很复杂，Raw SQL 就更有优势了。</li>
</ol>

<h2 id="常用数据库连接字符串">常用数据库连接字符串</h2>

<pre><code class="language-python">import sqlalchemy
from sqlalchemy import create_engine

# file database
# sqlite = create_engine('sqlite:////absolute/path/to/database.db')
# in-memory database, two ways
# sqlite = create_engine('sqlite://')
sqlite = create_engine('sqlite:///:memory:')
# postgresql
pgsql = create_engine('postgres://user:passwd@host:port/database')
# mysql
mysql = create_engine('mysql://user:passwd@host:port/database')
# oracle
oracle = create_engine('oracle://user:passwd@host:port/sidname')
# oracle via TNS name
oracle_tns = create_engine('oracle://user:passwd@tnsname')
# ms sql server using ODBC datasource names.
PyODBC is the default driver
# mssql = create_engine('mssql://mydsn')
mssql = create_engine('mssql://user:passwd@mydsn')
# firebird
firebird = create_engine('firebird://user:passwd@host/some.gdm')
</code></pre>

<h2 id="connection-less-执行和-connection-执行">Connection less 执行和 Connection 执行</h2>

<p>直接使用 <code>engine</code> 执行SQL的方式，叫做 connection less 执行。</p>

<p>先使用 <code>engine.connect()</code> 获取连接对象 <code>conn</code>，然后通过 <code>conn</code> 执行SQL的方式，叫做 connection 执行。</p>

<p>如果要在 transaction 模式下执行，推荐使用 connection 方式；如果不涉及 transaction，两种方法效果是一样的。</p>

<h2 id="使用-text-函数封装sql字符串">使用<code>text()</code>函数封装SQL字符串</h2>

<p>使用 <code>text()</code> 函数有很多好处：</p>

<p>1). 不同数据库，可以使用统一的SQL参数传递写法，参数需以“冒号”引出，在调用 <code>execute()</code> 的时候，使用 dict 结构将实参传进去。</p>

<pre><code class="language-python">from sqlalchemy import text

result = db.execute(
   text('select * from table where id &lt; :id and typeName = :type'),
   {'id': 2, 'type': 'USER_TABLE'})
</code></pre>

<p>2). 如果不指定参数的类型，默认为字符串类型；如果要传递日期参数，需要使用 <code>text()</code> 的 <code>bindparams</code> 参数来声明。</p>

<pre><code class="language-python">from datetime import datetime, timedelta
from sqlalchemy import DateTime, bindparam

# ten days ago
date_param = datetime.today() + timedelta(days=-1*10)
sql = 'delete from caw_job_alarm_log where alarm_time &lt; :alarm_time_param'
t = text(sql, bindparams=[
        bindparam('alarm_time_param', type_=DateTime, required=True)])
db.execute(t, {'alarm_time_param': date_param})
</code></pre>

<blockquote>
<p>参数 <code>bindparam</code> 可以使用 <code>type_</code> 来制定参数的类型，也可以使用 initial 值来指定参数的类型。</p>
</blockquote>

<pre><code>bindparam('alart_time_param', type_=DateTime)  # or
bindparam('alart_time_param', DateTime())
</code></pre>

<p>3). 如果要转换查询结果中的数据类型，可以通过 <code>text()</code> 的参数 <code>typemap</code> 参数指定。这点比 mybatis 还灵活：</p>

<pre><code class="language-python">from sqlalchemy import Integer, Unicode

t = text('select id, name from users',
         typemap={'id': Integer, 'name': Unicode})
</code></pre>

<p>4). 其他好处，详见 sqlalchemy/sql/expression.py 中的 docstring。</p>

<h2 id="sqlalchemy-访问数据库">SQLAlchemy 访问数据库</h2>

<p><code>create_engine</code> 函数返回一个 <code>Engine</code> 对象。通过 <code>Engine</code> 对象的 <code>execute</code> 方法可以执行数据库操作。</p>

<p><code>execute</code> 方法返回一个 <code>ResultProxy</code> 对象，<code>ResultProxy</code> 类是对 <code>Cursor</code> 类的封装，其中的 <code>cursor</code> 属性对应原来的 <code>cursor</code>，这个类有很多方法对应着 <code>Cursor</code> 类的方法，另外又扩展了一些属性和方法。</p>

<p>对 <code>ResultProxy</code> 对象进行遍历时，得到的每一行都是一个 <code>RowProxy</code> 对象，获取字段的方法非常灵活，索引、字段名，甚至属性都行。</p>

<pre><code>row_proxy[0] == row_proxy['id'] == row_proxy.id
</code></pre>

<p>看得出来，<code>RowProxy</code> 跟Java的 POJO 类有相似的特性。</p>

<pre><code class="language-python">from sqlalchemy import create_engine

db = create_engine('sqlite:///:memory:', echo=True)

# DDL
db.execute('create table users(userid char(10), username char(50))')

# DML
result = db.execute(
    &quot;insert into users (userid, username) values ('user1', 'tony')&quot;)
# get rows affected by an UPDATE or DELETE statement,
# it is not intended to provide the number of rows present from SELECT
result.rowcount
# True if this ResultProxy returns rows.
result.returns_rows

# Query
result = db.execute(&quot;select * from users&quot;)
result.scalar()  # 可以返回一个标量查询的值
result.fetchall()   # 取回所有的行
result.fetchmany()  # 取回多行
result.fetchone()   # 取回一行，并判断有且只有一行，若超出一行则报错
result.first()      # 取回第一行
result.close()  # result 用完之后，需要关闭
</code></pre>

<p>SQLAlchemy 支持事务，甚至可以嵌套事务。</p>

<p>缺省情况下事务自动提交，即执行一条SQL就自动提交。</p>

<p>如果要更精准的控制事务，最简单的方法是使用 <code>connection</code>，然后通过 <code>connection</code> 获取 <code>transaction</code> 对象。</p>

<pre><code class="language-python"># transaction
connection = db.connect()
trans = connection.begin()
try:
    do_something_with(connection)
    trans.commit()
except:
    trans.rollback()
</code></pre>

<p>还有一种方式是在创建 <code>engine</code> 对象时指定 <code>strategy='threadlocal'</code> 参数，这样会自动创建一个线程局部的连接，对于后续的无连接的执行都会自动使用这个连接，这样在处理事务时，只要使用 <code>engine</code> 对象来操作事务就行了。</p>

<p>例如：</p>

<pre><code class="language-python">db = create_engine(connection, strategy='threadlocal')
db.begin()
try:
    do_something()
except:
    db.rollback()
else:
    db.commit()
</code></pre>

<p>如果希望手动提交事务，也可以在 <code>connection</code> 和 <code>statement</code> 上通过 <code>execute_options()</code> 方法修改为手动提交模式。</p>

<pre><code>conn.execute_options(autocommit=False)
</code></pre>

<p>设置为手动提交模式后，要提交事务，需要调用 <code>conn.commit()</code> 方法。</p>

<h2 id="数据库连接池">数据库连接池</h2>

<p>SQLAlchemy 默认的连接池算法选用规则为：</p>

<ol>
<li>连接内存中的 SQLite，默认的连接池算法为 <code>SingletonThreadPool</code> 类，即两个线程允许一个连接；</li>
<li>连接基于文件的 SQLite，默认的i连接池算法为 <code>NullPool</code> 类，即不使用连接池；</li>
<li>对于其他情况，默认的连接池算法为 <code>QueuePool</code> 类。</li>
</ol>

<p>我们也可以实现自己的连接池算法：</p>

<pre><code>db = create_engine('sqlite:///file.db', poolclass=YourPoolClass)
</code></pre>

<p><code>create_engine()</code> 函数和连接池相关的参数有：</p>

<ul>
<li>pool_recycle: 默认为-1，推荐设置为7200，即如果 <code>connection</code> 空闲了 7200秒 = 2小时，自动重新获取，以防止 <code>connection</code> 被数据库服务器关闭；</li>
<li>pool_size: 保持连接数，默认为5，正式环境下该数值偏小，需根据实际情况调整；</li>
<li>max_overflow: 超出 <code>pool_size</code> 后允许的最大连接数，默认为10，这10个连接在使用过后，不放在连接池中，而是被真正关闭的。</li>
<li>pool_timeout: 获取连接的超时阀值，默认为30秒。</li>
</ul>

<p>国内的云服务平台 <a href="http://sae.sina.com.cn">SAE</a> 中的共享型 MySQL 服务不支持连接池，如果在其上部署应用，需要禁用连接池，也就是使用 <code>NullPool</code> 类，否则会报 &lsquo;(2006, MySQL server has gone away)&rsquo; 错误。</p>

<h2 id="日志输出">日志输出</h2>

<p>SQLAlchemy 默认输出日志到 <code>sys.stdout</code>。</p>

<p>如果要输出到文件，log 文件不具备 rotate 功能，不推荐在生产环境中使用。</p>

<pre><code>import logging
logging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)
</code></pre>

<p>调用 <code>create_engine()</code> 函数时，可选传递一个参数 <code>echo=True</code> 来打开详细信息输出，这个功能信息量非常大，只适合调试使用，不建议生产环境中使用。</p>

<h2 id="最佳实践与使用心得">最佳实践与使用心得</h2>

<p>使用 ORM 方式构建复杂查询比较困难，使用 Raw SQL 和 SQL Expression 会比较合适一些。</p>

<p><code>declarative</code> 是 SQLAlchemy 的一个新的扩展功能 ，只能用在 ORM 方式中，不适用在 Raw SQL 和 SQL Expression 方式。</p>

<p>如果使用 ORM 方式，表必须有主键，使用 Raw SQL 和 SQL Express 方式没有这个约束。</p>

<p>查询有简单的也有复杂的，使用 Raw SQL 会比较方便。</p>

<p>增、删、改，多是单表操作，使用 SQL Expression 就足够了。具体讲，比如一个 <code>User</code> 类，<strong>可以包含一个固定的 <code>_table</code> 成员</strong>，增删改直接使用 <code>_table</code> 对象来完成。</p>

<pre><code>_table = Table('users', metadata, autoload=True)
</code></pre>

<p>批量的 insert/update/delete 操作，可以将每行数据组成一个 dict，在将这些 dict 组成一个 list，和 _table.insert()/update()/delete() 一起作为参数传给 `conn.execute()&lsquo;。</p>

<pre><code class="language-python">from sqlalchemy import Table

# _table member object
_table = Table('users', metadata, autoload=True)
# insert
_table.insert().values(f1=value1, f2=value2)
# update
_table.update().values(f1=new_value1, f2=new_value2).where(
    _table.c.f1 == value1).where(_table.c.f2 == value2)
# delete
_table.delete().where(_table.c.f1 == value1).where(
    _table.c.f2 == value2)

# batch opration
conn.execute(_table.insert(), [
        {'user_id': 1, 'email_address': 'jack@yahoo.com'},
        {'user_id': 1, 'email_address': 'jack@msn.com'},
        {'user_id': 2, 'email_address': 'susan@example.com'},
        {'user_id': 2, 'email_address': 'susan@example.org'}
    ])
</code></pre>

<p>SQL Expression 也可以像 Raw SQL 的 <code>text()</code> 函数一样使用 <code>bindparam</code>，方法是在调用 insert()/update()/delete() 时声明参数，然后在 <code>conn.execute()</code> 执行时候，将参数传递进去。</p>

<pre><code class="language-python">d = _table.delete().where(_table.c.hire_date &lt;= bindparam(
    'hire_day', DateTime(), required=True))
conn.execute(d, {'hire_day': datetime.today()})
</code></pre>

<p><code>where()</code> 和 ORM 中的 <code>filter()</code> 接受一样的参数，各种SQL条件都支持：</p>

<pre><code class="language-python"># equals, not equals
where(_table.c.name == 'ed')
where(_table.c.name != 'ed')
# like
where(_table.c.name.like('%ed%'))
# in, not in
where(_table.c.name.in_(['ed', 'wendy', 'jack']))
where(~_table.c.name.in_(['ed', 'wendy', 'jack']))
# is null, is not null
where(_table.c.name == None)
where(_table.c.name != None)

# and, or
from sqlalchemy import and_, or_
where(and_(_table.c.name == 'ed', _table.c.fullname == 'Ed Jones'))
where(or_(_table.c.name == 'ed', _table.c.name == 'wendy'))
# and can also be written with multiple where clause
where(_table.c.name == 'ed').where(_table.c.fullname == 'Ed Jones')

# match: contents of the match parameter are database backend specific
where(_table.c.name.match('wendy'))
</code></pre>
<div class="footnotes">

<hr />

<ol>
<li id="fn:fn-copyright">出处已不可考，如有侵权请联系我。
 <a class="footnote-return" href="#fnref:fn-copyright"><sup>[return]</sup></a></li>
</ol>
</div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://hshsh.me/post/2016-04-02-linux-pack-and-unpack/">
        Linux常用打包解包命令备忘
      </a>
    </h1>

    <span class="post-date">Sat, Apr 2, 2016</span>

    

<h2 id="tar">.tar</h2>

<p>打包： <code>tar cvf file.tar dirname</code></p>

<p>解包： <code>tar xvf filename.tar</code></p>

<h2 id="gz">.gz</h2>

<p>打包： <code>gzip dirname</code></p>

<p>解包： <code>gzip -d filename.gz</code></p>

<h2 id="tar-gz">.tar.gz</h2>

<p>打包： <code>tar zcvf file.tar.gz dirname</code></p>

<p>解包： <code>tar zxvf file.tar.gz</code></p>

<h2 id="tar-bz2">.tar.bz2</h2>

<p>打包： <code>tar jcvf file.tar.bz2 dirname</code></p>

<p>解包： <code>tar jxvf filename.tar.bz2</code> or <code>tar jxvf filename.tar.bz</code></p>

<h2 id="zip">.zip</h2>

<p>打包： <code>zip file.zip dirname</code></p>

<p>解包： <code>unzip filename.zip</code></p>

<h2 id="rar">.rar</h2>

<p>安装： <code>sudo apt-get install rar</code></p>

<p>打包： <code>rar e dirname</code></p>

<p>解包： <code>rar a filename.rar</code></p>

<h2 id="z">.z</h2>

<p>打包： <code>compress dirname</code></p>

<p>解包： <code>uncompress filename.z</code></p>

<h2 id="tar-z">.tar.Z</h2>

<p>打包： <code>tar Zcvf file.tar.Z dirname</code></p>

<p>解包： <code>tar Zxvf filename.tar.Z</code></p>

<h2 id="tgz">.tgz</h2>

<p>解包： <code>tar zxvf filename.tgz</code></p>

<h2 id="tar-tgz">.tar.tgz</h2>

<p>打包： <code>tar zcvf file.tar.tgz dirname</code></p>

<p>解包： <code>tar zxvf filename.tar.tgz</code></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://hshsh.me/post/2014-01-13-make-beautiful-ebooks-with-pandoc/">
        使用Pandoc制作精美的EPUB、MOBI、PDF电子书
      </a>
    </h1>

    <span class="post-date">Mon, Jan 13, 2014</span>

    

<h2 id="缘起">缘起</h2>

<p>在亚马逊上购买了一部 Kindle Paperwhite，设备还没有拿到手，就先在网上找一找电子书。结果是网络上的电子书资源确实是不少，但是质量确是参差不齐，只有少数书籍排版精美，书签完整，大多数简直惨不忍睹，可以说严重影响阅读体验。</p>

<p>之前接触到了Pandoc这个文本格式转换的瑞士军刀，知道它可以简便的输出HTML、PDF、EPUB、DOCX等等各类格式，所以就想着能不能自己动手制作排版精美的电子书。经过一番探索，发现可以使用Pandoc和KindleGen非常方便的制作排版精良的电子书。本着造福买不起纸质书的广大群众的意愿，将所得方法及操作技巧发布出来，于是有了本文。</p>

<h2 id="简便制作和精美电子书的定义">简便制作和精美电子书的定义</h2>

<p><strong>简便制作</strong>：</p>

<ul>
<li>从网络上到处可以下载到的TXT格式可以方便的转换为排好版的电子书</li>
<li>源文件一次整理，可以直接输出为不同格式的电子文档，比如通过一条命名，或者一两次鼠标点击</li>
<li>简单方便的控制书籍排版格式（全书格式统一），比如通过一个CSS文件</li>
<li>修改书籍内容时，不影响书籍排版格式</li>
</ul>

<p><strong>精美电子书</strong>：</p>

<ul>
<li>带有目录，支持EPUB、MOBI阅读软件导航功能</li>
<li>标题、正文、引用字段使用不同字体区分，便于识别阅读</li>
<li>段落距离、缩进与行距设置适中，便于识别阅读</li>
<li>整书排版格式统一，清晰易读，无过多分散注意力的元素</li>
</ul>

<hr />

<h2 id="相关技术说明">相关技术说明</h2>

<p>这里应该先从Markdown说起，作为一种轻量级的标记语言，Markdown很适合用来写作，能够让作者把精力集中在书籍的内容撰写上面，而不会被排版等问题分散精力。在一份典型的Markdown文件中，作者使用简单的符号来标记诸如标题、段落、引用、列表、图表等等结构元素，而不记录排版相关的内容。具体的排版工作留给编译器来处理，也就是在从Markdown源文件输出HTML、PDF、EPUB等出版格式的时候才确定，并且可以简便的通过格式控制指令（CSS文件、Latex模板）来控制排版格式。Pandoc就是这么一个编译器，它与其它Markdown解释器相比，有它自己的优点：扩展的Markdown的语法，弥补了Markdown语法结构简单的问题；可以生成很多种格式的文件，甚至Word文档也可以；使用模板控制输出结果，定制简单，等等。关于Markdown和Pandoc的更多信息，请参阅本节的<a href="#markdown-pandoc-readings">拓展阅读</a>和文末的<a href="#foot-reference">参考资料</a>。</p>

<p>对于使用OCR或者网络下载的TXT文件制作电子书来说，并不需要掌握Markdown和Pandoc的全部知识，只需要知道基本的Markdown语法就行了：标题、段落、引用、列表、图表等，这些很简单，查看Markdown的语法说明，一会儿就能掌握。按照Markdown的语法对TXT文件进行格式化，然后使用本文中展示的模板文件和编译命令，就可以直接输出为EPUB、MOBI、PDF（A4、6寸，或者其它定制尺寸）文件了。</p>

<p>生成EPUB是Pandoc原生自带的功能，只需要在源文件中填入相关的metadata属性，整理好书籍内容，直接一条命名就OK了。</p>

<p>生成MOBI使用亚马逊的KindleGen软件，由上面生成的EPUB文件为参数转换而来，生成的MOBI文件同时包含旧的MOBI格式和新的KF8格式，可以直接向Amazon网上商店发布，也可以使用Calibre软件（需要安装MobiUnpack插件）拆分为单独的MOBI和AZW3文件。</p>

<p>生成6寸或A4尺寸的PDF，也是Pandoc原生自带的功能，Pandoc根据源文件生成Latex文件，然后使用Latex来生成PDF文件。Latex生成的PDF质量高、排版精美是众所周知的，但是它的语法非常复杂，学习曲线陡峭，把一般用户远远地挡在了大门外。现在有了Pandoc，我们就可以以非常少的工作享用Latex输出的高质量PDF了，当然了这需要Latex软件，只要安装Pandoc用户指南中推荐的<a href="http://miktex.org/">MikTex</a>即可。不同尺寸的PDF只需要在运行pandoc命令时指定相应的模板文件就行了。</p>

<p><span id="markdown-pandoc-readings"></span>
关于Markdown和Pandoc的拓展阅读：</p>

<ol>
<li><a href="http://zh.wikipedia.org/wiki/Markdown">Markdown - 维基百科</a></li>
<li><a href="http://iout.in/archives/454.html">Markdown+Pandoc 最佳写作拍档</a></li>
</ol>

<hr />

<h2 id="操作步骤">操作步骤</h2>

<p>前面已经把使用到的程序，相关的原理都已经交代了，实际上真正操作起来，步骤非常简单。</p>

<h3 id="安装使用到的软件">安装使用到的软件</h3>

<p><strong>安装Pandoc</strong>：</p>

<p>从Pandoc官方网站<a href="https://code.google.com/p/pandoc/downloads/list">下载适用于Windows的安装包</a>安装，下一步、下一步，就OK了<sup class="footnote-ref" id="fnref:fn-pandoc"><a rel="footnote" href="#fn:fn-pandoc">1</a></sup>。</p>

<p><strong>安装MikTex</strong>：</p>

<p>从<a href="http://miktex.org/">MikTex</a>官方网站下载MikTex安装包进行安装，可以选择安装版本或者便携版本均可。 如果选择使用便携版本，需要把MikTex的执行文件路径添加到系统路径中。</p>

<h3 id="整理源文件">整理源文件</h3>

<p>用Pandoc的语法把文章的结构标记出来，各章节标题、引用、表格、插图等等。并把书籍内容文件命名为 <code>book.md</code> 。如果对正则表达式熟悉的话，这里可以使用sed或者其它编辑器对文件进行批量处理。这不是本文的讨论内容，这里就不再详述，如果有兴趣，请自行查阅相关资料。</p>

<p>添加EPUB元数据到 <code>meta.md</code> 文件，这里使用markdown文件中内嵌的YAML来书写，详见<a href="http://johnmacfarlane.net/pandoc/README.html#epub-metadata">Pandoc用户指南中的EPUB元数据部分</a>和本文样书源文件中的 <code>meta.md</code> 。</p>

<p>下载或制作一张封面图片命名为 <code>cover.jpg</code> 与书籍的源文件放在同一个目录。</p>

<h3 id="根据需要修改模板文件">根据需要修改模板文件</h3>

<p>本文附件模板中的 epub.css 文件是控制生成的EPUB和MOBI文件排版格式的，如果有需要，请根据需要进行修改，比如正文字体、引用段落字体、各级标题字体字号等。需要注意的是MOBI并不能支持完整的CSS规范，详情请参阅<a href="#foot-reference">参考资料 Amazon Kindle Publishing Guidelines</a>。</p>

<p>附件模板中的 default.epub 文件是Pandoc生成EPUB时使用的HTML模板，需要放在用户数据目录下的 <code>templates</code> 文件夹中，详细见<a href="#foot-reference">参考资料 Pandoc User&rsquo;s Guide</a> 中的 <code>--data-dir</code> 部分。如有需要，也可以进行修改。</p>

<p>附件模板中的 kindle.latex (6 inch) 和 zhtw.latex (A4) 文件是控制生成的PDF文件排版格式的，如果有需要，请根据需要进行修改，页边距、字体、字号、段落间距、行间距等。</p>

<h3 id="生成电子书">生成电子书</h3>

<p>把源文件整理好后，只需要运行下面的命令就可以输出包含目录、支持导航、排版精美的电子书了。</p>

<p>使用Pandoc和KindleGen生成电子书的命令（模板文件 <code>_tmpl/build.bat</code>）：</p>

<div><pre><code>:: A4 pdf
pandoc meta.md book.md --toc --template=zhtw --latex-engine=xelatex ^
    --no-tex-ligatures -V documentclass=scrartcl -o A4.pdf
:: 6 inch pdf
pandoc meta.md book.md --toc --template=kindle --latex-engine=xelatex ^
    --no-tex-ligatures -V documentclass=scrartcl -o output.pdf
:: epub
pandoc meta.md book.md -o output.epub
:: mobi
kindelgen output.epub
</code></pre></div>

<p>通过上面这几条命令，已经生成好了各种格式和尺寸的电子书，是不是很简便？有没有很过瘾的感觉？</p>

<p>顺带一提，本文及本网站也都是用Pandoc生成的 ~ ~</p>

<p>最后，忠心的祝愿网络上能够有越来越多的高质量电子书供我们这些买不起纸质书的穷屌丝们阅读。</p>

<hr />

<h2 id="模板及样书下载">模板及样书下载</h2>

<p>前文中提到的文件模板，以及南怀瑾大师著述的《南怀瑾选集·第十卷·原本大学微言》源文件和生成的电子书（包括EPUB、MOBI、6寸和A4尺寸的PDF）作为示例样书，可以从我的百度网盘下载：</p>

<p><small style="color: red; font-style:italic">（2016/4/2更新：链接已失效，暂时没有时间更新）</small></p>

<ul>
<li>模板：<s><a href="http://pan.baidu.com/s/1o6rvZAU">http://pan.baidu.com/s/1o6rvZAU</a></s></li>
<li>源文件：<s><a href="http://pan.baidu.com/s/1iWWdo">http://pan.baidu.com/s/1iWWdo</a></s></li>
<li>电子书：<s><a href="http://pan.baidu.com/s/1nt6Xcud">http://pan.baidu.com/s/1nt6Xcud</a></s></li>
</ul>

<p><span id="foot-reference"></span></p>

<h2 id="参考资料">参考资料</h2>

<ol>
<li><a href="http://johnmacfarlane.net/pandoc/README.html">Pandoc User&rsquo;s Guide</a>：Pandoc用法，以YAML格式书写 epub metadata</li>
<li><a href="http://kindlegen.s3.amazonaws.com/AmazonKindlePublishingGuidelines.pdf">Amazon Kindle Publishing Guidelines</a>：Amazon Kindle MOBI 格式参考</li>
<li><a href="http://puppetlabs.com/blog/automated-ebook-generation-convert-markdown-epub-mobi-pandoc-kindlegen">How We Automated Our Ebook Builds With Pandoc and KindleGen</a>：从epub文件自动生成mobi文件</li>
<li><a href="https://github.com/tzengyuxio/pages/tree/gh-pages/pandoc">Pandoc latex 模板参考</a>：Pandoc 中文 PDF 模板</li>
<li><a href="http://www.bibodeng.com/tools/140.html">xelatex在文档处理中的使用</a>：Pandoc latex 中文字体处理</li>
<li><a href="hi.baidu.com/asnahu/item/59ce80a9ce7e9a15a8cfb707">LaTeX quote 重定义</a>：Pandoc latex 模板定制</li>
</ol>
<div class="footnotes">

<hr />

<ol>
<li id="fn:fn-pandoc">撰写本文时，Pandoc最新版本为 1.12.3
 <a class="footnote-return" href="#fnref:fn-pandoc"><sup>[return]</sup></a></li>
</ol>
</div>

  </div>
  
</div>
</div>

  </body>
</html>
